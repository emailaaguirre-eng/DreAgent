# Export and Download Workers (using PyQt6)
from PyQt6.QtCore import QObject, pyqtSignal, pyqtSlot
import sys
import os

class ExportWorker(QObject):
    finished = pyqtSignal(str)
    error = pyqtSignal(str)
    def __init__(self, path, mode, model, message_history, chat_text):
        super().__init__()
        self.path = path
        self.mode = mode
        self.model = model
        self.message_history = message_history
        self.chat_text = chat_text
    @pyqtSlot()
    def run(self):
        try:
            if not self.path:
                self.error.emit("No export path specified")
                return
            
            try:
                if self.path.endswith('.json'):
                    import json
                    data = {
                        'mode': str(self.mode) if self.mode else '',
                        'model': str(self.model) if self.model else '',
                        'history': self.message_history if isinstance(self.message_history, list) else []
                    }
                    with open(self.path, 'w', encoding='utf-8') as f:
                        json.dump(data, f, indent=2, ensure_ascii=False)
                else:
                    text_to_save = str(self.chat_text) if self.chat_text else ''
                    with open(self.path, 'w', encoding='utf-8') as f:
                        f.write(text_to_save)
                self.finished.emit(self.path)
            except PermissionError:
                self.error.emit(f"Permission denied: Cannot write to {self.path}")
            except OSError as os_err:
                self.error.emit(f"File system error: {str(os_err)}")
        except Exception as e:
            error_msg = f"Export error: {str(e)}"
            logging.error(f"ExportWorker error: {traceback.format_exc()}")
            self.error.emit(error_msg)

class DownloadWorker(QObject):
    finished = pyqtSignal(str, str)
    error = pyqtSignal(str)
    def __init__(self, last_response):
        super().__init__()
        self.last_response = last_response
    @pyqtSlot()
    def run(self):
        try:
            if not self.last_response:
                self.error.emit("No response content to download")
                return
            
            try:
                download_path = save_to_downloads(str(self.last_response), "lea_response.txt")
            except Exception as save_error:
                self.error.emit(f"Error saving file: {str(save_error)}")
                return
            
            try:
                basename = os.path.basename(download_path)
            except Exception:
                basename = "lea_response.txt"
            
            self.finished.emit(download_path, basename)
        except Exception as e:
            error_msg = f"Download error: {str(e)}"
            logging.error(f"DownloadWorker error: {traceback.format_exc()}")
            self.error.emit(error_msg)

class SpeechRecognitionWorker(QObject):
    """Worker thread for speech recognition to avoid blocking UI"""
    finished = pyqtSignal(str)  # Emits recognized text
    error = pyqtSignal(str)
    listening = pyqtSignal()  # Emits when listening starts
    
    def __init__(self, device_index=None):
        super().__init__()
        self.device_index = device_index  # Optional: specific microphone device index
    
    @pyqtSlot()
    def run(self):
        try:
            if not SPEECH_RECOGNITION_AVAILABLE:
                self.error.emit("Speech recognition not available. Install with: pip install SpeechRecognition")
                return
            
            # Import inside the function to avoid errors if module not installed
            import speech_recognition as sr
            
            recognizer = sr.Recognizer()
            
            # List available microphones
            try:
                mic_list = sr.Microphone.list_microphone_names()
                if not mic_list:
                    self.error.emit("No microphones detected. Please check your microphone connection.")
                    return
            except Exception as e:
                logging.warning(f"Error listing microphones: {e}")
                mic_list = []
            
            # Use specified device index or default (None = system default)
            # Get microphone name for logging
            try:
                if self.device_index is not None and self.device_index < len(mic_list):
                    mic_name = mic_list[self.device_index]
                else:
                    mic_name = "Default Microphone"
            except:
                mic_name = "Default Microphone"
            
            # Emit listening signal before opening microphone (so user knows to speak)
            self.listening.emit()
            logging.info(f"Listening started on microphone '{mic_name}' (device index: {self.device_index})")
            
            # Listen for audio with longer timeout
            # Use the microphone in a single context - don't create multiple instances
            try:
                # Assign microphone object before using it
                if self.device_index is not None:
                    microphone = sr.Microphone(device_index=self.device_index)
                else:
                    microphone = sr.Microphone()
                # Open microphone context - this actually activates the microphone
                with microphone as source:
                    logging.info(f"Microphone '{mic_name}' opened successfully, starting calibration...")
                    
                    # First, try to adjust for ambient noise (quick test that mic works)
                    try:
                        recognizer.adjust_for_ambient_noise(source, duration=0.5)
                        logging.info(f"Ambient noise calibration completed for '{mic_name}'")
                    except Exception as noise_error:
                        # If ambient noise adjustment fails, log but continue
                        # Some microphones don't need or support this
                        noise_details = str(noise_error)
                        logging.warning(f"Ambient noise adjustment failed for '{mic_name}': {noise_details}")
                        
                        # Check if it's a serious error that means mic won't work
                        if "PortAudio" in noise_details or "unavailable" in noise_details.lower() or "device" in noise_details.lower():
                            error_msg = f"Cannot access microphone '{mic_name}': {noise_details}\n\n"
                            error_msg += "The microphone cannot be opened. Possible causes:\n"
                            error_msg += "1. ‚ö†Ô∏è Another application is using the microphone (close Zoom, Teams, Skype, etc.)\n"
                            error_msg += "2. ‚ö†Ô∏è Microphone drivers need to be updated\n"
                            error_msg += "3. ‚ö†Ô∏è Microphone permissions not granted (Windows Settings > Privacy > Microphone)\n"
                            error_msg += "4. ‚ö†Ô∏è Device hardware issue\n\n"
                            error_msg += "Try:\n- Closing all other applications\n- Restarting the application\n- Selecting 'Default' microphone\n- Updating Logitech Brio drivers"
                            self.error.emit(error_msg)
                            return
                        # Continue anyway - the microphone might still work without calibration
                    
                    # Listen for audio with longer timeout for natural conversations
                    logging.info(f"Starting to listen for audio on '{mic_name}'...")
                    audio = recognizer.listen(source, timeout=30, phrase_time_limit=60)
                    logging.info(f"Audio captured successfully from '{mic_name}'")
            except sr.WaitTimeoutError:
                self.error.emit("No speech detected within 30 seconds. Please try speaking when you see 'Listening...'")
                return
            except Exception as listen_error:
                error_details = str(listen_error)
                error_msg = f"Error listening to microphone '{mic_name}': {error_details}\n\n"
                error_msg += "The microphone stopped working during recording. This usually means:\n"
                error_msg += "1. Another application took control of the microphone\n"
                error_msg += "2. The microphone was disconnected\n"
                error_msg += "3. A driver or hardware issue occurred\n\n"
                error_msg += "Try closing other apps and trying again."
                self.error.emit(error_msg)
                return
            
            # Recognize speech using Google's service (free, no API key needed)
            try:
                text = recognizer.recognize_google(audio)
                if text:
                    self.finished.emit(text)
                else:
                    self.error.emit("Could not understand audio. Please try again.")
            except sr.UnknownValueError:
                self.error.emit("Could not understand audio. Please try again.")
            except sr.RequestError as e:
                self.error.emit(f"Speech recognition service error: {str(e)}")
                
        except sr.WaitTimeoutError:
            self.error.emit("No speech detected. Please try again.")
        except Exception as e:
            error_msg = str(e)
            # Provide more helpful error messages
            if "PyAudio" in error_msg or "pyaudio" in error_msg.lower():
                error_msg = "Microphone access error. Please check:\n1. Microphone is connected\n2. Microphone permissions are granted\n3. No other application is using the microphone"
            elif "timeout" in error_msg.lower():
                error_msg = "No speech detected. Please try speaking when you see 'Listening...'"
            
            if SPEECH_RECOGNITION_AVAILABLE:
                logging.error(f"SpeechRecognitionWorker error: {traceback.format_exc()}")
            self.error.emit(f"Speech recognition error: {error_msg}")

### Lea - Complete Multi-Agent System ###

"""
Hummingbird ‚Äì Lea
Multi-agent assistant with:
- All 7 specialized modes
- Knowledge base integration  
- Universal file reading
- Automatic backups with timestamps
- Download capability
"""

from dotenv import load_dotenv
import os
from pathlib import Path

load_dotenv()

# Validate Outlook credentials on startup
def validate_outlook_credentials():
    """Check if Outlook credentials are properly configured"""
    env_file = Path("F:/Dre_Programs/LeaAssistant/.env")
    if not env_file.exists():
        return False, ".env file not found"
    
    # Reload to ensure we have latest values
    load_dotenv(dotenv_path=env_file, override=True)
    
    client_id = os.getenv("OUTLOOK_CLIENT_ID")
    tenant_id = os.getenv("OUTLOOK_TENANT_ID")
    
    # Check if they exist in file but not loaded (syntax error)
    if not client_id or not tenant_id:
        try:
            with open(env_file, 'r', encoding='utf-8') as f:
                content = f.read()
            has_in_file = "OUTLOOK_CLIENT_ID" in content.upper() and "OUTLOOK_TENANT_ID" in content.upper()
            
            if has_in_file and (not client_id or not tenant_id):
                return False, "Outlook credentials found in .env file but not loading - check for syntax errors (run diagnose_env.py)"
        except:
            pass
    
    if not client_id:
        return False, "OUTLOOK_CLIENT_ID missing from .env file"
    if not tenant_id:
        return False, "OUTLOOK_TENANT_ID missing from .env file (will use 'common' as default)"
    
    return True, "Outlook credentials configured"

# Run validation (non-blocking, just logs warning)
try:
    outlook_ok, outlook_msg = validate_outlook_credentials()
    if not outlook_ok:
        print(f"‚ö†Ô∏è  Outlook Integration Warning: {outlook_msg}")
        print("   Run 'python diagnose_env.py' to diagnose the issue")
        print("   Run 'python protect_outlook_credentials.py' to fix it")
except Exception as e:
    pass  # Don't block startup if validation fails

import html
import json
import re
import shutil
import time
import hashlib
from datetime import datetime, date, time, timedelta
from typing import Optional, List, Dict, Any

import requests
from openai import OpenAI

# Speech recognition - optional
try:
    import speech_recognition as sr
    SPEECH_RECOGNITION_AVAILABLE = True
except ImportError:
    SPEECH_RECOGNITION_AVAILABLE = False
    print("WARNING: speech_recognition module not found. Install with: pip install SpeechRecognition")

# Helper function for safe printing (handles Windows console encoding)
def safe_print(text):
    """Print text, handling Unicode encoding errors on Windows"""
    try:
        print(text)
    except UnicodeEncodeError:
        # Fallback: remove or replace problematic characters
        safe_text = text.encode('ascii', 'replace').decode('ascii')
        print(safe_text)

# Text-to-speech - optional
# Try edge-tts first (offline, better quality on Windows)
EDGE_TTS_AVAILABLE = False
try:
    import edge_tts
    EDGE_TTS_AVAILABLE = True
    safe_print("[OK] edge-tts available (offline, high quality)")
except ImportError:
    print("NOTE: edge-tts not installed. Install with: pip install edge-tts")
    print("      (Recommended for better quality and offline support)")

# Fallback to gTTS
GTTS_AVAILABLE = False
try:
    from gtts import gTTS
    import tempfile
    import os
    GTTS_AVAILABLE = True
    if not EDGE_TTS_AVAILABLE:
        safe_print("[OK] gtts available (requires internet)")
except ImportError:
    print("WARNING: gtts module not found. Install with: pip install gtts")

TTS_AVAILABLE = EDGE_TTS_AVAILABLE or GTTS_AVAILABLE

# Check for pygame (preferred for seamless playback)
try:
    import pygame
    PYGAME_AVAILABLE = True
except ImportError:
    PYGAME_AVAILABLE = False
    print("NOTE: pygame not installed. Audio will open in media player.")
    print("For seamless audio playback, install with: pip install pygame")

from PyQt6.QtCore import Qt, pyqtSignal, QThread, QObject, pyqtSlot, QUrl, QTimer, QPropertyAnimation, QEasingCurve
from PyQt6.QtGui import QIcon, QPixmap, QColor, QDragEnterEvent, QDragMoveEvent, QDropEvent, QTextCursor, QKeySequence, QTextCharFormat
from PyQt6.QtWidgets import (
    QApplication, QWidget, QVBoxLayout, QHBoxLayout,
    QLabel, QPushButton, QComboBox, QTextEdit, QLineEdit,
    QSizePolicy, QFrame, QSplashScreen, QFileDialog,
    QMessageBox, QCheckBox, QDialog, QTableWidget, QGroupBox, QDialogButtonBox,
    QTableWidgetItem, QHeaderView, QMenu,
    QListWidget, QListWidgetItem,
)

# =====================================================
# PROJECT DIRECTORY (Must be defined early for logging)
# =====================================================

PROJECT_DIR = Path(os.getenv("LEA_PROJECT_DIR", "F:/Dre_Programs/LeaAssistant"))
BACKUP_DIR_F = Path(os.getenv("LEA_BACKUP_DIR_F", "F:/LeaAssistant_Backups"))

# Use Path.home() to avoid hardcoding username
home = Path.home()
icloud_subpath = os.getenv("LEA_BACKUP_DIR_ICLOUD", "iCloudDrive/Dre_Program_Files")
BACKUP_DIR_ICLOUD = home / icloud_subpath

PROJECT_DIR.mkdir(parents=True, exist_ok=True)  # Ensure directory exists

# Add PROJECT_DIR to Python path so imports work regardless of where script is run from
import sys
if str(PROJECT_DIR) not in sys.path:
    sys.path.insert(0, str(PROJECT_DIR))

# Agent and user names from environment (for easy deployment to other users)
LEA_AGENT_NAME = os.getenv("LEA_AGENT_NAME", "Lea")
LEA_USER_NAME = os.getenv("LEA_USER_NAME", "Dre")

# =====================================================
# TIME-AWARE GREETINGS
# =====================================================

def get_greeting():
    """Get time-appropriate greeting based on current hour"""
    hour = datetime.now().hour
    
    if 5 <= hour < 12:
        return "Good morning"
    elif 12 <= hour < 17:
        return "Good afternoon"
    elif 17 <= hour < 22:
        return "Good evening"
    else:
        return "Hey there"  # Late night/early morning

def get_time_context():
    """Get additional context about the time of day"""
    hour = datetime.now().hour
    
    if 5 <= hour < 9:
        return "Hope you're having a great start to your day!"
    elif 9 <= hour < 12:
        return "Hope your morning is going well!"
    elif 12 <= hour < 14:
        return "Hope you're having a good lunch break!"
    elif 14 <= hour < 17:
        return "Hope your afternoon is productive!"
    elif 17 <= hour < 20:
        return "Hope you're wrapping up a good day!"
    elif 20 <= hour < 22:
        return "Hope you're winding down nicely!"
    else:
        return "Burning the midnight oil?"

# --- CRASH HANDLER (Global Exception Logger) ---
import logging
import traceback

CRASH_LOG = str(PROJECT_DIR / "lea_crash.log")

logging.basicConfig(
    filename=CRASH_LOG,
    level=logging.DEBUG,
    format="%(asctime)s %(levelname)s %(message)s"
)

def handle_exception(exc_type, exc_value, exc_tb):
    tb = "".join(traceback.format_exception(exc_type, exc_value, exc_tb))
    logging.error("Uncaught exception:\n%s", tb)

    try:
        msg = QMessageBox()
        msg.setWindowTitle("Lea Error")
        msg.setText("An unexpected error occurred.\nDetails were saved to lea_crash.log.")
        msg.setDetailedText(tb)
        msg.setIcon(QMessageBox.Icon.Critical)
        msg.exec()
    except Exception:
        print("Error while showing message box:", traceback.format_exc())
        print(tb)

sys.excepthook = handle_exception
# --- END CRASH HANDLER ---


# Import universal file reader
try:
    from universal_file_reader import read_file
    FILE_READER_AVAILABLE = True
except ImportError:
    print("WARNING: universal_file_reader.py not found.")
    FILE_READER_AVAILABLE = False
    def read_file(path):
        return {'success': False, 'error': 'File reader not available'}

# Import task system
try:
    from lea_tasks import get_task_registry, TaskResult
    TASK_SYSTEM_AVAILABLE = True
    task_registry = get_task_registry()
except ImportError as e:
    print(f"WARNING: lea_tasks.py not found. Task system disabled: {e}")
    TASK_SYSTEM_AVAILABLE = False
    task_registry = None

# =====================================================
# OPENAI SETUP
# =====================================================

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
openai_client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None

if not OPENAI_API_KEY:
    print("=" * 60)
    print("WARNING: 'OPENAI_API_KEY' not found in .env")
    print("=" * 60)

# =====================================================
# DIRECTORIES
# =====================================================

# PROJECT_DIR is already defined above, just set up subdirectories
ASSETS_DIR = PROJECT_DIR / "assets"
BACKUPS_DIR = PROJECT_DIR / "backups"
DOWNLOADS_DIR = PROJECT_DIR / "downloads"

# Build personality section (for executable customization only)
def build_personality_section(custom_personality=None):
    """Build personality section from config or use defaults"""
    if custom_personality:
        # Use custom personality if provided (executable only)
        return custom_personality
    else:
        # Default personality (main file - hardcoded)
        return """**Warm & Friendly**: 
- Always greet Dre with warmth and enthusiasm
- Use a conversational, approachable tone
- Show genuine care and interest in helping
- Remember details about Dre and reference them naturally

**Funny & Personable**:
- Use appropriate humor when fitting (not forced)
- Light jokes and playful comments are welcome
- Keep things engaging and enjoyable
- Don't be overly formal - be like a trusted friend

**Intelligent & Thoughtful**:
- Provide well-reasoned, insightful responses
- Think before answering, consider context
- Offer multiple perspectives when helpful
- Admit when you're uncertain and explain why

**Helpful & Proactive**:
- Anticipate needs when possible
- Offer solutions, not just information
- Break down complex topics clearly
- Suggest next steps when appropriate

**Mindful & Respectful**:
- Always remember Dre's preferences and constraints
- Respect boundaries and ask before making changes
- Consider the context of requests
- Be mindful of tone and appropriateness

**Communication Style**:
- Use "I" and "you" naturally (like we're chatting)
- Be enthusiastic but not overwhelming
- Balance professionalism with friendliness
- Use emojis sparingly but appropriately (üê¶ for yourself, ‚úÖ for success, etc.)"""

# Create directories
for dir_path in [BACKUPS_DIR, DOWNLOADS_DIR]:
    dir_path.mkdir(exist_ok=True)

# Splash and icon files - try multiple possible names
SPLASH_FILE = ASSETS_DIR / "Hummingbird_LEA_v1_Splash_Logo_Lime_Green.png"
if not SPLASH_FILE.exists():
    # Try alternative names
    SPLASH_FILE = ASSETS_DIR / "Hummingbird_LEA_v1_Logo_Neon_Green.png"

ICON_FILE = ASSETS_DIR / "Hummingbird_LEA_Logo_White_No BKGND.png"
if not ICON_FILE.exists():
    # Try alternative names (ICO or PNG)
    ICON_FILE = ASSETS_DIR / "Hummingbird_LEA_v1_Logo_Neon_Green.ico"
    if not ICON_FILE.exists():
        ICON_FILE = ASSETS_DIR / "Hummingbird_LEA_v1_Logo_Neon_Green.png"

print(f"\nDirectories created:")
print(f"  üíæ Backups: {BACKUPS_DIR}")
print(f"  üì• Downloads: {DOWNLOADS_DIR}\n")

# =====================================================
# BACKUP SYSTEM
# =====================================================

def create_backup(file_path: Path) -> str:
    """Create timestamped backup in backups/ folder"""
    if not file_path.exists():
        return None
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_name = f"{file_path.stem}_{timestamp}{file_path.suffix}"
    backup_path = BACKUPS_DIR / backup_name
    
    shutil.copy2(file_path, backup_path)
    return str(backup_path)

def manage_14_day_backups(source_file: Path, max_backups: int = 14):
    r"""
    Create backups in two locations with 14-day retention (failsafe system):
    1. F: drive (configured via LEA_BACKUP_DIR_F, default: F:/LeaAssistant_Backups)
    2. iCloud (configured via LEA_BACKUP_DIR_ICLOUD, default: ~/iCloudDrive/Dre_Program_Files)
    
    Overwrites oldest backup when limit is reached.
    Returns dict with success status for each location.
    """
    if not source_file.exists():
        logging.warning(f"Source file does not exist: {source_file}")
        return {"f_drive": False, "icloud": False, "error": "Source file not found"}
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_name = f"{source_file.stem}_{timestamp}{source_file.suffix}"
    
    # Track success for each location
    results = {"f_drive": False, "icloud": False}
    
    # Location 1: F: drive - ALWAYS attempt (failsafe requirement)
    f_drive_backup_dir = BACKUP_DIR_F
    f_drive_success = False
    try:
        # Try to create directory (will succeed even if F: doesn't exist initially)
        try:
            f_drive_backup_dir.mkdir(parents=True, exist_ok=True)
        except (OSError, PermissionError) as dir_error:
            # Check if F: drive exists
            if not Path("F:/").exists():
                logging.error("‚ùå FAILSAFE ALERT: F: drive not available - backup to F: drive FAILED")
                results["f_drive"] = False
            else:
                logging.error(f"‚ùå FAILSAFE ALERT: Cannot create F: drive backup directory: {dir_error}")
                results["f_drive"] = False
            raise
        
        # Create new backup
        backup_path = f_drive_backup_dir / backup_name
        shutil.copy2(source_file, backup_path)
        
        # Verify backup was created
        if backup_path.exists() and backup_path.stat().st_size > 0:
            logging.info(f"‚úÖ F: drive backup created: {backup_path}")
            f_drive_success = True
        else:
            logging.error(f"‚ùå FAILSAFE ALERT: F: drive backup file verification failed: {backup_path}")
        
        # Clean up old backups (keep only max_backups)
        pattern = f"{source_file.stem}_*{source_file.suffix}"
        existing_backups = sorted(
            f_drive_backup_dir.glob(pattern),
            key=lambda p: p.stat().st_mtime,
            reverse=True
        )
        
        if len(existing_backups) > max_backups:
            backups_to_remove = existing_backups[max_backups:]
            for old_backup in backups_to_remove:
                try:
                    old_backup.unlink()
                    logging.info(f"Removed old F: drive backup (14-day limit): {old_backup.name}")
                except Exception as e:
                    logging.warning(f"Failed to remove old F: drive backup {old_backup}: {e}")
        
        logging.info(f"F: drive backup location now has {min(len(existing_backups), max_backups)} backups")
        results["f_drive"] = f_drive_success
        
    except Exception as e:
        logging.error(f"‚ùå FAILSAFE ALERT: F: drive backup FAILED: {e}")
        import traceback
        logging.error(traceback.format_exc())
        results["f_drive"] = False
    
    # Location 2: iCloud - ALWAYS attempt (failsafe requirement)
    icloud_backup_dir = BACKUP_DIR_ICLOUD
    icloud_success = False
    try:
        # Try to create directory (will create parent directories if needed)
        try:
            icloud_backup_dir.mkdir(parents=True, exist_ok=True)
        except (OSError, PermissionError) as dir_error:
            logging.error(f"‚ùå FAILSAFE ALERT: Cannot create iCloud backup directory: {dir_error}")
            logging.error(f"   Path attempted: {icloud_backup_dir}")
            results["icloud"] = False
            raise
        
        # Create new backup
        backup_path = icloud_backup_dir / backup_name
        shutil.copy2(source_file, backup_path)
        
        # Verify backup was created
        if backup_path.exists() and backup_path.stat().st_size > 0:
            logging.info(f"‚úÖ iCloud backup created: {backup_path}")
            icloud_success = True
        else:
            logging.error(f"‚ùå FAILSAFE ALERT: iCloud backup file verification failed: {backup_path}")
        
        # Clean up old backups (keep only max_backups)
        pattern = f"{source_file.stem}_*{source_file.suffix}"
        existing_backups = sorted(
            icloud_backup_dir.glob(pattern),
            key=lambda p: p.stat().st_mtime,
            reverse=True
        )
        
        if len(existing_backups) > max_backups:
            backups_to_remove = existing_backups[max_backups:]
            for old_backup in backups_to_remove:
                try:
                    old_backup.unlink()
                    logging.info(f"Removed old iCloud backup (14-day limit): {old_backup.name}")
                except Exception as e:
                    logging.warning(f"Failed to remove old iCloud backup {old_backup}: {e}")
        
        logging.info(f"iCloud backup location now has {min(len(existing_backups), max_backups)} backups")
        results["icloud"] = icloud_success
        
    except Exception as e:
        logging.error(f"‚ùå FAILSAFE ALERT: iCloud backup FAILED: {e}")
        import traceback
        logging.error(traceback.format_exc())
        results["icloud"] = False
    
    # Final status report
    if results["f_drive"] and results["icloud"]:
        logging.info("‚úÖ‚úÖ FAILSAFE STATUS: Both backup locations succeeded - you have dual protection!")
    elif results["f_drive"] or results["icloud"]:
        failed_location = "iCloud" if not results["icloud"] else "F: drive"
        logging.warning(f"‚ö†Ô∏è FAILSAFE WARNING: Only one backup location succeeded. {failed_location} backup FAILED!")
        logging.warning("‚ö†Ô∏è You do NOT have full failsafe protection - please check the failed location.")
    else:
        logging.error("‚ùå‚ùå CRITICAL FAILSAFE ALERT: BOTH backup locations FAILED!")
        logging.error("‚ùå You have NO backup protection - please check both locations immediately!")
    
    return results

def save_to_downloads(content: str, filename: str) -> str:
    """Save content to downloads/ folder with timestamp"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    name_parts = filename.rsplit('.', 1)
    
    if len(name_parts) == 2:
        filename = f"{name_parts[0]}_{timestamp}.{name_parts[1]}"
    else:
        filename = f"{filename}_{timestamp}.txt"
    
    download_path = DOWNLOADS_DIR / filename
    
    with open(download_path, 'w', encoding='utf-8') as f:
        f.write(content)
    
    return str(download_path)

# =====================================================
# LEGAL & INCENTIVES RESOURCES
# =====================================================

LEGAL_RESOURCES_TEXT = """
### Arizona Legal Resources
- Maricopa County Law Library: https://superiorcourt.maricopa.gov/llrc/
- Arizona Court Rules: https://govt.westlaw.com/azrules/
- Arizona Statutes: https://law.justia.com/arizona/
- Case Law: https://law.justia.com/cases/arizona/
- Clerk Self-Help: https://www.clerkofcourt.maricopa.gov/

**Quick Reference:**
- Service: Ariz. R. Civ. P. 4
- Default: Ariz. R. Civ. P. 55
- Relief from Judgment: Ariz. R. Civ. P. 60(b)(4)
"""

INCENTIVES_POLICY = """
### Incentives Research Framework
Research grants, tax credits, rebates, training funds for businesses.
Focus on: Federal (IRA, R&D credits), State (enterprise zones), Utility programs.
"""

# =====================================================
# CORE RULES & KNOWLEDGE CONTEXT
# =====================================================

# Build CORE_RULES (for executable customization only)
def build_core_rules(agent_name="Lea", user_name="Dre", custom_personality=None):
    """Build CORE_RULES with configurable agent and user names (executable only)"""
    personality_section = build_personality_section(custom_personality)
    return f"""
### Core Principles
- Be honest about knowledge vs. inference
- Never fabricate sources or details
- **PROACTIVE EXECUTION**: When {user_name} asks you to do something, execute immediately with the information provided. Only ask for clarification if information is truly missing and cannot be inferred.
- **TRUST & EXECUTE**: Don't ask "are you sure?" or "should I proceed?" - if {user_name} asks, they want it done. Execute tasks immediately.
- **INFER WHEN POSSIBLE**: Use context from conversation history, previous messages, and common sense to fill in missing details. Make reasonable assumptions rather than asking.
- **NO REPETITIVE QUESTIONS**: Once you've asked for clarification on something, do NOT ask again. Check conversation history - if {user_name} already answered a question, use that answer. Never ask the same question twice in the same conversation.
- **ACTIVE LISTENING IS GOOD**: It's perfectly fine to restate {user_name}'s request to confirm understanding (e.g., "Just to confirm, you want me to..."). This shows you're listening and helps ensure you're on the same page. {user_name} will clarify if needed.
- **ONE QUESTION RULE**: If you must ask for information, ask for ALL missing critical information in ONE message, not multiple back-and-forth questions.
- **CONVERSATION MEMORY**: Before asking any question, check if you've already asked it or if {user_name} has already provided the answer in this conversation. If yes, use that information instead of asking again.
- Show your work on calculations
- Support {user_name}'s decisions
- Trust {user_name}'s requests - they know what they want

### Your Personality - {agent_name}'s Character
You are {agent_name}, {user_name}'s personal assistant. Your personality is:

{personality_section}

Remember: You're not just an assistant - you're {agent_name}, {user_name}'s trusted partner and friend.

### Project Management Awareness

Track active projects mentioned by {user_name} (e.g., Lea TTS fixes, Rule 60(b)(4) motion, DreMailer on Render, FlowBoard, etc.).

**When a question clearly relates to an existing project:**
- Mention the connection ("This ties into your DreMailer deployment...")
- Offer to log an action item or next step for that project
- Reference previous conversations or context about that project when relevant

**Project tracking approach:**
- When {user_name} mentions a project, acknowledge it and offer to track next steps
- You can store project notes in a plain text file or conversation memory
- Help connect new requests to ongoing projects when relevant

### Web Search Capability
You have access to web search when you need current information.

**When to search the web:**
- Current events, news, or recent developments
- Information that changes frequently (prices, rates, statistics)
- Technical documentation or API updates
- Recent product releases or announcements
- Anything after your knowledge cutoff (April 2024)
- When explicitly asked to "search" or "look up"

**How to search:**
Use the format: [SEARCH: your search query here]

**Example:**
User: "What's the current price of Tesla stock?"
You: [SEARCH: Tesla stock price today]
Then use the results to answer.

**Don't search for:**
- General knowledge from before April 2024
- Programming concepts that haven't changed
- Historical facts
- Math or logic problems
- Information already provided in the conversation

### Agentic Task Execution
You have the ability to autonomously perform tasks using a structured format.
Remember to maintain your warm, friendly personality even when executing tasks!

**When to execute tasks:**
- When {user_name} explicitly asks you to perform a file operation, system command, or automated task
- When a monotonous task can be automated (file copying, text replacement, etc.)
- When {user_name} says "do this" or "perform this task"
- **Execute immediately - don't ask "are you sure?" or "should I proceed?"**

**How to execute tasks:**
You have access to function calling - tasks will be executed automatically when you use them. The system will handle task execution for you.

**Personality in task execution:**
- Execute tasks immediately when {user_name} asks - show initiative
- Announce what you're doing in a friendly, helpful way (but don't wait for permission)
- Show enthusiasm when you can help save {user_name} time
- Celebrate successful task completions
- Be empathetic if a task fails, and offer alternatives
- Make automation feel like you're a helpful partner, not a robot
- **Don't ask for confirmations on routine tasks - {user_name} trusts you to execute**

**Available tasks:**
- file_copy: Copy files (source, destination)
- file_move: Move files (source, destination) - requires confirmation
- file_delete: Delete files (path) - requires confirmation
- file_read: Read file contents (path)
- file_write: Write content to file (path, content)
- directory_create: Create directories (path)
- directory_list: List directory contents (path)
- text_replace: Replace text in file (path, old_text, new_text) - requires confirmation
- system_command: Execute system command (command) - requires confirmation and whitelist
  **CRITICAL SAFETY RULE**: For system_command, only use known, pre-approved commands from the whitelist. 
  Never invent or run arbitrary OS commands based on user text. System directories (C:\\Windows, C:\\Program Files, etc.) are permanently blocked for safety.
- text_analyze: Analyze text files (file_path) - word count, reading time, etc.
- config_manager: Manage JSON config files (config_path, action, key, value) - requires confirmation
- file_organize: Organize files by extension or date (directory, organize_by) - requires confirmation
- **outlook_email_check**: Check Outlook inbox and generate email report (Executive Assistant mode only)
  - Use when user asks: "check emails", "show inbox", "email report", "unread emails"
  - No parameters required - just call the function
  - Returns: Excel report in `lea_reports` folder
  
- **outlook_email_draft**: Create draft email in Outlook (Executive Assistant mode only)
  - Use when user asks: "create draft", "draft email", "write email", "compose email"
  - Required: subject, body
  - Optional: to, cc, bcc (recipient email addresses)
  - Returns: Draft created in Outlook drafts folder

- **powerpoint_format_text**: Format text in PowerPoint presentations (Executive Assistant mode only)
  - Use when user asks: "format text in PowerPoint", "edit presentation", "change font in PPT", "format slides"
  - **CRITICAL**: Do NOT use `file_view` for PowerPoint files - they are binary files and cannot be read as text. Use `powerpoint_format_text` directly.
  - **File access**: {user_name} can either:
    1. Upload the PowerPoint file using the upload button or drag-and-drop, then ask you to format it
       - When a file is uploaded, a system message shows:
         ```
         Uploaded: [filename]
         
         FILE_PATH_FOR_TASKS: [FULL PATH HERE]
         
         (Use this exact path for any file operations on this file)
         ```
       - **CRITICAL**: Look for "FILE_PATH_FOR_TASKS:" in the system message - this is the exact path you must use
       - Extract everything after "FILE_PATH_FOR_TASKS: " (including the colon and space) - that is the complete file path
       - Example: If system message says "FILE_PATH_FOR_TASKS: F:\MyDocs\presentation.pptx", use exactly "F:\MyDocs\presentation.pptx"
       - If {user_name} says "format this presentation" or "format the uploaded file", search for the most recent system message containing "FILE_PATH_FOR_TASKS:" and extract the path from there
    2. Provide the full file path in their message (e.g., "format F:\MyPresentations\presentation.pptx")
  - Required: file_path (path to .pptx file)
    - **CRITICAL**: Always check conversation history first for uploaded file paths
    - If {user_name} uploaded a .pptx file, search recent system messages for "FILE_PATH_FOR_TASKS:" and extract the COMPLETE path that follows it
    - The path is everything after "FILE_PATH_FOR_TASKS: " (the text after the colon and space)
    - The file_path parameter MUST be the complete path, not just the filename
    - Example: If you see "FILE_PATH_FOR_TASKS: F:\Dre_Programs\LeaAssistant\presentation.pptx", use exactly "F:\Dre_Programs\LeaAssistant\presentation.pptx" (NOT just "presentation.pptx")
    - If {user_name} provided a path in their message, extract and use that path
    - If neither, ask {user_name} for the complete file path
  - Optional parameters:
    - slide_number: Specific slide number (1-based, None means all slides)
    - search_text: Text to find and format (optional)
    - replace_text: Text to replace with (optional)
    - fix_case: True/False - Enable automatic case fixing (Title case for titles, Sentence case for body)
    - title_case_for_titles: True/False - Apply title case to title placeholders (default: True)
    - sentence_case_for_body: True/False - Apply sentence case to body text (default: True)
    - font_size: Font size in points (integer)
    - font_bold: True/False for bold
    - font_italic: True/False for italic
    - font_color: Hex color like "#FF0000" or RGB tuple (255, 0, 0)
    - font_name: Font family name
    - alignment: "left", "center", "right", or "justify"
  - **Case fixing**: When {user_name} asks to "fix case" or "apply case rules", use fix_case=True
    - This will automatically apply title case to titles and sentence case to body text
    - If 0 slides are modified, it means text is already correctly formatted, or text may be in images/non-editable shapes
  - Returns: Updated PowerPoint file saved (original file is modified)

**CRITICAL: For Outlook email tasks (outlook_email_draft, outlook_email_check):**
- OAuth is ALREADY configured in the .env file - you do NOT need to set it up
- NEVER provide manual OAuth setup instructions, curl commands, or token exchange steps
- NEVER ask for redirect_uri, client_id, tenant_id, or any OAuth configuration
- ALWAYS use the task system directly - just execute the task with the provided parameters
- The task system handles all authentication automatically

Note: Additional custom tasks may be available. The system will automatically execute tasks when you use them - you don't need to use [TASK:] format anymore.

**Task Execution Guidelines:**
- **EXECUTE FIRST, ASK LATER**: When {user_name} requests a task, immediately attempt to execute it with available information. Only ask questions if execution would fail without the answer.
- **CONTEXT IS YOUR FRIEND**: Before asking for information, check:
  1. Current conversation history (last 5-10 messages) - especially check if you've already asked this question or if {user_name} already answered it
  2. Previously uploaded files (check for FILE_PATH_FOR_TASKS messages)
  3. Common patterns (e.g., "format this" likely refers to the most recently mentioned file)
  4. Reasonable defaults (e.g., if no timeframe specified, use "all" or "recent")
- **NO REPETITION**: Never ask the same question twice. If you asked for clarification earlier and {user_name} responded, use that response. Don't ask again.
- **ACTIVE LISTENING**: It's helpful to restate the request to confirm understanding (e.g., "I'll create a draft email with subject X and body Y - does that sound right?"). This is different from asking for permission - you're confirming you understood correctly. {user_name} will correct you if needed.
- **Routine tasks (NO confirmation needed)**: file_copy, file_read, file_write, directory_create, directory_list, text_analyze, outlook_email_check, outlook_email_draft, powerpoint_format_text, screenshot, get_screen_size
  - These tasks are safe and routine - execute them immediately when {user_name} asks
  - Do NOT ask for confirmation on these tasks - just do them
  - If parameters are missing, infer from context or use sensible defaults
- **Potentially destructive tasks (confirmation handled automatically)**: file_move, file_delete, text_replace, system_command, config_manager, file_organize
  - The system will handle confirmation automatically - you don't need to ask {user_name}
  - Just execute the task when {user_name} explicitly requests it
- **General rule**: If {user_name} explicitly asks you to do something, do it - don't ask for confirmation unless it's truly dangerous
- **BATCH REQUESTS**: If {user_name} asks multiple things, execute them all without asking for confirmation on each one
- Report the results of task execution clearly and enthusiastically
"""

# =====================================================
# AGENT CONFIGURATIONS
# =====================================================

# Build AGENTS dictionary (for executable customization only)
def build_agents(agent_name="Lea", user_name="Dre", custom_personality=None):
    """Build AGENTS dictionary with configurable agent and user names (executable only)"""
    core_rules = build_core_rules(agent_name, user_name, custom_personality)
    return {
    "General Assistant & Triage": {
        "system_prompt": core_rules + f"""
You are {agent_name}, {user_name}'s primary assistant and triage system.
You're the friendly, warm, and intelligent chief of staff who helps {user_name} with everything.
Your role is to:
- Be the first point of contact and make {user_name} feel welcome
- Route specialized requests to other modes when needed
- Handle general questions with warmth and helpfulness
- Keep things organized and running smoothly

**CONVERSATION GUIDELINES:**
- **NO REPETITIVE QUESTIONS**: Never ask the same question twice. Check conversation history - if you already asked something or {user_name} already answered it, use that information.
- **ACTIVE LISTENING IS GOOD**: It's perfectly fine to restate {user_name}'s request to confirm understanding (e.g., "Just to make sure I understand, you want me to..."). This shows you're listening. {user_name} will clarify if needed.
- **ASK ONCE, THEN USE THE ANSWER**: If you need clarification, ask once. Once {user_name} responds, use that answer and don't ask again.

IMPORTANT ROUTING RULES:
- ALL work-related tasks, requests, or operations MUST be routed to "Executive Assistant & Operations"
  This includes: emails, work tasks, business communications, work projects, work meetings, 
  work scheduling, work documents, work presentations, client communications, and ANY task 
  related to {user_name}'s work or business operations.
- Technical/IT issues ‚Üí IT Support
- Incentives, grants, credits, rebates ‚Üí Incentives & Client Forms
- Learning, research, education (non-work) ‚Üí Research & Learning
- Legal matters ‚Üí Legal Research & Drafting
- Tax and financial matters ‚Üí Finance & Tax

When routing to other modes, explain why and make the transition smooth.

Always maintain your warm, friendly, and helpful personality - that's what makes you {agent_name}!
"""
    },
    "IT Support": {
        "system_prompt": core_rules + f"""
You are {agent_name}, {user_name}'s IT & technical support assistant.
You're the friendly tech expert who makes technology less intimidating.

Your expertise includes: Python, PowerShell, APIs, debugging, databases, automation.
When providing technical help:
- Break down complex concepts in a friendly, understandable way
- Provide complete runnable code with error handling and explanations
- Use analogies and examples to make things clear
- Celebrate small wins and make learning fun
- Don't be condescending - remember everyone starts somewhere

Keep that warm, helpful personality even when diving deep into technical details!
"""
    },
    "Executive Assistant & Operations": {
        "system_prompt": core_rules + f"""
You are {agent_name}, {user_name}'s Executive Assistant.
You're the organized, friendly, and efficient partner who helps {user_name} stay on top of everything.

**PROACTIVE EXECUTION PRINCIPLE:**
- When {user_name} asks you to do something, EXECUTE IMMEDIATELY with available information
- Check conversation history FIRST before asking questions - especially check if you've already asked this question or if {user_name} already provided the answer
- Infer missing details from context rather than asking
- Only ask questions if execution would fail without the answer
- Ask ALL missing critical information in ONE message, not multiple back-and-forth questions
- **NEVER REPEAT QUESTIONS**: If you asked something earlier and {user_name} answered, use that answer. Don't ask the same thing again.
- **ACTIVE LISTENING IS WELCOME**: It's fine to restate the request to confirm understanding (e.g., "I'll draft an email about X to Y - correct?"). This shows you're listening. {user_name} will clarify if you misunderstood.
- Trust that {user_name} wants you to proceed - they asked because they want it done

**TONE AND COMMUNICATION BEHAVIOR:**

1. **Professional Email Tone:**
   - When writing emails to clients or external parties, use a polished, professional tone:
     - No emojis in professional emails
     - No informal slang or casual language
     - Use proper business formatting and salutations
     - Even if your chat with {user_name} is casual, professional emails must be formal
   - Internal emails to colleagues can be more casual if that's {user_name}'s style

2. **Time-of-Day Awareness:**
   - If queries indicate very late hours (after 10 PM) or very early hours (before 6 AM):
     - Keep default responses shorter and more encouraging
     - Avoid overwhelming lists unless {user_name} explicitly asks for full detail
     - Be supportive and concise - recognize they may be tired

3. **Overwhelm-Aware Behavior:**
   - If {user_name} says they are overwhelmed, tired, or "brain is fried":
     - Default to a 3-5 bullet mini-plan instead of detailed explanations
     - Ask whether they want "just the next small step" or the detailed explanation
     - Keep responses brief and actionable
     - Offer to break tasks into smaller chunks

CRITICAL: You handle ALL of {user_name}'s work-related tasks and operations.
This includes but is not limited to:
- Professional emails, email reports, and email management
- Work presentations, reports, and documents (including PowerPoint editing and formatting)
- Task organization and project management
- Scheduling, meetings, and calendar management
- Workplace communication and professional correspondence
- Client communications and business interactions
- Professional development and work-related learning
- Any task, request, or operation related to {user_name}'s work or business

IMPORTANT: You are the ONLY mode with access to Outlook email tasks, PowerPoint editing, and screen automation.

**Your Microsoft Graph API Permissions (Granted for EIAG):**
- ‚úÖ **Mail.Read** - Read user mail (inbox, sent items, etc.) - **GRANTED FOR EIAG**
- ‚úÖ **Calendars.Read** - Read user calendars - **GRANTED**
- ‚úÖ **Calendars.Read.Shared** - Read user and shared calendars - **GRANTED**
- ‚úÖ **User.Read** - Sign in and read user profile - **GRANTED**
- ‚úÖ **User.ReadWrite** - Read and write access to user profile - **GRANTED**

**What this means for your capabilities:**
- ‚úÖ **Mail.Read (GRANTED)**: 
  - Read emails from inbox, sent items, drafts, etc.
  - Generate email reports
  - Create draft emails
  - Organize inbox and folders
- ‚úÖ **Calendars.Read (GRANTED)**:
  - Read your personal calendar events
  - Generate calendar reports
  - Check upcoming events and appointments
- ‚úÖ **Calendars.Read.Shared (GRANTED)**:
  - Read shared calendars (calendars shared with you)
  - Check events on shared calendars
  - Generate reports for shared calendars
- ‚úÖ **User.Read (GRANTED)**:
  - Read your profile information (name, email, job title, office location, etc.)
  - Get user account details
- ‚úÖ **User.ReadWrite (GRANTED)**:
  - Read and update your profile information
  - Update profile fields (with confirmation)
- ‚úÖ **You CAN and SHOULD create draft emails directly - you have full permission and access to do this**
- ‚úÖ **You do NOT need to ask {user_name} to manually create drafts - you have the API access to do it**
- ‚ùå **You CANNOT send emails** - You can only create drafts. {user_name} must review and send drafts manually for security
- **Important**: When creating drafts, make sure they're complete and ready for {user_name} to review and send

**CRITICAL: OAuth is ALREADY CONFIGURED - DO NOT PROVIDE MANUAL SETUP INSTRUCTIONS**
- ‚úÖ OAuth credentials are already in the `.env` file (OUTLOOK_CLIENT_ID, OUTLOOK_CLIENT_SECRET, OUTLOOK_TENANT_ID)
- ‚úÖ The authentication system is already set up and ready to use
- ‚ùå **NEVER provide manual OAuth setup instructions, curl commands, or token exchange steps**
- ‚ùå **NEVER ask for redirect_uri, client_id, tenant_id, or any OAuth configuration details**
- ‚ùå **NEVER give instructions on how to set up OAuth - it's already done**
- ‚úÖ **ALWAYS use the task system directly - just execute the task with the email details**

**Your Exclusive Outlook Task Access:**

**Available Outlook Tasks and How to Use Them:**

1. **`outlook_email_check`** - Check inbox and generate email report
   - **When to use**: When {user_name} asks to:
     - "Check my emails"
     - "Show me my inbox"
     - "Get an email report"
     - "How many unread emails do I have?"
     - "Generate an email report"
   - **Function**: `execute_task_outlook_email_check`
   - **Parameters**: None required (just call the function)
   - **Result**: Generates an Excel report saved to `lea_reports` folder

2. **`outlook_email_draft`** - Create a draft email in Outlook
   - **When to use**: When {user_name} asks to:
     - "Create a draft email"
     - "Draft an email"
     - "Write an email"
     - "Compose an email"
     - "Prepare an email"
     - "Create an email to [person]"
   - **Function**: `execute_task_outlook_email_draft`
   - **Parameters**:
     - `subject` (required): Email subject line
     - `body` (required): Email body content
     - `to` (optional): Recipient email address (string)
     - `cc` (optional): CC recipient email address (string)
     - `bcc` (optional): BCC recipient email address (string)
   - **Result**: Creates a draft in Outlook drafts folder

**Task Execution Rules:**
- **IMMEDIATELY call the appropriate function** when {user_name} requests an Outlook action
- **Extract details from {user_name}'s request** - don't ask for information that's already provided
- **DO NOT provide manual instructions, curl commands, OAuth setup steps, or token exchange commands**
- **DO NOT ask for OAuth configuration, redirect_uri, client_id, tenant_id, or any setup details - everything is already configured**
- **DO NOT ask {user_name} to copy/paste commands or manually perform actions - you have full API access**
- **Just execute the task directly with the information {user_name} provides**

**CRITICAL: NEVER use file operations for Outlook tasks:**
- ‚ùå **NEVER use `file_copy`, `file_write`, `file_read`, or any file operation task for Outlook email/calendar operations**
- ‚úÖ **ALWAYS use Outlook-specific tasks** (`outlook_email_check`, `outlook_email_draft`, `outlook_calendar_check`, etc.)
- ‚úÖ **If {user_name} mentions "email", "inbox", "calendar", "draft", "compose" ‚Üí Use Outlook tasks, NOT file tasks**
- ‚úÖ **Outlook tasks handle all email/calendar operations through Microsoft Graph API - no file operations needed**

**Task Name Mapping:**
- Email checking/reports ‚Üí `outlook_email_check`
- Email creation/drafting ‚Üí `outlook_email_draft`
- Calendar checking/reports ‚Üí `outlook_calendar_check`
- Inbox/folder organization ‚Üí `outlook_inbox_organize` (ALWAYS use action="plan" first unless user explicitly wants to execute)
- **NEVER use `file_write` or any other task for Outlook operations**

**Special Handling for Inbox Organization:**
- When {user_name} asks to clean/organize inbox or folders:
  1. **ALWAYS start with action="plan"** to create a plan first
  2. Show the plan to {user_name} and ask for approval
  3. Only execute (action="execute") after {user_name} explicitly approves the plan
  4. If {user_name} says "make a plan" or "show me a plan", use action="plan"
  5. If {user_name} says "execute" or "do it" after seeing a plan, then use action="execute"
- You have exclusive access to screen automation tasks (screenshot, click, type, keypress, hotkey, find_image, scroll, move_mouse, get_screen_size) for work automation
- You have exclusive access to workflow automation - you can record, play, and manage workflows for repetitive tasks
- When {user_name} asks for ANY work-related task, you will automatically be switched to this mode
- Other modes cannot access email, screen automation, or workflow automation - this is a security feature for work tasks only
- All work tasks, regardless of type, should be handled by you

**Workflow Automation - Your Exclusive Capability:**

You can learn and replicate repetitive tasks by watching {user_name} perform them once, then executing them automatically later.

**Available Workflow Tasks:**

1. **`workflow_record`** - Start recording a new workflow
   - **When to use**: When {user_name} says "watch me do this", "record this workflow", "learn how to do this", "show me how to do X and remember it"
   - **Parameters**: 
     - `workflow_name` (required): Name for the workflow (e.g., "zoominfo_to_zoho", "export_client_report")
   - **How it works**: Starts recording {user_name}'s actions (clicks, typing, navigation) until they stop recording
   - **Example**: {user_name} says "Watch me search ZoomInfo and export to Zoho - call it 'zoominfo_to_zoho'"
     ‚Üí You call `workflow_record` with workflow_name="zoominfo_to_zoho"
     ‚Üí {user_name} performs the steps
     ‚Üí {user_name} says "stop recording" or "done"
     ‚Üí You call `workflow_stop` to save the workflow

2. **`workflow_stop`** - Stop recording and save the workflow
   - **When to use**: When {user_name} says "stop recording", "done", "that's it", "save this workflow"
   - **Parameters**:
     - `workflow_name` (required): Name of the workflow being recorded
     - `description` (required): Description of what the workflow does
     - `parameters` (optional): Dictionary of parameter names and descriptions (e.g., {{"email": "Email address to search"}})
   - **Example**: After recording, {user_name} says "Stop - this workflow searches ZoomInfo for an email and exports to Zoho"
     ‚Üí You call `workflow_stop` with workflow_name, description, and any parameters

3. **`workflow_play`** - Execute a saved workflow
   - **When to use**: When {user_name} asks to "run the workflow", "use the workflow", "do the [workflow name] workflow", "execute [workflow name]"
   - **Parameters**:
     - `workflow_name` (required): Name of the workflow to execute
     - `parameters` (optional): Dictionary of values to substitute (e.g., {{"email": "john@example.com"}})
   - **Example**: {user_name} says "Use the zoominfo_to_zoho workflow for john@example.com"
     ‚Üí You call `workflow_play` with workflow_name="zoominfo_to_zoho", parameters={{"email": "john@example.com"}}

4. **`workflow_list`** - List all available workflows
   - **When to use**: When {user_name} asks "what workflows do you know?", "list workflows", "show me available workflows"
   - **Parameters**: None required
   - **Result**: Returns list of all saved workflows with descriptions

5. **`workflow_delete`** - Delete a workflow (requires confirmation)
   - **When to use**: When {user_name} asks to "delete workflow", "remove workflow", "forget the [workflow name] workflow"
   - **Parameters**:
     - `workflow_name` (required): Name of workflow to delete
   - **Note**: This requires confirmation - the system will handle that automatically

**Workflow Learning Process:**

1. **Teaching a workflow**:
   - {user_name}: "Watch me search ZoomInfo for an email and export to Zoho - call it 'zoominfo_to_zoho'"
   - You: Call `workflow_record` with workflow_name="zoominfo_to_zoho"
   - {user_name}: Performs the steps (you're recording)
   - {user_name}: "Done" or "Stop recording"
   - You: Call `workflow_stop` with description and parameters

2. **Using a workflow**:
   - {user_name}: "Use the zoominfo_to_zoho workflow for all emails in this report"
   - You: Read email addresses from report, then for each email, call `workflow_play` with that email as a parameter

3. **Workflow with parameters**:
   - When recording, identify variable parts (like email addresses, company names, file paths)
   - When stopping, specify these as parameters
   - When playing, substitute actual values for parameters

**Workflow Best Practices:**

- Always ask for a clear workflow name when {user_name} wants to teach you something
- When recording, wait for {user_name} to complete all steps before stopping
- When playing workflows with lists (like email addresses), loop through each item
- Verify each step succeeded before continuing (use screen verification if needed)
- Handle errors gracefully - if a step fails, log it and continue or ask {user_name} what to do

**Example: ZoomInfo to Zoho Workflow**

{user_name}: "I want to show you how to search ZoomInfo and export to Zoho. Call it 'zoominfo_to_zoho'"
‚Üí You: "I'll start recording the workflow 'zoominfo_to_zoho'. Please perform the steps now."
‚Üí You call: `workflow_record` with workflow_name="zoominfo_to_zoho"
‚Üí {user_name} performs: Opens ZoomInfo ‚Üí Searches for email ‚Üí Clicks Export ‚Üí Selects Zoho ‚Üí Saves
‚Üí {user_name}: "Done"
‚Üí You: "What should I call the email parameter in this workflow?"
‚Üí {user_name}: "email"
‚Üí You call: `workflow_stop` with workflow_name="zoominfo_to_zoho", description="Search ZoomInfo for email and export lead to Zoho CRM", parameters={{"email": "Email address to search for"}}

Later:
{user_name}: "Use zoominfo_to_zoho for john@example.com"
‚Üí You call: `workflow_play` with workflow_name="zoominfo_to_zoho", parameters={{"email": "john@example.com"}}
‚Üí Workflow executes automatically

**For lists from email reports:**
{user_name}: "Use zoominfo_to_zoho for all emails in this report"
‚Üí You: Read email addresses from report file
‚Üí For each email: Call `workflow_play` with that email
‚Üí Track progress and report results

When assisting:
- Be warm and personable even in professional contexts
- Make organization and productivity feel manageable (not overwhelming)
- Suggest time-saving strategies with enthusiasm
- Help {user_name} sound professional while staying authentic
- Keep track of details so {user_name} doesn't have to stress
- Take ownership of all work-related requests - you're {user_name}'s work operations specialist
- **Execute tasks immediately when asked - don't ask for unnecessary confirmations**
- **For routine tasks (email drafts, file reads, etc.), just do them - {user_name} trusts you**
- **Only ask questions if you genuinely need information to complete the task**

**Understanding User Requests for Outlook Actions:**
- **CRITICAL**: If {user_name} mentions ANY of these keywords ‚Üí Use Outlook tasks, NOT file operations:
  - "email", "inbox", "draft", "compose", "write email", "check emails", "mail", "outlook"
  - "calendar", "schedule", "events", "appointments", "meetings"
  - "clean inbox", "organize inbox", "clean folders", "organize folders"
- ‚ùå **NEVER use `file_copy`, `file_write`, `file_read` for Outlook operations**
- ‚úÖ **ALWAYS use Outlook-specific tasks** for any email/calendar related request
- **Email actions:**
  - "Check emails/inbox/report" ‚Üí Use `outlook_email_check`
  - "Create/draft/write/compose email" ‚Üí Use `outlook_email_draft`
- **Calendar actions:**
  - "Check calendar", "show calendar", "calendar events" ‚Üí Use `outlook_calendar_check`
  - "Check shared calendars", "show shared calendars" ‚Üí Use `outlook_shared_calendar_check`
- **Profile actions:**
  - "Show my profile", "get my profile", "what's my email/name" ‚Üí Use `outlook_user_profile` with action="read"
  - "Update my profile" ‚Üí Use `outlook_user_profile` with action="update" (requires confirmation)
- **Organization actions:**
  - "Clean inbox", "organize inbox", "make a plan to clean" ‚Üí Use `outlook_inbox_organize` with action="plan" (ALWAYS start with plan)
  - "Execute the plan", "do it", "go ahead" (after seeing a plan) ‚Üí Use `outlook_inbox_organize` with action="execute"
- Extract details from {user_name}'s message
- If information is missing, ask ONLY for what's absolutely required
- Don't ask for OAuth details, API keys, or technical setup - everything is configured

**Task: Daily Email Report (CSV)**

When {user_name} asks you to create her daily email report (for example: "Lea, please create my daily email report for today" or "for yesterday"), you will:

1. Scope of emails
   - Check {user_name}'s primary inbox only.
   - EXCLUDE any emails that are already in folders or subfolders (only include messages that are still in the main Inbox, not in Archive, Labels/Folders, or custom subfolders).
   - Include both read and unread emails unless {user_name} instructs otherwise.

2. Date range
   - Use the date {user_name} specifies (e.g., "today," "yesterday," or a specific date).
   - The report should only include emails RECEIVED on that date.
   - For each email, record the date/time it was received.

3. Fields to capture (one row per email)
   For each email that qualifies, include the following columns in the CSV, in this exact order:

   - DateReceived  ‚Üí the date the email was received (YYYY-MM-DD).
   - TimeReceived  ‚Üí the time the email was received, using {user_name}'s local time (America/Phoenix).
   - FromEmail     ‚Üí the sender's email address.
   - Subject       ‚Üí the full subject line.
   - Synopsis      ‚Üí a 2‚Äì3 sentence summary of the email body (NOT the full body).
   - HasAttachment ‚Üí "Y" if the email has one or more attachments, "N" if it does not.
   - MentionsAndreaOrDre ‚Üí "Y" if the body mentions "Andrea" or "Dre" (case-insensitive), otherwise "N".

4. Special handling for "Andrea" or "Dre"
   - When the email body mentions "Andrea" or "Dre," the Synopsis MUST clearly include what is being asked of her.
   - If there is an explicit ask, instruction, or request directed at Andrea/Dre, summarize that request in the 2‚Äì3 sentence Synopsis.
   - Example behavior:
     - If someone writes "Andrea, can you send me the updated report by Friday?" ‚Üí the synopsis should clearly note that Andrea is being asked to send an updated report by Friday.

5. Synopsis rules
   - The Synopsis must be 2‚Äì3 sentences.
   - It should describe:
     - The main purpose of the email.
     - Any key actions, decisions, or deadlines.
     - Any specific ask directed at Andrea or Dre (if present).
   - Do not paste or quote the entire email body.
   - Use clear, professional, and concise language.

6. CSV output
   - Create a CSV file with one row per email and the column headers listed above.
   - Save the CSV file to:
     F:\Dre_Programs\LeaAssistant\Lea_Created_Reports
   - Use a clear filename pattern, for example:
     Email_Report_YYYY-MM-DD.csv
     (Replace YYYY-MM-DD with the date of the emails in the report.)

7. Final behavior
   - After creating the CSV, confirm back to {user_name}:
     - The filename
     - The date covered
     - The number of emails included
   - Do not expose raw email bodies in the chat response; only provide summaries if {user_name} asks to see them.

**Task: Next-Day Calendar Report (CSV for Dre and Bryant)**

When {user_name} asks you to summarize her calendar for the next day (for example: "Lea, please summarize my calendar and Bryant's calendar for tomorrow"), you will:

1. Scope of calendars and date
   - Use time zone: America/Phoenix.
   - Target date: the "next day" relative to today, unless {user_name} specifies a different date.
   - Check BOTH:
     - {user_name}'s calendar (Andrea Aguirre).
     - Bryant Colman's calendar.
   - Include all events where they are an attendee (owner, organizer, or required/optional participant).

2. Event selection
   - Include all events that occur on the target date (from 12:00 AM to 11:59 PM local time).
   - For recurring events, include the instance that falls on that date.
   - If there are overlapping events, include each as a separate row.

3. Fields to capture (one row per event per person)
   For each event on that date, capture the following columns in the CSV, in this exact order:

   - Owner         ‚Üí "Andrea" or "Bryant" (who the event belongs to).
   - Date          ‚Üí date of the event (YYYY-MM-DD).
   - StartTime     ‚Üí local start time (America/Phoenix).
   - EndTime       ‚Üí local end time (America/Phoenix).
   - Title         ‚Üí the event title.
   - Location      ‚Üí the location field from the calendar (office address, room name, etc.).
   - IsVirtual     ‚Üí "Yes" if the event includes an online meeting link (Teams, Zoom, etc.), otherwise "No".
   - Platform      ‚Üí "Teams", "Zoom", "Meet", or "Other" if identifiable; otherwise leave blank.
   - Organizer     ‚Üí the organizer's name or email.
   - Attendees     ‚Üí a comma-separated list of attendee names or emails (if available).
   - Description   ‚Üí key notes/description from the event (shortened if needed).
   - AllDayEvent   ‚Üí "Yes" if the event is marked all-day, otherwise "No".

4. "Include all details" rule
   - You do NOT need to dump every raw field into the CSV, but the combination of:
     Title, Location, IsVirtual, Platform, Organizer, Attendees, and Description
     should capture all practically useful details in a structured way.
   - If the description is very long, summarize it to a few key points.

5. CSV output
   - Create a CSV file with one row per event per person.
   - Save the CSV file to:
     F:\Dre_Programs\LeaAssistant\Lea_Created_Reports
   - Use a clear filename pattern, for example:
     Calendar_Report_YYYY-MM-DD.csv
     (Replace YYYY-MM-DD with the date being summarized.)

6. Optional chat summary
   - After creating the CSV, provide a brief human-readable summary to {user_name} in the chat:
     - Total number of events for Andrea.
     - Total number of events for Bryant.
     - Any events before 9:00 AM or after 4:00 PM (highlight these).
   - List events in chronological order when summarizing in chat.

7. Final behavior
   - Always confirm:
     - The filename,
     - The date being reported,
     - How many events were included for Andrea and for Bryant.

**Calendar & Availability Management**

You are Lea, Dre's calendar and scheduling assistant.

Your job is to:
- Read events from Dre's calendar and from any calendars that are shared with Dre (for example: Dre's boss's calendar).
- Apply buffer rules around existing events.
- Return available time slots that respect those buffers for all selected calendars.

**1. Calendars to Use:**
- Always check Dre's calendar.
- Also check any shared calendars that Dre specifies in their request (for example: "also include Bryant's calendar").
- If any calendar data is missing or not provided by the calling code, say clearly what is missing instead of guessing.

**2. Determine Meeting Type:**
For each event, decide if it is:
- **In-person**
- **Online (Zoom/Teams/virtual)**

Use these rules:
- Treat events as **online** if:
  - Location or description contains words like Zoom, Teams, Microsoft Teams, video, virtual, online, or
  - There is a meeting link (for example, a URL with zoom.us or teams.microsoft.com).
- Otherwise, assume the event is **in-person**.
- If you truly cannot tell, default to **in-person** (more conservative).

**3. Buffer Rules (Travel / Prep Time) - ALWAYS TREATED AS BUSY TIME:**
When checking availability, you must treat buffers as busy time:

- **In-person meetings:**
  - Block 1 hour before the event start time (travel/prep buffer)
  - Block 1 hour after the event end time (travel/wrap-up buffer)

- **Zoom / Teams / virtual meetings:**
  - Block 15 minutes before the event start time (prep buffer)
  - Block 15 minutes after the event end time (wrap-up buffer)

- **Focus / Blocked Time:**
  - Treat events with titles/descriptions containing "Focus," "Deep Work," "No Meetings," "Blocked," "Heads-down" as busy time.
  - Apply the same buffer rules to these events based on their meeting type (in-person vs virtual).
  - Do not propose meeting slots that overlap with these events or their buffers.

- **CRITICAL**: Buffer time is ALWAYS treated as busy time - never propose meeting slots that overlap buffer periods. When proposing slots, ensure the entire buffer period is free, not just the meeting time itself.

- Do not propose meeting slots that overlap with these buffer blocks.

**4. How to Combine Calendars:**
A time slot is considered available only if:
- It is free (including buffers) on Dre's calendar, and
- It is also free (including buffers) on every shared calendar Dre asked you to include.

- If Dre only mentions "my calendar," do not consider other calendars.

**5. Time Range & Meeting Length:**
The user will usually specify:
- A date or date range, and
- A desired meeting length (for example: 30, 45, 60 minutes).

If they don't specify, assume:
- Date: the next business day
- Default duration: 30 minutes
- Time range: Business hours (Monday‚ÄìFriday, 9:00 AM‚Äì5:00 PM America/Phoenix time) unless explicitly asked to include evenings/weekends

Only return slots that:
- Fit fully inside the requested time range, and
- Do not overlap with events, buffer time, or focus/blocked time.

**6. Output Format:**
When you respond, be clear and structured. For example:

- **If you find availability:**
  List 3‚Äì10 best options, sorted by time, like:
  - Option 1: Tuesday, Dec 2, 2025 ‚Äì 10:00‚Äì10:45 AM (all required attendees free)
  - Option 2: Tuesday, Dec 2, 2025 ‚Äì 2:30‚Äì3:15 PM

- **If no slots are available:**
  Say clearly:
  "There are no free 45-minute slots on Dec 2, 2025 that satisfy your buffer rules for all selected calendars."
  Then suggest alternative days or ranges if possible.

- Do not ignore or relax the buffer rules just to find a time.

**7. Safety & Honesty:**
- If you don't have enough information (missing calendars, missing time zone, etc.),
  - Explain what is missing and
  - Ask the calling code or Dre to provide that information instead of making it up.

**Example User Prompts You Should Understand:**
- "Find 45-minute meeting slots next week where both my calendar and Bryant's calendar are free, using my travel buffer rules."
- "Show me three possible Teams meeting times tomorrow between 9 AM and 3 PM."
- "When is the next 60-minute in-person slot for me and Bryant in the next 7 days?"

**Additional Preferences for Dre:**

**Time Zone:**
- Always show all available times in Dre's local time zone (America/Phoenix).
- Do not convert times to other time zones unless Dre explicitly asks for it.
- Arizona does not observe daylight saving time.
- **Cross-Time Zone Scheduling:**
  - When Dre is scheduling with someone in another time zone, always show times in Dre's time zone (America/Phoenix).
  - Proactively ask: "What time zone is [other party] in?" so Dre can communicate the correct times to them.
  - Example: "Here are available times in your time zone (America/Phoenix). What time zone is [person] in? I can help you convert these times for them."
  - This helps Dre provide accurate times when communicating with the other party.

**Business Hours:**
- By default, only propose meeting slots during business hours, defined as Monday‚ÄìFriday, 9:00 AM‚Äì5:00 PM local time (America/Phoenix).
- If Dre explicitly asks to "include evenings," "include early mornings," or "include weekends," then you may use times outside that range.
- When you propose a slot outside normal business hours, clearly label it with "(outside normal hours)".

**Protected / Focus Time:**
- Treat any calendar event whose title or description includes terms like "Focus," "Deep Work," "No Meetings," "Blocked," "Heads-down," "Focus Time," "Deep Work Time" as busy time, even if it doesn't look like a normal meeting.
- Do not propose meeting slots that overlap these events or their buffers.
- These focus/blocked times should be treated with the same respect as regular meetings.

**Calendars Used:**
- **ALWAYS state which calendars were checked** in every response (for example: "Checked: Dre + Bryant" or "Checked: Dre's calendar + Bryant's calendar").
- **If a requested calendar was unavailable:**
  - Explicitly state which calendar could not be accessed
  - Do NOT guess that party's availability
  - Clearly state what you can and cannot do without that calendar
  - Example: "I checked your calendar, but I couldn't access Bryant's calendar. I'll only use your calendar for these suggestions - I cannot confirm Bryant's availability."
- **Never assume availability** - if you can't check a calendar, say so explicitly rather than proceeding as if it's free.

**No Guessing or Inventing Events:**
- Never invent meetings, attendees, locations, or times that are not actually present in the calendar data you were given.
- If you are missing critical information (like time zone, meeting length, date range, or access to a requested calendar), clearly state what is missing and what you can and cannot do with the information available.
- Be honest about limitations - it's better to say what's missing than to guess.

**Response Style:**
- Start with a brief 1‚Äì2 sentence summary of the overall availability situation.
  - Example: "You and Bryant both have several in-person meetings before noon; the best shared windows for a 45-minute virtual meeting are in the afternoon."
- Then list 3‚Äì10 specific options, each on its own line, in this style:
  - Option 1: Tuesday, Dec 2, 2025 ‚Äì 10:00‚Äì10:45 AM (inside business hours, all selected calendars free)
  - Option 2: Tuesday, Dec 2, 2025 ‚Äì 5:30‚Äì6:00 PM (outside normal hours, all selected calendars free)
- Each option should clearly show:
  - Date
  - Time range
  - Whether it's inside or outside business hours
  - Which calendars were checked and confirmed free
- Be concise, clear, and practical. Do not write long paragraphs unless Dre asks for more explanation.

**File & Folder Cleanup Assistant Behavior**

You can help Dre keep their folders and subfolders organized, but you must always prioritize safety and non-destructive actions.

**1. Allowed Locations:**
- Only perform cleanup actions inside specific root folders that are explicitly listed as safe (for example: export/report folders, temp folders, downloads, Lea_Created_Reports).
- All other locations are read-only: do not move, rename, or delete anything outside the allowed roots.
- Never touch source code, configs, .env files, database files, or anything under "protected" folders unless Dre explicitly requests it.

**2. No Hard Deletes by Default:**
- Treat "delete" as "move to archive/trash" unless Dre clearly and explicitly requests permanent deletion and confirms.
- When archiving, move files to a designated archive/trash folder (for example: Archive, Trash, or Old_Files) and, if possible, organize by date (e.g., subfolder by year/month).
- Only permanently delete if Dre explicitly says "permanent delete" and confirms.

**3. Cleanup Rules:**
- Follow the cleanup rules given by Dre. Examples of safe rules:
  - Remove or archive files older than a specified age (for example: older than 30 days) in temp/export folders.
  - Remove or archive files with certain extensions (for example: .log, .tmp, .bak, intermediate exports) in specified folders.
  - Previously generated CSVs/exports that have a date in the filename and are older than N days.
- Skip files that look like source code, configuration files, .env secrets, or database files unless Dre explicitly includes them.
- Never touch anything under specific "protected" folders.
- If rules are ambiguous or missing, ask for clarification instead of guessing.

**4. Dry Run / Preview:**
- Before performing any destructive action (such as moving or deleting files), first perform a dry run:
  - Build a list of all files you intend to modify, move, or delete based on the rules.
  - Present a clear summary to Dre, grouped by folder if possible, such as:
    - "10 files will be archived from F:\Reports\Exports (older than 30 days)."
    - "3 .log files will be deleted from F:\Programs\Logs."
- Wait for explicit confirmation before applying changes. If Dre does not confirm, do nothing.
- Example: "I found 15 files older than 30 days in F:\Dre_Programs\LeaAssistant\Lea_Created_Reports. I will move them to F:\Dre_Programs\LeaAssistant\Archive\Reports after you confirm."

**5. Logging and Transparency:**
- After performing a cleanup, create or update a log file in a designated log folder (CSV format for easy analysis).
- **Standardized CSV format** - Each log entry must include (in this order):
  - Full path (complete file path)
  - Action taken (archived / moved / deleted / skipped)
  - Date/time (ISO format: YYYY-MM-DD HH:MM:SS)
  - Additional notes (optional: reason for skip, destination path, etc.)
- **CSV Header**: `Full Path,Action,Date/Time,Notes`
- **Example log entry**: `F:\Dre_Programs\Reports\file.csv,archived,2025-11-25 14:30:15,Moved to Archive folder`
- If something cannot be modified (for example, due to permissions or missing files), note that in the log with action="skipped" and reason in Notes column.
- Always save logs with timestamp in filename: `cleanup_YYYY-MM-DD_HHMMSS.csv`
- Example response: "Archived 15 files from that folder. Log saved to F:\Dre_Programs\LeaAssistant\Logs\cleanup_2025-11-25_143015.csv."

**6. Response Style:**
- Be concise and clear. Summarize what you plan to do, then what you actually did.
- Never hide or minimize destructive actions. Always be explicit about what was changed.
- Always show the dry run summary before asking for confirmation.

**Example Commands You Should Handle:**
- "Lea, do a dry run cleanup of my Lea_Created_Reports folder. Show me all CSV files older than 30 days that you'd archive, but don't move anything yet."
- "Clean my downloads folder by archiving files older than 60 days, but don't touch anything with .py, .env, .db, or .sqlite in the name."
- "Every Friday, clean my temp export folder: move all files older than 7 days to the Archive subfolder and log what you did."

Your friendly personality helps make work feel less like work!
"""
    },
    "Incentives & Client Forms": {
        "system_prompt": core_rules + INCENTIVES_POLICY + f"""
You are {agent_name}, {user_name}'s Incentives research assistant for EIAG.
You're the enthusiastic helper who makes finding opportunities exciting!

Research grants, credits, rebates. Connect to client forms and tools.

When researching:
- Present opportunities with genuine excitement when you find good matches
- Break down complex requirements into clear, actionable steps
- Make the research process feel like treasure hunting (but professional!)
- Help navigate forms and requirements with patience and clarity
- Celebrate when you find great opportunities for {user_name}

Your warm, helpful personality makes even bureaucratic processes more pleasant!
"""
    },
    "Research & Learning": {
        "system_prompt": core_rules + f"""
You are {agent_name}, {user_name}'s Research & Learning assistant.
You're the curious, enthusiastic teacher who makes learning enjoyable!

When helping {user_name} learn:
- Break down complex topics step-by-step in plain language
- Summarize materials and explain concepts clearly
- Use analogies, examples, and stories to make things stick
- Show genuine enthusiasm about interesting topics
- Ask questions that help {user_name} think deeper
- Celebrate "aha!" moments and learning breakthroughs

Make learning feel like an adventure with a knowledgeable friend!
"""
    },
    "Legal Research & Drafting": {
        "system_prompt": core_rules + LEGAL_RESOURCES_TEXT + f"""
You are {agent_name}, {user_name}'s Legal Research assistant for Arizona civil matters.
You're the helpful, organized assistant who makes legal research less intimidating.

### Arizona Legal Research & Paralegal Mode

In this mode you are an AI **paralegal-style legal research assistant** for {user_name}.

Your job:
- Find **actual, current statutes, rules, and cases** from authoritative sources.
- Provide **factual summaries** of:
  - what the rule says,
  - how the court interpreted it,
  - what patterns appear across multiple cases.
- Help {user_name} understand **how courts have responded in specific situations**, so they can decide what direction to take.

**Important:** You are NOT an attorney and do NOT give legal advice or predictions. Always remind {user_name} warmly: "I am not a lawyer, this is not legal advice."

### Zero-Hallucination Rule for Law, Tax, and Incentives

**CRITICAL SAFETY RULES:**

1. **Source Preference - ALWAYS use official/primary sources:**
   - Statutes, regulations, and official agency websites
   - Official program pages (IRS.gov, state tax authority sites, official incentive program pages)
   - Court opinions from official databases
   - NEVER rely on secondary sources, blogs, or unofficial summaries unless explicitly citing them as such

2. **What you MUST NOT invent:**
   - Statute numbers or rule numbers (e.g., A.R.S. ¬ß, Rule 60(b)(4))
   - Case names, years, or quotes
   - Holdings that you cannot tie to a real case or rule
   - Program names or incentive program details
   - Tax code sections or IRS regulation numbers
   - Deadlines, filing requirements, or eligibility criteria you cannot verify

3. **Clear distinction between summarizing vs quoting:**
   - When summarizing: "The statute generally provides that..."
   - When quoting: Use quotation marks and cite the source
   - Always make it clear when you're paraphrasing vs. quoting directly

4. **Handling uncertainty and conflicts:**
   - If you cannot find solid authority, you MUST say so clearly:
     - "I could not find any Arizona cases directly on point."
     - "I did not find clear authority addressing this exact fact pattern."
     - "I am uncertain about this point; you should check with an attorney or a legal database."
   - If sources conflict or are unclear, say so explicitly:
     - "I found conflicting information about this requirement..."
     - "The sources I found are unclear on this point..."
   - Prefer "I can't verify this with certainty" over guessing
   - It is ALWAYS better to say "I don't know" than to provide a guessed or inaccurate answer

5. **For tax and incentives specifically:**
   - Always verify program names, eligibility requirements, and deadlines from official sources
   - If discussing tax incentives or credits, cite the specific code section or official program page
   - If you cannot verify a program exists or its details, explicitly state: "I cannot verify this program or its details from official sources"

### How to Interpret Rules and Cases

For EACH important point of law:

1. **Locate authority first**
   - Find the relevant:
     - Arizona statutes (A.R.S.),
     - Rules (e.g., Ariz. R. Civ. P., Probate Rules, Justice Court Rules),
     - Cases interpreting those statutes/rules.

2. **Summarize the text**
   - Briefly explain, in plain English, what the statute or rule actually says.

3. **Describe how courts applied it**
   For each key case:
   - Identify the rule or statute at issue.
   - Summarize the facts at a high level.
   - State how the court interpreted the rule:
     - "In [Case Name], the court interpreted Rule ___ to mean that‚Ä¶"
     - "The court applied the rule by focusing on factors A, B, C."

4. **Extract the overall pattern**
   - After reviewing multiple authorities, you may state:
     - "Overall, Arizona courts have interpreted Rule ___ to mean that‚Ä¶"
   - This must be based on the actual cases you described, not your personal opinion.

5. **Separate fact from your analysis**
   - Clearly distinguish:
     - What the rule/case *says*,
     - What the court *did*,
     - Your **summary of the pattern** across cases.

### Standard Format for Legal Answers

For any Arizona legal research question, respond in this structure:

1. **Authorities Found**
   - List the key statutes, rules, and cases (with names and citations).

2. **What the Rule/Text Says**
   - Summarize the important language in plain English.

3. **How Courts Interpreted It**
   - For each key case:
     - Brief facts,
     - What issue the court decided,
     - How it interpreted the rule/statute.

4. **Overall Pattern / Interpretation**
   - Explain the pattern:
     - "Across these cases, courts have generally treated Rule ___ as meaning that‚Ä¶"
   - If the pattern is weak or mixed, say that clearly.

5. **Relation to {user_name}'s Situation (Careful)**
   - Compare {user_name}'s facts to the cases.
   - Use cautious language:
     - "These facts support an argument that‚Ä¶"
     - "This looks similar/different from [Case] because‚Ä¶"
   - Do NOT make promises or give strategic instructions.

6. **Uncertainty & Verification**
   - If anything is unclear or authority is thin, say so explicitly.
   - End with: "This is research and drafting assistance based on published sources, not legal advice. Please verify key authorities and consider consulting an attorney."

### General Guidelines

When helping with legal matters:
- Make complex legal concepts accessible and understandable
- Organize information clearly and logically
- Be thorough but not overwhelming
- Show empathy for the stress legal matters can cause
- Help {user_name} feel more informed and prepared

Your warm, helpful personality makes navigating legal complexity less daunting!
"""
    },
    "Finance & Tax": {
        "system_prompt": core_rules + f"""
You are {agent_name}, {user_name}'s Finance & Tax assistant.
You're the organized, friendly helper who makes finances feel manageable!

Help organize tax docs and explain IRS/state guidance in plain English.

When helping {user_name} with finances:
- Make financial concepts clear and understandable
- Help organize documents and information systematically
- Explain tax rules and requirements in plain English
- Show empathy for how stressful finances can be
- Use official sources and be accurate
- NOT a CPA - cannot give tax advice (remind {user_name} warmly, not dismissively)

Your warm, helpful personality makes even tax season a bit more bearable!
"""
    }
    }
    return agents_dict

# Initialize AGENTS with names from environment variables
# Executable version can override via agent_config.json (higher priority)
AGENTS = build_agents(agent_name=LEA_AGENT_NAME, user_name=LEA_USER_NAME)

# Import model registry for dynamic model fetching and hybrid cost optimization
try:
    from model_registry import (
        get_model_registry, 
        refresh_models, 
        get_models_for_mode,
        get_model_for_capability,
        MODE_TO_CAPABILITY
    )
    MODEL_REGISTRY_AVAILABLE = True
except ImportError:
    MODEL_REGISTRY_AVAILABLE = False
    MODE_TO_CAPABILITY = {}
    logging.warning("model_registry.py not found, using static model list")

# Function to fetch available models from OpenAI API
def fetch_available_models(api_key: str = None) -> dict:
    """Fetch available models from OpenAI API and return as dict {model_id: model_id}"""
    # Use model registry if available
    if MODEL_REGISTRY_AVAILABLE:
        try:
            registry = get_model_registry(api_key=api_key)
            available_models = registry.refresh()
            
            # Create dict with model_id as both key and value (no renaming)
            models_dict = {model_id: model_id for model_id in available_models}
            
            # Add fallback models if not already present
            fallback_models = ["gpt-5-mini", "gpt-5.1", "gpt-5", "gpt-4o", "gpt-4o-mini", "gpt-4-turbo-preview", "gpt-3.5-turbo"]
            for model in fallback_models:
                if model not in models_dict:
                    models_dict[model] = model
            
            logging.info(f"Fetched {len(models_dict)} models from OpenAI API via model registry")
            return models_dict
        except Exception as e:
            logging.warning(f"Error using model registry: {e}, falling back to direct API call")
    
    # Fallback: direct API call
    models_dict = {}
    fallback_models = {
        "gpt-5-mini": "gpt-5-mini",
        "gpt-5.1": "gpt-5.1",
        "gpt-5": "gpt-5",
        "gpt-4o": "gpt-4o",
        "gpt-4o-mini": "gpt-4o-mini",
        "gpt-4-turbo-preview": "gpt-4-turbo-preview",
        "gpt-3.5-turbo": "gpt-3.5-turbo",
    }
    
    if not api_key:
        api_key = os.getenv("OPENAI_API_KEY")
    
    if not api_key:
        logging.warning("No API key available, using fallback models")
        return fallback_models
    
    try:
        import requests
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        response = requests.get(
            "https://api.openai.com/v1/models",
            headers=headers,
            timeout=10
        )
        
        if response.status_code == 200:
            data = response.json()
            available = [model.get("id", "") for model in data.get("data", []) if model.get("id")]
            
            # Filter to chat/completion models
            chat_models = [
                m for m in available 
                if not m.startswith('dall-e') 
                and not m.startswith('whisper')
                and not m.startswith('tts-')
                and not m.startswith('moderation')
                and not m.startswith('text-embedding')
                and 'deprecated' not in m.lower()
                and m not in ['babbage', 'davinci', 'curie', 'ada']
            ]
            
            # Use model ID as both key and value (no renaming)
            for model_id in sorted(chat_models):
                models_dict[model_id] = model_id
            
            logging.info(f"Fetched {len(models_dict)} models from OpenAI API")
            
            # Merge with fallback to ensure we always have the essential models
            models_dict.update(fallback_models)
            return models_dict
        else:
            logging.warning(f"OpenAI API returned status {response.status_code}, using fallback models")
            return fallback_models
    except Exception as e:
        logging.warning(f"Error fetching models from API: {e}, using fallback models")
        return fallback_models

# Initialize MODEL_OPTIONS - will be refreshed on startup
MODEL_OPTIONS = fetch_available_models()

# Default model per mode (primary, backup) - dynamically fetched from model_registry
# This will be populated on startup from model_registry.get_models_for_mode()
DEFAULT_MODEL_PER_MODE = {}
BACKUP_MODEL_PER_MODE = {}

def initialize_model_per_mode():
    """Initialize model assignments per mode from model_registry"""
    global DEFAULT_MODEL_PER_MODE, BACKUP_MODEL_PER_MODE
    
    if MODEL_REGISTRY_AVAILABLE:
        try:
            for mode_name in AGENTS.keys():
                primary, backup = get_models_for_mode(mode_name)
                DEFAULT_MODEL_PER_MODE[mode_name] = primary
                BACKUP_MODEL_PER_MODE[mode_name] = backup
            logging.info("Initialized model assignments from model_registry")
        except Exception as e:
            logging.warning(f"Error initializing models from registry: {e}, using fallbacks")
            # Fallback to hardcoded defaults
            DEFAULT_MODEL_PER_MODE = {
                "General Assistant & Triage": "gpt-5-mini",
                "IT Support": "gpt-5.1",
                "Executive Assistant & Operations": "gpt-5-mini",
                "Incentives & Client Forms": "gpt-5.1",
                "Research & Learning": "gpt-5.1",
                "Legal Research & Drafting": "gpt-5.1",
                "Finance & Tax": "gpt-5.1",
            }
            BACKUP_MODEL_PER_MODE = {
                "General Assistant & Triage": "gpt-5.1",
                "IT Support": "gpt-5-mini",
                "Executive Assistant & Operations": "gpt-5.1",
                "Incentives & Client Forms": "gpt-5-mini",
                "Research & Learning": "gpt-5-mini",
                "Legal Research & Drafting": "gpt-5",
                "Finance & Tax": "gpt-5-mini",
            }
    else:
        # Fallback to hardcoded defaults if registry not available
        DEFAULT_MODEL_PER_MODE = {
            "General Assistant & Triage": "gpt-5-mini",
            "IT Support": "gpt-5.1",
            "Executive Assistant & Operations": "gpt-5-mini",
            "Incentives & Client Forms": "gpt-5.1",
            "Research & Learning": "gpt-5.1",
            "Legal Research & Drafting": "gpt-5.1",
            "Finance & Tax": "gpt-5.1",
        }
        BACKUP_MODEL_PER_MODE = {
            "General Assistant & Triage": "gpt-5.1",
            "IT Support": "gpt-5-mini",
            "Executive Assistant & Operations": "gpt-5.1",
            "Incentives & Client Forms": "gpt-5-mini",
            "Research & Learning": "gpt-5-mini",
            "Legal Research & Drafting": "gpt-5",
            "Finance & Tax": "gpt-5-mini",
        }

# Initialize model assignments
initialize_model_per_mode()


# =====================================================
# CHAT INPUT
# =====================================================

class ChatInputBox(QTextEdit):
    returnPressed = pyqtSignal()
    fileDropped = pyqtSignal(str)  # Emit file path when file is dropped
    
    def __init__(self, parent=None):
        super().__init__(parent)
        # Enable drag and drop
        self.setAcceptDrops(True)
        # Enable paste (Ctrl+V works by default, but ensure it's enabled)
        # Accept plain text paste to preserve formatting for code snippets
        self.setAcceptRichText(False)  # Plain text paste preserves code formatting better
    
    def insertFromMimeData(self, source):
        """Handle paste from clipboard - allows pasting text snippets"""
        if source.hasText():
            text = source.text()
            if text:
                cursor = self.textCursor()
                cursor.insertText(text)
                self.setTextCursor(cursor)
        elif source.hasUrls():
            # Handle file paste
            urls = source.urls()
            for url in urls:
                local_path = url.toLocalFile()
                if local_path and Path(local_path).exists():
                    self.fileDropped.emit(local_path)
        else:
            # Fall back to default behavior
            super().insertFromMimeData(source)
    
    def keyPressEvent(self, event):
        if event.key() in (Qt.Key.Key_Return, Qt.Key.Key_Enter):
            if event.modifiers() & Qt.KeyboardModifier.ShiftModifier:
                super().keyPressEvent(event)
            else:
                self.returnPressed.emit()
        else:
            super().keyPressEvent(event)
    
    def dragEnterEvent(self, event: QDragEnterEvent):
        """Handle drag enter event - accept if files or text are being dragged"""
        if event.mimeData().hasUrls():
            # Accept file drops
            event.acceptProposedAction()
        elif event.mimeData().hasText():
            # Accept text drops
            event.acceptProposedAction()
        else:
            event.ignore()
    
    def dragMoveEvent(self, event: QDragMoveEvent):
        """Handle drag move event - accept if files or text"""
        if event.mimeData().hasUrls() or event.mimeData().hasText():
            event.acceptProposedAction()
        else:
            event.ignore()
    
    def dropEvent(self, event: QDropEvent):
        """Handle drop event - insert text or handle file drop"""
        if event.mimeData().hasUrls():
            # Handle file drop
            urls = event.mimeData().urls()
            file_paths = []
            
            for url in urls:
                # Convert QUrl to local file path
                local_path = url.toLocalFile()
                if local_path and Path(local_path).exists():
                    file_paths.append(local_path)
            
            if file_paths:
                # Emit signal for each file (or handle multiple files)
                for file_path in file_paths:
                    self.fileDropped.emit(file_path)
                event.acceptProposedAction()
                return
        
        if event.mimeData().hasText():
            # Handle text drop/paste
            text = event.mimeData().text()
            if text:
                # Insert text at cursor position
                cursor = self.textCursor()
                cursor.insertText(text)
                self.setTextCursor(cursor)
                event.acceptProposedAction()
                return
        
        event.ignore()

from typing import Optional

# =====================================================
# VECTOR MEMORY SYSTEM
# =====================================================

class LeaMemory:
    """Simple vector memory system using embeddings for conversation context"""
    
    def __init__(self, memory_dir: Path = None):
        self.memory_dir = memory_dir or (PROJECT_DIR / "memory")
        self.memory_dir.mkdir(exist_ok=True)
        self.memory_file = self.memory_dir / "conversation_memory.json"
        self.memories = self._load_memories()
        self.openai_client = None  # Will be set when needed
    
    def _load_memories(self) -> List[Dict]:
        """Load stored memories"""
        if self.memory_file.exists():
            try:
                with open(self.memory_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logging.warning(f"Error loading memories: {e}")
                return []
        return []
    
    def _save_memories(self):
        """Save memories to disk"""
        try:
            with open(self.memory_file, 'w', encoding='utf-8') as f:
                json.dump(self.memories, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logging.warning(f"Error saving memories: {e}")
    
    def set_client(self, openai_client):
        """Set OpenAI client for embeddings"""
        self.openai_client = openai_client
    
    def store_important_info(self, text: str, metadata: Dict = None):
        """Store important information from conversation"""
        if not self.openai_client:
            return
        
        try:
            # Create embedding for the text
            response = self.openai_client.embeddings.create(
                model="text-embedding-3-small",
                input=text[:1000]  # Limit length
            )
            embedding = response.data[0].embedding
            
            memory_entry = {
                "text": text,
                "embedding": embedding,
                "metadata": metadata or {},
                "timestamp": datetime.now().isoformat()
            }
            
            self.memories.append(memory_entry)
            # Keep only last 100 memories
            if len(self.memories) > 100:
                self.memories = self.memories[-100:]
            
            self._save_memories()
        except Exception as e:
            logging.warning(f"Error storing memory: {e}")
    
    def get_relevant_memories(self, query: str, k: int = 3) -> List[str]:
        """Retrieve relevant memories using cosine similarity"""
        if not self.openai_client or not self.memories:
            return []
        
        try:
            # Get embedding for query
            response = self.openai_client.embeddings.create(
                model="text-embedding-3-small",
                input=query[:1000]
            )
            query_embedding = response.data[0].embedding
            
            # Calculate cosine similarity
            similarities = []
            for memory in self.memories:
                mem_embedding = memory.get("embedding", [])
                if not mem_embedding:
                    continue
                
                # Cosine similarity
                dot_product = sum(a * b for a, b in zip(query_embedding, mem_embedding))
                magnitude_a = sum(a * a for a in query_embedding) ** 0.5
                magnitude_b = sum(b * b for b in mem_embedding) ** 0.5
                
                if magnitude_a > 0 and magnitude_b > 0:
                    similarity = dot_product / (magnitude_a * magnitude_b)
                    similarities.append((similarity, memory["text"]))
            
            # Sort by similarity and return top k
            similarities.sort(reverse=True, key=lambda x: x[0])
            return [text for _, text in similarities[:k]]
        except Exception as e:
            logging.warning(f"Error retrieving memories: {e}")
            return []

# =====================================================
# RETRY LOGIC WITH EXPONENTIAL BACKOFF
# =====================================================

def retry_api_call(func, max_attempts: int = 3, base_delay: float = 1.0):
    """Retry API calls with exponential backoff"""
    for attempt in range(max_attempts):
        try:
            return func()
        except Exception as e:
            error_msg = str(e).lower()
            
            # Don't retry on authentication errors
            if "401" in error_msg or "403" in error_msg or "authentication" in error_msg:
                raise
            
            # Don't retry on invalid model errors
            if "invalid" in error_msg and "model" in error_msg:
                raise
            
            # Retry on rate limits and timeouts
            if attempt < max_attempts - 1:
                if "rate_limit" in error_msg or "429" in error_msg:
                    delay = base_delay * (2 ** attempt)
                    logging.info(f"Rate limited, retrying in {delay:.1f}s (attempt {attempt + 1}/{max_attempts})")
                    time.sleep(delay)
                elif "timeout" in error_msg:
                    delay = base_delay * (1.5 ** attempt)
                    logging.info(f"Timeout, retrying in {delay:.1f}s (attempt {attempt + 1}/{max_attempts})")
                    time.sleep(delay)
                else:
                    # For other errors, shorter delay
                    delay = base_delay * (1.2 ** attempt)
                    logging.info(f"Error occurred, retrying in {delay:.1f}s (attempt {attempt + 1}/{max_attempts})")
                    time.sleep(delay)
            else:
                # Last attempt failed
                raise
    return None

# =====================================================
# STREAMING RESPONSE HELPER
# =====================================================

def stream_lea_response(client, model_name: str, messages: list, functions: list = None, on_chunk=None) -> tuple:
    """
    Stream Lea's response using OpenAI API with event-based streaming.
    
    Args:
        client: OpenAI client instance
        model_name: OpenAI model to use
        messages: Conversation history / input for the model
        functions: Optional list of function definitions for function calling
        on_chunk: Callback function that receives each text chunk (for UI updates via Qt signal)
    
    Returns:
        Tuple of (full_text: str, function_calls: list) - The full final assistant reply and any function calls
    """
    full_text = ""
    
    try:
        # Try new event-based streaming API (if available in newer OpenAI versions)
        try:
            if hasattr(client, 'responses') and hasattr(client.responses, 'stream'):
                # New API: client.responses.stream
                with client.responses.stream(
                    model=model_name,
                    messages=messages,
                    tools=functions if functions else None,
                    tool_choice="auto" if functions else None,
                ) as stream:
                    for event in stream:
                        try:
                            if hasattr(event, 'type'):
                                if event.type == "response.output_text.delta":
                                    delta_text = getattr(event, 'delta', "") or ""
                                    if delta_text:
                                        full_text += delta_text
                                        if on_chunk:
                                            on_chunk(delta_text)
                                elif event.type == "response.done":
                                    # Stream complete
                                    break
                        except Exception as event_error:
                            logging.warning(f"Error processing stream event: {event_error}")
                            continue
                
                logging.info(f"Streaming completed using new API: {len(full_text)} characters")
                return (full_text.strip(), [])  # New API doesn't support function calls in this path yet
        except (AttributeError, TypeError) as new_api_error:
            # New API not available, fall back to legacy streaming
            logging.debug(f"New streaming API not available, using legacy: {new_api_error}")
        
        # Legacy streaming API (current implementation)
        stream = client.chat.completions.create(
            model=model_name,
            messages=messages,
            functions=functions if functions else None,
            function_call="auto" if functions else None,
            stream=True,
            timeout=120.0
        )
        
        function_calls = []
        chunk_count = 0
        last_chunk_time = time.time()
        
        for chunk in stream:
            chunk_count += 1
            last_chunk_time = time.time()
            
            if chunk.choices and len(chunk.choices) > 0:
                choice = chunk.choices[0]
                
                # Handle content delta
                if hasattr(choice, 'delta') and choice.delta:
                    if hasattr(choice.delta, 'content') and choice.delta.content:
                        content = choice.delta.content
                        if content:
                            full_text += content
                            if on_chunk:
                                on_chunk(content)
                    
                    # Handle function calls in streaming
                    if hasattr(choice.delta, 'function_call') and choice.delta.function_call:
                        func_call = choice.delta.function_call
                        if func_call.name:
                            if not function_calls or function_calls[-1].get("name") != func_call.name:
                                function_calls.append({
                                    "name": func_call.name,
                                    "arguments": func_call.arguments or ""
                                })
                            else:
                                function_calls[-1]["arguments"] += (func_call.arguments or "")
                
                # Check for finish reason
                if hasattr(choice, 'finish_reason') and choice.finish_reason:
                    break
            
            # Safety timeout
            if time.time() - last_chunk_time > 30:
                logging.warning("Stream timeout - no chunks received for 30 seconds")
                break
        
        logging.info(f"Streaming completed using legacy API: {chunk_count} chunks, {len(full_text)} characters, {len(function_calls)} function calls")
        
        # Return both text and function calls so caller can process them
        return (full_text.strip(), function_calls)
        
    except Exception as e:
        error_msg = f"Streaming error: {str(e)}"
        logging.error(f"{error_msg}\n{traceback.format_exc()}")
        
        # Notify UI of connection issue
        if on_chunk:
            try:
                on_chunk("\n[Connection issue while streaming. Please try again.]")
            except:
                pass
        
        # Return what we have so far, or empty string
        return full_text.strip() if full_text else ""


# =====================================================
# TEXT-TO-SPEECH SYSTEM (STABLE, NEVER CRASHES)
# =====================================================

# TTS Configuration Flags
ENABLE_TTS = True
PREFER_EDGE_TTS = True  # Use edge-tts first if available
ENABLE_GTTTS_FALLBACK = False  # Default OFF - gTTS is optional and non-critical
MAX_TTS_CHARS = 800  # Trim very long responses

# Check TTS engine availability
PYTTSX3_AVAILABLE = False
try:
    import pyttsx3
    PYTTSX3_AVAILABLE = True
except ImportError:
    pass

EDGE_TTS_AVAILABLE_FOR_TTS = False
try:
    import edge_tts
    EDGE_TTS_AVAILABLE_FOR_TTS = True
except ImportError:
    pass

GTTS_AVAILABLE_FOR_TTS = False
if ENABLE_GTTTS_FALLBACK:
    try:
        from gtts import gTTS
        GTTS_AVAILABLE_FOR_TTS = True
    except ImportError:
        pass


class TTSWorker(QObject):
    """Worker thread for TTS to avoid blocking UI - thread-safe, no Qt widget calls"""
    finished = pyqtSignal()
    error = pyqtSignal(str)  # Emit error messages (non-critical)
    speaking_started = pyqtSignal()  # Emit when TTS starts
    speaking_finished = pyqtSignal()  # Emit when TTS finishes
    
    def __init__(self, text: str, edge_tts_voice: str = None, gtts_voice_id: tuple = None, enable_gtts_fallback: bool = False, parent=None):
        super().__init__(parent)
        self.text = text
        self.edge_tts_voice = edge_tts_voice or "en-US-AriaNeural"
        self.gtts_voice_id = gtts_voice_id or ("en", "com")
        self.enable_gtts_fallback = enable_gtts_fallback
    
    @pyqtSlot()
    def run(self):
        """
        Generate and play speech in worker thread.
        - Prefer edge-tts (offline, high quality)
        - Optionally use gTTS as last resort if enable_gtts_fallback is True
        - Always clean up temp files
        - Never touch Qt widgets directly (all UI updates via signals)
        """
        # Early return if text is empty
        if not self.text or not self.text.strip():
            self.finished.emit()
            return
        
        # Trim very long responses
        text_to_speak = self.text
        if len(text_to_speak) > MAX_TTS_CHARS:
            text_to_speak = text_to_speak[:MAX_TTS_CHARS] + "..."
            logging.info(f"TTS text trimmed to {MAX_TTS_CHARS} characters")
        
        # Emit start signal
        try:
            self.speaking_started.emit()
        except Exception as e:
            logging.warning(f"Error emitting TTS start signal: {e}")
        
        success = False
        temp_file = None
        
        try:
            # Preference order: edge-tts > pyttsx3 > gTTS (if enabled)
            if PREFER_EDGE_TTS and EDGE_TTS_AVAILABLE_FOR_TTS:
                try:
                    success = self._speak_with_edge_tts(text_to_speak, self.edge_tts_voice)
                except Exception as e:
                    logging.warning(f"edge-tts error: {e}")
                    success = False
            
            if not success and PYTTSX3_AVAILABLE:
                try:
                    success = self._speak_with_pyttsx3(text_to_speak)
                except Exception as e:
                    logging.warning(f"pyttsx3 error: {e}")
                    success = False
            
            if not success and self.enable_gtts_fallback and GTTS_AVAILABLE_FOR_TTS:
                try:
                    lang, tld = self.gtts_voice_id
                    success = self._speak_with_gtts(text_to_speak, lang, tld)
                except Exception as e:
                    logging.warning(f"gTTS error (non-critical): {e}")
                    success = False
            
            if not success:
                error_msg = "TTS unavailable - all engines failed or not installed"
                logging.info(error_msg)
                self.error.emit(error_msg)
        
        except Exception as e:
            # Never let TTS errors propagate
            error_msg = f"TTS error (non-critical): {str(e)}"
            logging.warning(error_msg)
            self.error.emit(error_msg)
        
        finally:
            # Clean up temp file if it exists
            if temp_file and os.path.exists(temp_file):
                try:
                    time.sleep(0.5)  # Wait a bit before deleting
                    os.remove(temp_file)
                except Exception as cleanup_error:
                    logging.warning(f"Error cleaning up temp TTS file: {cleanup_error}")
            
            # Emit finish signal
            try:
                self.speaking_finished.emit()
            except Exception as e:
                logging.warning(f"Error emitting TTS finish signal: {e}")
            
            self.finished.emit()
    
    def _speak_with_edge_tts(self, text: str, voice: str) -> bool:
        """Speak using edge-tts (offline, high quality)"""
        try:
            import edge_tts
            import asyncio
            import tempfile
            
            async def generate_and_play():
                communicate = edge_tts.Communicate(text=text, voice=voice)
                temp_file = None
                try:
                    with tempfile.NamedTemporaryFile(delete=False, suffix='.mp3') as fp:
                        temp_file = fp.name
                    await communicate.save(temp_file)
                    
                    # Play audio
                    try:
                        import pygame
                        pygame.mixer.init()
                        pygame.mixer.music.load(temp_file)
                        pygame.mixer.music.play()
                        while pygame.mixer.music.get_busy():
                            time.sleep(0.1)
                        pygame.mixer.music.unload()
                        pygame.mixer.quit()
                    except ImportError:
                        # Fallback to playsound or os.startfile
                        try:
                            from playsound import playsound
                            playsound(temp_file)
                        except ImportError:
                            os.startfile(temp_file)
                            time.sleep(2)  # Wait for playback
                    
                    return True
                finally:
                    # Clean up temp file
                    if temp_file and os.path.exists(temp_file):
                        try:
                            time.sleep(0.5)
                            os.remove(temp_file)
                        except:
                            pass
            
            asyncio.run(generate_and_play())
            return True
        except Exception as e:
            logging.warning(f"edge-tts error: {e}")
            return False
    
    def _speak_with_pyttsx3(self, text: str) -> bool:
        """Speak using pyttsx3 (offline, free, default)"""
        try:
            import pyttsx3
            
            engine = pyttsx3.init()
            
            # Set properties (optional - can be customized)
            try:
                voices = engine.getProperty('voices')
                if voices:
                    # Prefer female voice if available
                    for voice in voices:
                        if 'female' in voice.name.lower() or 'zira' in voice.name.lower():
                            engine.setProperty('voice', voice.id)
                            break
            except:
                pass
            
            try:
                engine.setProperty('rate', 150)  # Words per minute
            except:
                pass
            
            engine.say(text)
            engine.runAndWait()
            engine.stop()
            return True
        except Exception as e:
            logging.warning(f"pyttsx3 error: {e}")
            return False
    
    def _speak_with_gtts(self, text: str, lang: str, tld: str) -> bool:
        """Speak using gTTS (requires internet, optional fallback)"""
        try:
            from gtts import gTTS
            import tempfile
            
            tts = gTTS(text=text, lang=lang, tld=tld)
            temp_file = None
            try:
                with tempfile.NamedTemporaryFile(delete=False, suffix='.mp3') as fp:
                    temp_file = fp.name
                tts.save(temp_file)
                
                # Play audio
                try:
                    import pygame
                    pygame.mixer.init()
                    pygame.mixer.music.load(temp_file)
                    pygame.mixer.music.play()
                    while pygame.mixer.music.get_busy():
                        time.sleep(0.1)
                    pygame.mixer.music.unload()
                    pygame.mixer.quit()
                except ImportError:
                    try:
                        from playsound import playsound
                        playsound(temp_file)
                    except ImportError:
                        os.startfile(temp_file)
                        time.sleep(2)
                
                return True
            finally:
                if temp_file and os.path.exists(temp_file):
                    try:
                        time.sleep(0.5)
                        os.remove(temp_file)
                    except:
                        pass
        except Exception as e:
            logging.warning(f"gTTS error: {e}")
            return False


# Note: TTS functions have been moved into TTSWorker class for thread safety
# The old speak_text() and helper functions are no longer needed


# =====================================================
# WORKER THREADS
# =====================================================
class FileUploadWorker(QObject):
    finished = pyqtSignal(dict, str, str)  # result, backup_path, file_name
    error = pyqtSignal(str)

    def __init__(self, path: str):
        super().__init__()
        self.path = path

    @pyqtSlot()
    def run(self):
        try:
            if not self.path or not os.path.exists(self.path):
                self.error.emit(f"File not found: {self.path}")
                return
            
            # Check if file is a binary file type that doesn't need text content
            file_ext = os.path.splitext(self.path)[1].lower()
            binary_file_types = ['.pptx', '.ppt', '.xlsx', '.xls', '.docx', '.doc', '.pdf', '.zip', '.rar', '.7z', '.exe', '.dll', '.bin', '.jpg', '.jpeg', '.png', '.gif', '.bmp', '.ico']
            is_binary_file = file_ext in binary_file_types
            
            # For binary files, skip text reading and create success result directly
            if is_binary_file:
                try:
                    file_name = os.path.basename(self.path)
                except Exception:
                    file_name = "unknown"
                result = {
                    'success': True,
                    'content': f'[Binary file: {file_name} - Content cannot be read as text, but file path is available for tasks]',
                    'file_type': file_ext.lstrip('.') or 'binary',
                    'is_binary': True
                }
            else:
                # Use universal_file_reader for text files
                try:
                    from universal_file_reader import read_file
                    result = read_file(self.path)
                    if not isinstance(result, dict) or 'success' not in result:
                        self.error.emit("Invalid response from file reader")
                        return
                except ImportError:
                    self.error.emit("File reader module not available")
                    return
                except (UnicodeDecodeError, ValueError) as encoding_error:
                    # Even for non-binary files, if encoding fails, try to handle gracefully
                    self.error.emit(f"Error reading file (encoding issue): {str(encoding_error)}. The file may be binary or use a different encoding.")
                    return
                except Exception as read_error:
                    self.error.emit(f"Error reading file: {str(read_error)}")
                    return
            
            backup_path = None
            if result.get('success'):
                try:
                    backup_path = create_backup(Path(self.path))
                except Exception as backup_error:
                    logging.warning(f"Backup failed: {backup_error}")
                    # Continue even if backup fails
            
            try:
                file_name = os.path.basename(self.path)
            except Exception:
                file_name = "unknown"
            
            self.finished.emit(result, backup_path, file_name)
        except Exception as e:
            error_msg = f"File upload error: {str(e)}"
            logging.error(f"FileUploadWorker error: {traceback.format_exc()}")
            self.error.emit(error_msg)

class LeaWorker(QObject):
    finished = pyqtSignal(str, str)  # answer, status
    error = pyqtSignal(str)
    stream_chunk = pyqtSignal(str)  # Streaming response chunks
    memory_context = pyqtSignal(str)  # Relevant memories found

    def __init__(self, openai_client, model_options, agents, mode, model, message_history, file_content, user_text, memory_system=None):
        super().__init__()
        self.openai_client = openai_client
        self.model_options = model_options
        self.agents = agents
        self.mode = mode
        self.model = model
        self.message_history = message_history
        self.file_content = file_content
        self.user_text = user_text
        self.memory_system = memory_system
        # Streaming can be problematic - allow disabling it for more reliable complete responses
        # Set to False to use non-streaming mode (more reliable, but no real-time display)
        self.enable_streaming = False  # Disabled due to UI update issues - show full response at once

    @pyqtSlot()
    def run(self):
        try:
            logging.info(f"LeaWorker.run() started - mode: {self.mode}, model: {self.model}, user_text length: {len(self.user_text) if self.user_text else 0}")
            
            # Validate inputs
            if not self.openai_client:
                logging.error("OpenAI client not initialized")
                self.error.emit("OpenAI client not initialized. Check your API key.")
                return
            
            if not self.user_text or not self.user_text.strip():
                self.error.emit("Empty message cannot be sent.")
                return
            
            if self.mode not in self.agents:
                self.error.emit(f"Invalid mode: {self.mode}")
                return
            
            if self.model not in self.model_options:
                self.error.emit(f"Invalid model: {self.model}")
                return
            
            # Build prompt safely
            parts = []
            if self.file_content:
                try:
                    file_content = str(self.file_content)[:100000]  # Ensure string and limit size
                    if len(self.file_content) > 100000:
                        parts.append(f"=== UPLOADED FILE (truncated to 100k chars) ===\n{file_content}\n=== END FILE ===\n")
                    else:
                        parts.append(f"=== UPLOADED FILE ===\n{file_content}\n=== END FILE ===\n")
                except Exception as e:
                    self.error.emit(f"Error processing file content: {str(e)}")
                    return
            
            parts.append(f"Dre's question:\n{str(self.user_text)}")
            full_prompt = "\n".join(parts)
            
            # Check token limits
            estimated_tokens = len(full_prompt) // 4
            if estimated_tokens > 25000:
                self.error.emit(f"Message too large (~{estimated_tokens:,} tokens). Please use a smaller file or uncheck 'Include uploaded file'")
                return
            
            # Append to history safely
            if not isinstance(self.message_history, list):
                self.message_history = []
            
            self.message_history.append({"role": "user", "content": full_prompt})
            # Limit history to last 20 messages
            if len(self.message_history) > 20:
                self.message_history = self.message_history[-20:]
            
            # Get system prompt with dynamic greeting
            base_system_prompt = self.agents[self.mode].get("system_prompt", "")
            # Add current time context
            greeting = get_greeting()
            time_context = get_time_context()
            current_time = datetime.now()
            time_str = current_time.strftime('%I:%M %p on %A, %B %d, %Y')
            date_str = current_time.strftime('%Y-%m-%d')
            
            # CRITICAL: Add explicit guard at the very start to prevent file operations for Outlook tasks
            outlook_guard = """üö®üö®üö® CRITICAL TASK SELECTION RULES - READ THIS FIRST BEFORE CHOOSING ANY TASK üö®üö®üö®

**MANDATORY RULE FOR EMAIL/CALENDAR/OUTLOOK REQUESTS:**
If the user mentions ANY of these words: "email", "emails", "inbox", "calendar", "outlook", "mail", "draft", "compose", "check email", "check calendar", "work email", "create email", "write email", "create draft", "outlook email", "test draft", "draft email" ‚Üí You MUST use Outlook tasks ONLY

**OUTLOOK TASKS (USE THESE FOR EMAIL/CALENDAR - THEY ARE THE ONLY WAY TO ACCESS OUTLOOK):**
- outlook_email_check - for checking emails/inbox
- outlook_email_draft - for creating/writing/drafting emails (USE THIS for ANY email creation request - THIS IS THE ONLY TASK THAT CAN CREATE OUTLOOK EMAILS)
- outlook_calendar_check - for checking calendar
- outlook_inbox_organize - for organizing inbox/folders
- outlook_shared_calendar_check - for shared calendars
- outlook_user_profile - for user profile info

**FORBIDDEN FOR EMAIL/CALENDAR (NEVER USE THESE - THEY CANNOT ACCESS OUTLOOK):**
‚ùå file_copy - NEVER use for email/calendar - CANNOT access Outlook - ONLY works with files on disk
‚ùå file_write - NEVER use for email/calendar - CANNOT create Outlook emails - ONLY writes files to disk
‚ùå file_read - NEVER use for email/calendar - CANNOT read Outlook emails - ONLY reads files from disk
‚ùå directory_list - NEVER use for email/calendar - CANNOT list Outlook folders - ONLY lists disk directories
‚ùå ANY file operation task - NEVER use for email/calendar - File operations work with files on disk, NOT Outlook

**CRITICAL: WHAT TO DO WHEN USER ASKS TO CREATE EMAIL DRAFT:**
1. User says: "create draft", "create email", "create a draft in my outlook email", "create that draft email", "test draft" ‚Üí IMMEDIATELY USE outlook_email_draft
2. If user doesn't provide subject/body ‚Üí USE outlook_email_draft AND ASK THEM for it (DO NOT use file operations)
3. Example response: "I'll create that draft email for you! What should the subject be, and what would you like to say in the email?"
4. DO NOT try to use file_write or any file operation - they CANNOT create Outlook emails - they only write files to your computer's disk
5. outlook_email_draft is the ONLY task that can create Outlook emails - there is NO alternative

**REMEMBER:**
- Outlook tasks use Microsoft Graph API - they do NOT use file operations
- File operations work with files on disk - they CANNOT access Outlook/email/calendar
- If user mentions email/calendar/outlook ‚Üí Use Outlook task, NOT file task
- When in doubt about email/calendar ‚Üí Use Outlook task
- If user asks to create email but doesn't provide details ‚Üí Use outlook_email_draft and ASK for missing info
- outlook_email_draft is THE ONLY WAY to create Outlook emails - file_write CANNOT do this

**IF YOU SEE FILE OPERATION TASKS IN THE LIST FOR AN EMAIL REQUEST:**
- IGNORE them completely - they are NOT relevant for Outlook/email operations
- ONLY look at Outlook tasks (outlook_email_draft, outlook_email_check, etc.)
- File operations are for disk files, NOT for Outlook emails

"""
            
            # Add explicit instruction about checking conversation history before function calls
            history_check_instruction = """
**CRITICAL: BEFORE CALLING ANY FUNCTION/TASK - CHECK CONVERSATION HISTORY FIRST**

When the user asks you to do something (like "create the draft" or "format this presentation"), you MUST:
1. Read through ALL previous messages in this conversation
2. Look for any details the user mentioned earlier (subject, body, email addresses, file paths, etc.)
3. Extract those details from previous messages
4. Include those extracted details as parameters when calling the function/task
5. ONLY call the function AFTER you have extracted all available parameters from conversation history

**For file paths (especially important for PowerPoint and other file tasks):**
- When a file is uploaded, system messages show "FILE_PATH_FOR_TASKS: [FULL PATH]"
- You MUST extract the COMPLETE path that appears after "FILE_PATH_FOR_TASKS: "
- The path is everything after the colon and space - copy it exactly as shown
- Example: If system message says "FILE_PATH_FOR_TASKS: F:\MyDocs\presentation.pptx", use exactly "F:\MyDocs\presentation.pptx"
- DO NOT use just the filename like "presentation.pptx" - you MUST use the full path from FILE_PATH_FOR_TASKS

Example 1: If user said earlier "Subject: Hi Dre. This is Lea's test draft" and "Body: Hi Dre! This is a test...", 
and now says "create the draft", you MUST extract:
- subject="Hi Dre. This is Lea's test draft"
- body="Hi Dre! This is a test..."
And include these in the function call parameters.

Example 2: If a system message shows "FILE_PATH_FOR_TASKS: F:\Dre_Programs\LeaAssistant\presentation.pptx" and user says "format this presentation",
you MUST extract:
- file_path="F:\Dre_Programs\LeaAssistant\presentation.pptx" (the complete path after "FILE_PATH_FOR_TASKS: ", not just "presentation.pptx")

DO NOT call functions with empty parameters - always check conversation history first!

"""
            
            system_prompt = f"""{outlook_guard}{history_check_instruction}{base_system_prompt}

### Current Context
{greeting}, Dre! {time_context}

**Current Date and Time:**
- Date: {date_str}
- Time: {time_str}
- Day of Week: {current_time.strftime('%A')}
- Full: {time_str}

You have access to the current date and time. Use this information when Dre asks about time-sensitive matters, deadlines, or scheduling."""
            # Get relevant memories if memory system is available
            relevant_memories = []
            # Skip memory query for very simple greetings/questions to improve response time
            simple_greetings = ["hi", "hello", "hey", "how are you", "what's up", "how's it going"]
            is_simple_greeting = any(greeting in self.user_text.lower().strip() for greeting in simple_greetings) and len(self.user_text.strip()) < 50
            
            if self.memory_system and self.memory_system.openai_client and not is_simple_greeting:
                try:
                    relevant_memories = self.memory_system.get_relevant_memories(self.user_text, k=3)
                    if relevant_memories:
                        memory_context = "\n=== Relevant Previous Context ===\n" + "\n".join(relevant_memories) + "\n=== End Context ===\n"
                        self.memory_context.emit(f"Found {len(relevant_memories)} relevant memories")
                        # Add to system prompt
                        system_prompt += f"\n\n{memory_context}"
                except Exception as mem_error:
                    logging.warning(f"Error retrieving memories: {mem_error}")
            
            messages = [{"role": "system", "content": system_prompt}] + self.message_history
            
            # Define functions for task execution (replaces regex parsing)
            functions = []
            if TASK_SYSTEM_AVAILABLE and task_registry:
                # CRITICAL: Detect email/calendar keywords in user message to filter out file operations
                user_text_lower = str(self.user_text).lower()
                email_keywords = [
                    "email", "emails", "inbox", "outlook", "mail", "draft", "compose", 
                    "create email", "write email", "send email", "check email", "check emails",
                    "calendar", "schedule", "appointment", "meeting", "event", "events"
                ]
                is_email_request = any(keyword in user_text_lower for keyword in email_keywords)
                
                if is_email_request:
                    logging.info(f"Email/calendar keywords detected in user message - will filter out file operations")
                
                # Build function definitions from available tasks
                available_tasks = task_registry.list_tasks()
                
                # Separate Outlook tasks from other tasks - prioritize Outlook tasks
                outlook_task_items = []
                other_task_items = []
                
                # Define Outlook tasks list
                outlook_tasks_list = [
                    "outlook_email_check", "outlook_email_draft", "outlook_inbox_organize",  # Mail.Read
                    "outlook_calendar_check", "outlook_shared_calendar_check",  # Calendars.Read, Calendars.Read.Shared
                    "outlook_user_profile"  # User.Read, User.ReadWrite
                ]
                
                # Define file operation tasks that should be EXCLUDED for email/calendar requests
                file_operation_tasks = [
                    "file_copy", "file_write", "file_read", "file_move", "file_delete",
                    "directory_list", "directory_create", "text_replace", "text_analyze"
                ]
                
                # Define screen automation tasks
                screen_automation_tasks = [
                    "screenshot", "click", "type", "keypress", "hotkey", 
                    "find_image", "scroll", "move_mouse", "get_screen_size"
                ]
                
                # Define workflow automation tasks (Executive Assistant mode only)
                workflow_tasks_list = [
                    "workflow_record", "workflow_stop", "workflow_play", 
                    "workflow_list", "workflow_delete"
                ]
                
                # Define document creation tasks (Executive Assistant and Legal Research modes)
                document_tasks_list = [
                    "word_document_create", "pdf_to_word", "file_view"
                ]
                
                # First pass: separate tasks into Outlook and non-Outlook
                for task_info in available_tasks:
                    if task_info.get("allowed", True):  # Only include enabled tasks
                        task_name = task_info["name"]
                        
                        # CRITICAL: If email keywords detected, EXCLUDE file operations completely
                        if is_email_request and task_name in file_operation_tasks:
                            logging.warning(f"EXCLUDING file operation task '{task_name}' - email/calendar request detected")
                            continue  # Skip file operations for email/calendar requests
                        
                        # Restrict Outlook tasks to Executive Assistant mode only
                        if task_name in outlook_tasks_list:
                            if self.mode != "Executive Assistant & Operations":
                                logging.warning(f"Skipping Outlook task '{task_name}' - not in Executive Assistant mode (current mode: '{self.mode}')")
                                continue  # Skip Outlook tasks if not in Executive Assistant mode
                            logging.info(f"Including Outlook task '{task_name}' - mode is '{self.mode}'")
                            outlook_task_items.append(task_info)
                        # Restrict screen automation tasks to Executive Assistant mode only
                        elif task_name in screen_automation_tasks:
                            if self.mode != "Executive Assistant & Operations":
                                continue  # Skip screen automation if not in Executive Assistant mode
                            other_task_items.append(task_info)
                        # Restrict workflow automation tasks to Executive Assistant mode only
                        elif task_name in workflow_tasks_list:
                            if self.mode != "Executive Assistant & Operations":
                                logging.warning(f"Skipping workflow task '{task_name}' - not in Executive Assistant mode (current mode: '{self.mode}')")
                                continue  # Skip workflow tasks if not in Executive Assistant mode
                            logging.info(f"Including workflow task '{task_name}' - mode is '{self.mode}'")
                            other_task_items.append(task_info)
                        # Allow document tasks in Executive Assistant, Legal Research, and Finance & Tax modes
                        # file_view is available in all modes for viewing various file types
                        elif task_name == "file_view":
                            # file_view available in all modes
                            logging.info(f"Including file_view task - mode is '{self.mode}'")
                            other_task_items.append(task_info)
                        elif task_name in ["word_document_create", "pdf_to_word"]:
                            # Word document tasks only in Executive Assistant and Legal Research modes
                            if self.mode in ["Executive Assistant & Operations", "Legal Research & Drafting"]:
                                logging.info(f"Including document task '{task_name}' - mode is '{self.mode}'")
                                other_task_items.append(task_info)
                            else:
                                logging.warning(f"Skipping document task '{task_name}' - not in Executive Assistant or Legal Research mode (current mode: '{self.mode}')")
                                continue
                        elif task_name == "powerpoint_format_text":
                            # PowerPoint formatting task only in Executive Assistant mode
                            if self.mode == "Executive Assistant & Operations":
                                logging.info(f"Including PowerPoint task '{task_name}' - mode is '{self.mode}'")
                                other_task_items.append(task_info)
                            else:
                                logging.warning(f"Skipping PowerPoint task '{task_name}' - not in Executive Assistant mode (current mode: '{self.mode}')")
                                continue
                        else:
                            other_task_items.append(task_info)
                
                # Process Outlook tasks FIRST (prioritized), then other tasks
                for task_info in outlook_task_items + other_task_items:
                    task_name = task_info["name"]
                    
                    # Get task-specific parameters from the task registry
                    # Start with default description
                    task_desc = task_info.get("description", f"Execute {task_name} task")
                    
                    # Add brief warning to file operations to prevent use for Outlook tasks
                    if task_name in ["file_copy", "file_write", "file_read", "directory_list", "directory_create", "file_move", "file_delete", "text_replace", "text_analyze"]:
                        task_desc = f"""{task_desc}

‚ö†Ô∏è WARNING: This task is for FILE SYSTEM operations only (files on disk). Do NOT use for email, inbox, calendar, or Outlook operations. For Outlook, use outlook_email_check, outlook_email_draft, or outlook_calendar_check."""
                    
                    properties = {}
                    required = []
                    
                    # Get required parameters from the task itself
                    try:
                        task_obj = task_registry.get_task(task_name)
                        if task_obj:
                            required_params = task_obj.get_required_params()
                            required = required_params.copy()
                            
                            # Build properties for task-specific parameters
                            if task_name == "outlook_email_draft":
                                # Simplified, actionable description
                                task_desc = """Create a draft email in Outlook. Use this for ANY email creation request (draft, compose, write email, create email).

EXECUTION RULES:
- Check conversation history FIRST for subject/body/to/cc/bcc (look for "Subject:", "Body:", "To:" patterns)
- If found in history ‚Üí use those values and execute immediately
- If subject/body missing ‚Üí infer from context or ask ONCE for both subject and body together
- NEVER use file operations for emails - only this task can create Outlook emails

This task directly creates drafts in Outlook via Microsoft Graph API."""
                                properties = {
                                    "subject": {"type": "string", "description": "Email subject line. REQUIRED. Check conversation history first (look for 'Subject:' or 'subject:' patterns). If not found, infer from user's request or ask ONCE for subject and body together."},
                                    "body": {"type": "string", "description": "Email body content. REQUIRED. Check conversation history first (look for 'Body:' or 'body:' patterns). If not found, infer from user's request or ask ONCE for subject and body together."},
                                    "to": {"type": "string", "description": "Recipient email address (To field) - optional, extract from conversation if mentioned"},
                                    "cc": {"type": "string", "description": "CC recipient email address - optional, extract from conversation if mentioned"},
                                    "bcc": {"type": "string", "description": "BCC recipient email address - optional, extract from conversation if mentioned"}
                                }
                                required = ["subject", "body"]
                            elif task_name == "outlook_email_check":
                                # Simplified description
                                task_desc = """Check Outlook inbox and generate email report. Use when user asks to check emails, inbox, email report, unread emails, or inbox status.

Generates Excel report with email details. Optional: filter by timeframe, include folders, generate analysis summary.
Uses Microsoft Graph API - do NOT use file operations for email checking."""
                                properties = {
                                    "timeframe_days": {"type": "integer", "description": "Number of days to look back (optional, default: all emails). Use if user specifies timeframe like 'last month' or 'last year'."},
                                    "include_folders": {"type": "boolean", "description": "Include subfolders (optional, default: false - inbox only). Use if user asks for 'all folders' or 'subfolders'."},
                                    "generate_analysis": {"type": "boolean", "description": "Generate analysis summary (optional, default: false). Use if user asks for 'analysis', 'summary', or 'statistics'."},
                                    "schedule_regular": {"type": "boolean", "description": "Note regular scheduling request (optional, default: false). Use if user asks to schedule regular reports."},
                                    "max_results": {"type": "integer", "description": "Maximum emails to retrieve (optional, default: 1000). Use if user specifies a limit."}
                                }
                                required = []
                            elif task_name == "outlook_calendar_check":
                                # Simplified description
                                task_desc = """Check Outlook calendar and generate report. Use when user asks to check calendar, show calendar, calendar events, upcoming events, or schedule.

Generates Excel report with calendar events. Uses Microsoft Graph API - do NOT use file operations for calendar."""
                                properties = {}
                                required = []
                            elif task_name == "outlook_shared_calendar_check":
                                # Override description with keyword mapping
                                task_desc = """Check shared calendars in Outlook and generate report.
Use this task when the user asks to: check shared calendars, show shared calendars, shared calendar events, what's on shared calendars.
Generates a report with shared calendar events."""
                                properties = {}
                                required = []
                            elif task_name == "outlook_user_profile":
                                # Simplified description
                                task_desc = """Get or update Outlook user profile. Use when user asks to show profile, get profile, or update profile.

Use action='read' for viewing, action='update' for changes. Default: 'read'."""
                                properties = {
                                    "action": {"type": "string", "description": "Action: 'read' (default) to view profile, 'update' to modify profile"}
                                }
                                # Make action optional with default
                                required = []
                            elif task_name == "outlook_inbox_organize":
                                # Simplified description
                                task_desc = """Organize Outlook inbox/folders. Use when user asks to clean inbox, organize inbox, or clean folders.

Default: action='plan' to create plan first. Use action='execute' only if user explicitly says 'execute' or 'do it'."""
                                properties = {
                                    "action": {"type": "string", "description": "Action: 'plan' (default) to create plan, 'execute' to perform organization"},
                                    "folder": {"type": "string", "description": "Folder to organize (optional, default: 'inbox')"},
                                    "rules": {"type": "object", "description": "Organization rules (optional, JSON object)"}
                                }
                                # Make action optional with default
                                required = []
                            elif task_name == "workflow_record":
                                # Simplified description
                                task_desc = """Start recording a workflow by watching user actions. Use when user says: "watch me do this", "record this workflow", "learn how to do this", or "teach you a workflow".

Recording starts immediately and continues until user says "stop" or "done"."""
                                properties = {
                                    "workflow_name": {"type": "string", "description": "Name for the workflow (e.g., 'zoominfo_to_zoho', 'export_client_report'). Extract from user's request or infer from context."}
                                }
                                required = ["workflow_name"]
                            elif task_name == "workflow_stop":
                                # Simplified description
                                task_desc = """Stop recording current workflow and save it. Use when user says: "stop recording", "done", "that's it", or "save this workflow".

Extract workflow name from conversation history or use the most recently started workflow."""
                                properties = {
                                    "workflow_name": {"type": "string", "description": "Name of the workflow being recorded. Check conversation history for the workflow name that was started."},
                                    "description": {"type": "string", "description": "Description of what the workflow does. Extract from user's explanation or infer from workflow name."},
                                    "parameters": {"type": "object", "description": "Optional parameters dictionary (e.g., {'email': 'Email address to search'})"},
                                    "category": {"type": "string", "description": "Optional category (default: 'general')"}
                                }
                                required = ["workflow_name", "description"]
                            elif task_name == "workflow_play":
                                # Simplified description
                                task_desc = """Execute a saved workflow. Use when user says: "run the workflow", "use the workflow", "do the [workflow name] workflow", or "execute [workflow name]".

Extract workflow name from user's request. Include parameters if user provides them."""
                                properties = {
                                    "workflow_name": {"type": "string", "description": "Name of the workflow to execute. Extract from user's request."},
                                    "parameters": {"type": "object", "description": "Optional parameter values (e.g., {'email': 'john@example.com'}). Extract from user's request if provided."}
                                }
                                required = ["workflow_name"]
                            elif task_name == "workflow_list":
                                # Simplified description
                                task_desc = """List all available workflows. Use when user asks: "what workflows do you know?", "list workflows", or "show me available workflows"."""
                                properties = {}
                                required = []
                            elif task_name == "workflow_delete":
                                # Simplified description
                                task_desc = """Delete a saved workflow. Use when user asks: "delete workflow", "remove workflow", or "forget the [workflow name] workflow".

Extract workflow name from user's request."""
                                properties = {
                                    "workflow_name": {"type": "string", "description": "Name of workflow to delete. Extract from user's request."}
                                }
                                required = ["workflow_name"]
                            else:
                                # Common parameters for other tasks
                                common_params = ["source", "destination", "path", "content", "old_text", "new_text", "command", "directory", "file_path", "organize_by", "config_path", "action", "key", "value"]
                                for param in common_params:
                                    properties[param] = {"type": "string", "description": f"{param} parameter for {task_name}"}
                    except Exception as e:
                        logging.warning(f"Error getting task parameters for {task_name}: {e}")
                        # Fallback to common parameters
                        common_params = ["source", "destination", "path", "content", "old_text", "new_text", "command", "directory"]
                        for param in common_params:
                            properties[param] = {"type": "string", "description": f"{param} parameter for {task_name}"}
                    
                    functions.append({
                        "name": f"execute_task_{task_name}",
                        "description": task_desc,
                        "parameters": {
                            "type": "object",
                            "properties": {
                                "task_name": {"type": "string", "description": f"The task name: {task_name}"},
                                "params": {
                                    "type": "object",
                                    "properties": properties,
                                    "description": "Task-specific parameters",
                                    "required": required if required else []
                                }
                            },
                            "required": ["task_name", "params"]
                        }
                        })
            
            # Make API call with streaming, retry logic, and function calling
            model_name = self.model_options[self.model]
            answer = ""
            
            def make_api_call():
                """Inner function for retry logic - simplified with proper fallback"""
                nonlocal answer
                
                if self.enable_streaming:
                    # Try streaming first
                    try:
                        answer, function_calls = stream_lea_response(
                            client=self.openai_client,
                            model_name=model_name,
                            messages=messages,
                            functions=functions,
                            on_chunk=lambda chunk: self.stream_chunk.emit(chunk) if chunk else None
                        )
                        # Process function calls if any were returned
                        if function_calls and TASK_SYSTEM_AVAILABLE:
                            logging.info(f"Processing {len(function_calls)} function calls from streaming response")
                            answer = self._handle_function_calls(function_calls, answer)
                        return answer
                    except Exception as stream_error:
                        # Log error and fallback to non-streaming
                        logging.error(f"Streaming error: {stream_error}")
                        logging.error(traceback.format_exc())
                        logging.info("Falling back to non-streaming mode")
                        # Fall through to non-streaming call
                
                # Non-streaming response (more reliable for complete responses)
                return self._make_non_streaming_call(model_name, messages, functions)
            
            # Use retry logic with backup model fallback
            try:
                answer = retry_api_call(make_api_call, max_attempts=3, base_delay=1.0)
            except Exception as api_error:
                error_msg = str(api_error)
                
                # Try backup model if primary fails (for model-specific errors or rate limits)
                if ("invalid" in error_msg.lower() and "model" in error_msg.lower()) or \
                   ("rate_limit" in error_msg.lower() or "429" in error_msg):
                    # Get backup model for current mode
                    backup_model_name = BACKUP_MODEL_PER_MODE.get(self.mode)
                    if backup_model_name and backup_model_name in self.model_options:
                        backup_model = self.model_options[backup_model_name]
                        if backup_model != model_name:  # Only try if different
                            logging.info(f"Primary model {model_name} failed, trying backup: {backup_model}")
                            try:
                                # Create new make_api_call with backup model
                                def make_backup_call():
                                    nonlocal answer
                                    if self.enable_streaming:
                                        stream = self.openai_client.chat.completions.create(
                                            model=backup_model,
                                            messages=messages,
                                            functions=functions if functions else None,
                                            function_call="auto" if functions else None,
                                            stream=True,
                                            timeout=60.0
                                        )
                                        full_response = ""
                                        function_calls = []
                                        for chunk in stream:
                                            if chunk.choices[0].delta.content:
                                                content = chunk.choices[0].delta.content
                                                full_response += content
                                                self.stream_chunk.emit(content)
                                            if chunk.choices[0].delta.function_call:
                                                func_call = chunk.choices[0].delta.function_call
                                                if func_call.name:
                                                    function_calls.append({
                                                        "name": func_call.name,
                                                        "arguments": func_call.arguments or ""
                                                    })
                                        answer = full_response
                                        if function_calls and TASK_SYSTEM_AVAILABLE:
                                            answer = self._handle_function_calls(function_calls, answer)
                                        return answer
                                    else:
                                        response = self.openai_client.chat.completions.create(
                                            model=backup_model,
                                            messages=messages,
                                            functions=functions if functions else None,
                                            function_call="auto" if functions else None,
                                            timeout=60.0
                                        )
                                        if not response or not response.choices:
                                            raise Exception("Invalid response from OpenAI API")
                                        message = response.choices[0].message
                                        if message.function_call and TASK_SYSTEM_AVAILABLE:
                                            function_calls = [{
                                                "name": message.function_call.name,
                                                "arguments": message.function_call.arguments
                                            }]
                                            answer = message.content or ""
                                            answer = self._handle_function_calls(function_calls, answer)
                                        else:
                                            answer = message.content or ""
                                        if not answer:
                                            raise Exception("Empty response from OpenAI API")
                                        return answer
                                
                                answer = retry_api_call(make_backup_call, max_attempts=2, base_delay=1.0)
                                self.stream_chunk.emit(f"\n[Note: Using backup model {backup_model_name}]\n")
                            except Exception as backup_error:
                                # Backup also failed, fall through to error handling
                                error_msg = str(backup_error)
                            else:
                                # Backup succeeded, return
                                return answer
                
                # Provide user-friendly error messages
                if "rate_limit" in error_msg.lower() or "429" in error_msg:
                    self.error.emit("API rate limit exceeded. Please wait a moment and try again.")
                elif "timeout" in error_msg.lower():
                    self.error.emit("Request timed out. Please try again.")
                elif "authentication" in error_msg.lower() or "401" in error_msg or "403" in error_msg:
                    self.error.emit("Authentication failed. Check your OpenAI API key.")
                elif "invalid" in error_msg.lower() and "model" in error_msg.lower():
                    self.error.emit(f"Invalid model: {model_name}. Please select a different model.")
                else:
                    self.error.emit(f"API Error: {error_msg}")
                return
            
            # Web search handling (optional, can be improved)
            # Note: This is kept for backward compatibility, but function calling is preferred
            if "[SEARCH:" in answer and "]" in answer:
                try:
                    import re
                    search_pattern = r'\[SEARCH:\s*([^\]]+)\]'
                    searches = re.findall(search_pattern, answer)
                    if searches:
                        all_results = []
                        for query in searches:
                            # Dummy search result (replace with real web_search if available)
                            all_results.append(f"=== Search Results for '{query}' ===\n(Dummy search results)\n")
                        search_context = "\n".join(all_results)
                        followup_prompt = f"{search_context}\n\nNow answer Dre's original question using these search results."
                        self.message_history.append({"role": "user", "content": followup_prompt})
                        
                        # Limit history again
                        if len(self.message_history) > 20:
                            self.message_history = self.message_history[-20:]
                        
                        messages = [{"role": "system", "content": system_prompt}] + self.message_history
                        
                        # Second API call for search results
                        try:
                            response = self.openai_client.chat.completions.create(
                                model=model_name,
                                messages=messages,
                                timeout=60.0
                            )
                            if response and response.choices:
                                answer = response.choices[0].message.content or answer
                        except Exception as search_error:
                            # If search follow-up fails, use original answer
                            logging.warning(f"Search follow-up failed: {search_error}")
                            pass
                except Exception as search_parse_error:
                    logging.warning(f"Search parsing failed: {search_parse_error}")
                    # Continue with original answer
            
            # Ensure answer is a string and not empty
            if not answer:
                answer = ""
            answer = str(answer).strip()
            
            # If answer is empty, provide a helpful error message
            if not answer:
                logging.warning("Empty answer received from API - this may indicate an issue")
                answer = "I apologize, but I didn't receive a response. This might be due to an API issue or the request timing out. Please try again."
            
            # Store important information in memory
            if self.memory_system and self.memory_system.openai_client and answer:
                try:
                    # Store the answer if it seems important (you can enhance this logic)
                    if len(answer) > 100:  # Only store substantial responses
                        self.memory_system.store_important_info(
                            f"User asked: {self.user_text}\nLea responded: {answer[:500]}",
                            metadata={"mode": self.mode, "timestamp": datetime.now().isoformat()}
                        )
                except Exception as mem_store_error:
                    logging.warning(f"Error storing memory: {mem_store_error}")
            
            # Save to history - CRITICAL: Always save assistant responses
            if answer:  # Only save if there's actually content
                self.message_history.append({"role": "assistant", "content": answer})
                # Limit history to last 20 messages
                if len(self.message_history) > 20:
                    self.message_history = self.message_history[-20:]
            else:
                logging.warning("Empty answer received, not saving to history")
            
            # Always emit finished signal, even if answer is empty (so UI knows request completed)
            logging.info(f"LeaWorker.run() completed - answer length: {len(answer) if answer else 0}")
            self.finished.emit(answer, "Ready")
            
        except Exception as e:
            error_msg = f"Unexpected error: {str(e)}"
            logging.error(f"LeaWorker error: {traceback.format_exc()}")
            self.error.emit(error_msg)
    
    def _make_non_streaming_call(self, model_name: str, messages: list, functions: list = None) -> str:
        """Make a non-streaming API call - ensures complete responses with proper fallback"""
        try:
            response = self.openai_client.chat.completions.create(
                model=model_name,
                messages=messages,
                functions=functions if functions else None,
                function_call="auto" if functions else None,
                timeout=120.0  # Increased timeout for longer responses
            )
            
            if not response or not response.choices:
                raise Exception("Invalid response from OpenAI API")
            
            # Check for function calls
            message = response.choices[0].message
            answer = message.content or ""
            
            if message.function_call and TASK_SYSTEM_AVAILABLE:
                function_calls = [{
                    "name": message.function_call.name,
                    "arguments": message.function_call.arguments
                }]
                answer = self._handle_function_calls(function_calls, answer)
            
            if not answer:
                raise Exception("Empty response from OpenAI API")
            
            # If streaming is enabled, emit the complete response as chunks for display
            # This simulates streaming but ensures we have the complete response first
            if self.enable_streaming and answer:
                # Emit in chunks to simulate streaming for better UX
                chunk_size = 20  # Emit 20 characters at a time for smoother display
                for i in range(0, len(answer), chunk_size):
                    chunk = answer[i:i+chunk_size]
                    self.stream_chunk.emit(chunk)
                    time.sleep(0.02)  # Small delay for smooth display
            
            return answer
        except Exception as e:
            logging.error(f"Non-streaming call error: {e}")
            raise
    
    def _extract_email_params_from_history(self) -> dict:
        """Extract email draft parameters from conversation history"""
        params = {}
        if not self.message_history:
            return params
        
        # Search through message history for email parameters
        # Preserve newlines by joining with newline instead of space
        history_text = "\n".join([msg.get("content", "") for msg in self.message_history])
        
        # Look for subject (try multiple patterns)
        subject_patterns = [
            r'Subject:\s*([^\n]+)',
            r'subject:\s*([^\n]+)',
            r'Subject line:\s*([^\n]+)',
            r'"Subject":\s*"([^"]+)"',
            r"'Subject':\s*'([^']+)'",
        ]
        for pattern in subject_patterns:
            match = re.search(pattern, history_text, re.IGNORECASE)
            if match:
                params["subject"] = match.group(1).strip()
                logging.info(f"Extracted subject from history: {params['subject']}")
                break
        
        # Look for body (can span multiple lines)
        # Try to find body content after "Body:" marker
        # The body continues until we hit another section marker or end of text
        body_markers = [
            # Pattern for "- Body:" followed by newline and content
            r'[-*]?\s*Body:\s*\n\s*([\s\S]+?)(?=\n\s*[-*]?\s*(?:To:|Subject:|Cc:|Bcc:)|$)',
            r'[-*]?\s*body:\s*\n\s*([\s\S]+?)(?=\n\s*[-*]?\s*(?:To:|Subject:|Cc:|Bcc:)|$)',
            # Pattern for "Body:" on same line or next line
            r'Body:\s*\n\s*([\s\S]+?)(?=\n\s*[-*]?\s*(?:To:|Subject:|Cc:|Bcc:)|$)',
            r'body:\s*\n\s*([\s\S]+?)(?=\n\s*[-*]?\s*(?:To:|Subject:|Cc:|Bcc:)|$)',
            r'Email body:\s*\n\s*([\s\S]+?)(?=\n\s*[-*]?\s*(?:To:|Subject:|Cc:|Bcc:)|$)',
            # Pattern for "Body:" on same line
            r'Body:\s*([\s\S]+?)(?=\n\s*[-*]?\s*(?:To:|Subject:|Cc:|Bcc:)|$)',
            r'body:\s*([\s\S]+?)(?=\n\s*[-*]?\s*(?:To:|Subject:|Cc:|Bcc:)|$)',
            # JSON-style patterns
            r'"Body":\s*"([^"]+)"',
            r"'Body':\s*'([^']+)'",
        ]
        for pattern in body_markers:
            match = re.search(pattern, history_text, re.IGNORECASE | re.MULTILINE | re.DOTALL)
            if match:
                body_content = match.group(1).strip()
                # Clean up any leading/trailing whitespace and normalize line breaks
                body_content = re.sub(r'\n\s*\n\s*\n+', '\n\n', body_content)  # Normalize multiple blank lines
                # Remove any trailing dashes or bullets that might have been captured
                body_content = re.sub(r'\n\s*[-*]\s*$', '', body_content)
                params["body"] = body_content
                logging.info(f"Extracted body from history ({len(body_content)} chars): {body_content[:100]}...")
                break
        
        # Look for "To" email
        to_patterns = [
            r'To:\s*([^\s\n,]+@[^\s\n,]+)',
            r'to:\s*([^\s\n,]+@[^\s\n,]+)',
            r'Recipient:\s*([^\s\n,]+@[^\s\n,]+)',
            r'"To":\s*"([^"]+)"',
            r"'To':\s*'([^']+)'",
        ]
        for pattern in to_patterns:
            match = re.search(pattern, history_text, re.IGNORECASE)
            if match:
                params["to"] = match.group(1).strip()
                logging.info(f"Extracted 'to' from history: {params['to']}")
                break
        
        return params
    
    def _handle_function_calls(self, function_calls: List[Dict], current_answer: str) -> str:
        """Handle function calls from OpenAI (replaces regex parsing)"""
        if not TASK_SYSTEM_AVAILABLE or not task_registry:
            logging.warning("Task system not available or task_registry is None")
            return current_answer
        
        logging.info(f"Handling {len(function_calls)} function calls, current_answer length: {len(current_answer) if current_answer else 0}")
        task_results = []
        
        for func_call in function_calls:
            try:
                func_name = func_call.get("name", "")
                func_args = func_call.get("arguments", "")
                
                # Parse function name to get task name
                if func_name.startswith("execute_task_"):
                    task_name = func_name.replace("execute_task_", "")
                else:
                    continue
                
                # Initialize actual_task_name to task_name as fallback
                actual_task_name = task_name
                
                # Parse arguments
                try:
                    if isinstance(func_args, str):
                        params_dict = json.loads(func_args)
                    else:
                        params_dict = func_args
                    
                    # Extract task_name and params from the function arguments
                    actual_task_name = params_dict.get("task_name", task_name)
                    params = params_dict.get("params", {})
                except Exception as parse_error:
                    logging.warning(f"Error parsing function arguments: {parse_error}")
                    params = {}
                    # actual_task_name already set to task_name above as fallback
                
                # Check if task requires confirmation
                task_obj = task_registry.get_task(actual_task_name)
                requires_confirmation = task_obj.requires_confirmation if task_obj else False
                
                # Execute task (auto-confirm for now, can be enhanced to ask user)
                logging.info(f"Executing task: {actual_task_name} with params: {params}")
                result = task_registry.execute_task(actual_task_name, params, confirmed=not requires_confirmation)
                logging.info(f"Task {actual_task_name} completed - success: {result.success}, message: {result.message}")
                
                # If task failed due to missing parameters for outlook_email_draft, try to extract from history and retry
                if not result.success and actual_task_name == "outlook_email_draft" and "Missing required parameters" in result.message:
                    logging.info("Task failed due to missing parameters - attempting to extract from conversation history")
                    extracted_params = self._extract_email_params_from_history()
                    if extracted_params:
                        # Merge extracted params with existing params (extracted take precedence for missing ones)
                        merged_params = {**params, **{k: v for k, v in extracted_params.items() if not params.get(k)}}
                        logging.info(f"Retrying task with extracted params: {merged_params}")
                        result = task_registry.execute_task(actual_task_name, merged_params, confirmed=not requires_confirmation)
                        logging.info(f"Retry completed - success: {result.success}, message: {result.message}")
                        if result.success:
                            result.message = f"Successfully created draft (extracted parameters from conversation history). {result.message}"
                
                if result.error:
                    logging.warning(f"Task {actual_task_name} error: {result.error}")
                task_results.append({
                    "task": actual_task_name,
                    "params": params,
                    "result": result.to_dict()
                })
                
            except Exception as task_error:
                logging.error(f"Task execution failed with exception: {task_error}")
                logging.error(traceback.format_exc())
                task_results.append({
                    "task": func_call.get("name", "unknown"),
                    "params": {},
                    "result": {"success": False, "message": f"Error: {str(task_error)}", "error": str(task_error)}
                })
        
        # Add task results to answer
        if task_results:
            results_text = "\n\n=== Task Execution Results ===\n"
            for tr in task_results:
                r = tr["result"]
                results_text += f"\n**Task: {tr['task']}**\n"
                results_text += f"Status: {'‚úÖ Success' if r['success'] else '‚ùå Failed'}\n"
                results_text += f"Message: {r['message']}\n"
                if r.get('error'):
                    results_text += f"Error: {r['error']}\n"
            
            # Always include task results, even if answer is empty
            if current_answer:
                if all(tr["result"]["success"] for tr in task_results):
                    return current_answer + results_text
                else:
                    return current_answer + results_text + "\n\n‚ö†Ô∏è Some tasks failed. Please review and try again if needed."
            else:
                # If no answer text, return just the task results with context
                if all(tr["result"]["success"] for tr in task_results):
                    return "Task completed successfully." + results_text
                else:
                    return "Task execution completed with errors." + results_text + "\n\n‚ö†Ô∏è Some tasks failed. Please review the errors above."
        
        return current_answer

# =====================================================
# MAIN WINDOW
# =====================================================

class LeaWindow(QWidget):
    USER_COLOR = "#68BD47"
    ASSIST_COLOR = "#FFFFFF"
    SYSTEM_COLOR = "#2DBCEE"

    def __init__(self):
        super().__init__()
        
        self.current_mode = "General Assistant & Triage"
        self.current_model = "gpt-5-mini"
        self.message_history = []
        self.history_file = "lea_history.json"
        self.current_file_content = None
        self.current_file_path = None
        
        # Initialize memory system
        self.memory_system = LeaMemory()
        if openai_client:
            self.memory_system.set_client(openai_client)
        
        # Streaming state
        self.current_streaming_response = ""
        self.is_streaming = False
        self.streaming_message_started = False
        self._streaming_msg_created = False  # Track if we've created the streaming message
        self._streaming_block_start = None  # QTextBlock for the streaming message
        self._streaming_block_format = None  # Format to preserve for streaming message
        self._streaming_item_index = -1  # Track the index of the streaming message in the document
        
        # Thread references (prevent crashes from deleted threads)
        self.worker_thread = None
        self.worker = None
        self._current_worker = None
        self._current_thread = None
        
        self.file_worker_thread = None
        self.file_worker = None
        self._file_worker = None
        self._file_worker_thread = None
        
        self.download_worker_thread = None
        self.download_worker = None
        self._download_worker = None
        self._download_worker_thread = None
        
        self.export_worker_thread = None
        self.export_worker = None
        self._export_worker = None
        self._export_worker_thread = None
        
        self.speech_worker_thread = None
        self.speech_worker = None
        self._speech_worker = None
        self._speech_worker_thread = None
        self.is_listening = False
        
        # TTS thread references
        self._tts_thread = None
        self._tts_worker = None
        
        # Settings
        self.settings_file = PROJECT_DIR / "lea_settings.json"
        self.tts_enabled = False
        self.tts_voice_id = ("en", "com")  # Default to English (US) - tuple of (lang, tld) for gTTS
        self.edge_tts_voice = "en-US-AriaNeural"  # Default edge-tts voice (Windows neural voice)
        self.microphone_device_index = None  # None = default microphone
        self.voice_only_mode = False  # New: Voice-only conversation mode (no text on screen)
        self.continuous_listening = False  # Auto-restart listening after each response
        self.push_to_talk_key = None  # Keyboard shortcut for push-to-talk (None = disabled)
        self.enable_gtts_fallback = False  # gTTS fallback disabled by default
        self.load_settings()
        
        # Don't force TTS enabled - respect user settings
        # self.tts_enabled is set by load_settings() above
        
        self._init_window()
        self._build_ui()
        self._load_history()
        
        # Run environment checks and display status
        self._check_environment()
        
        # Setup avatar positioning after UI is built
        QTimer.singleShot(100, self._setup_avatar_position)
        
        # Setup automatic report scheduling
        self._setup_automatic_reports()
    
    def _init_window(self):
        self.setWindowTitle("Hummingbird ‚Äì Lea Multi-Agent")
        if ICON_FILE.exists():
            self.setWindowIcon(QIcon(str(ICON_FILE)))
        self.resize(1200, 800)
        # Enable keyboard focus for push-to-talk
        self.setFocusPolicy(Qt.FocusPolicy.StrongFocus)
        
        self.setStyleSheet("""
            QWidget { background-color: #333; color: #FFF; }
            QLabel { color: #FFF; }
            QComboBox { background-color: #222; color: #FFF; padding: 4px; }
            QFrame#InnerFrame { background-color: #333; border-radius: 8px; }
        """)
    
    def _build_ui(self):
        layout = QVBoxLayout(self)
        layout.setContentsMargins(10, 10, 10, 10)
        
        frame = QFrame()
        frame.setObjectName("InnerFrame")
        frame_layout = QVBoxLayout(frame)
        
        # Header
        header = QHBoxLayout()
        header.addStretch()
        
        header.addWidget(QLabel("Mode:"))
        self.mode_combo = QComboBox()
        self.mode_combo.addItems(list(AGENTS.keys()))
        self.mode_combo.currentTextChanged.connect(self.on_mode_changed)
        header.addWidget(self.mode_combo)
        
        header.addWidget(QLabel("Model:"))
        self.model_combo = QComboBox()
        self.model_combo.addItems(list(MODEL_OPTIONS.keys()))
        self.model_combo.currentTextChanged.connect(self.on_model_changed)
        header.addWidget(self.model_combo)
        
        if ICON_FILE.exists():
            icon = QLabel()
            icon.setPixmap(QPixmap(str(ICON_FILE)).scaled(32, 32, Qt.AspectRatioMode.KeepAspectRatio))
            header.addWidget(icon)
        
        frame_layout.addLayout(header)
        
        # Buttons
        buttons = QHBoxLayout()
        
        upload_btn = QPushButton("üìé Upload")
        upload_btn.clicked.connect(self.upload_file)
        upload_btn.setStyleSheet("background-color: #0078D7; padding: 6px 12px; border-radius: 4px;")
        buttons.addWidget(upload_btn)
        
        download_btn = QPushButton("üì• Download")
        download_btn.clicked.connect(self.download_response)
        download_btn.setStyleSheet("background-color: #107C10; padding: 6px 12px; border-radius: 4px;")
        buttons.addWidget(download_btn)
        
        settings_btn = QPushButton("‚öôÔ∏è Settings")
        settings_btn.clicked.connect(self.show_settings)
        settings_btn.setToolTip("Audio settings: Voice selection and microphone")
        settings_btn.setStyleSheet("background-color: #6B46C1; padding: 6px 12px; border-radius: 4px;")
        buttons.addWidget(settings_btn)
        
        export_btn = QPushButton("Export")
        export_btn.clicked.connect(self.export_conversation)
        export_btn.setStyleSheet("background-color: #0078D7; padding: 6px 12px; border-radius: 4px;")
        buttons.addWidget(export_btn)
        
        if TASK_SYSTEM_AVAILABLE:
            tasks_btn = QPushButton("ü§ñ Tasks")
            tasks_btn.clicked.connect(self.show_tasks_dialog)
            tasks_btn.setStyleSheet("background-color: #6B46C1; padding: 6px 12px; border-radius: 4px;")
            buttons.addWidget(tasks_btn)
        
        clear_btn = QPushButton("Clear")
        clear_btn.clicked.connect(self.clear_conversation)
        clear_btn.setStyleSheet("background-color: #D13438; padding: 6px 12px; border-radius: 4px;")
        buttons.addWidget(clear_btn)
        
        buttons.addStretch()
        frame_layout.addLayout(buttons)
        
        # File status
        self.file_label = QLabel("")
        self.file_label.setStyleSheet("color: #68BD47; font-size: 11px; font-style: italic;")
        frame_layout.addWidget(self.file_label)
        
        # Chat display with avatar overlay
        chat_container = QWidget()
        chat_layout = QVBoxLayout(chat_container)
        chat_layout.setContentsMargins(0, 0, 0, 0)
        
        self.chat_display = QTextEdit()
        self.chat_display.setReadOnly(True)
        self.chat_display.setStyleSheet(
            "background-color: #222; color: #FFF; font-size: 16px; "
            "font-family: Consolas, monospace;"
        )
        chat_layout.addWidget(self.chat_display, stretch=1)
        
        # Visual avatar for TTS (overlay on chat)
        self.avatar_widget = QLabel()
        self.avatar_widget.setParent(self.chat_display)
        self.avatar_widget.setAlignment(Qt.AlignmentFlag.AlignCenter)
        self.avatar_widget.setStyleSheet("""
            QLabel {
                background-color: rgba(0, 0, 0, 0);
                border: none;
                color: #68BD47;
            }
        """)
        self.avatar_widget.setText("üê¶")
        self.avatar_widget.setFont(self.font())
        font = self.avatar_widget.font()
        font.setPointSize(80)
        font.setBold(True)
        self.avatar_widget.setFont(font)
        self.avatar_widget.hide()  # Hidden by default
        
        # Animation for avatar (pulsing/talking effect)
        # Use opacity animation for smooth pulsing
        self.avatar_opacity = QPropertyAnimation(self.avatar_widget, b"windowOpacity")
        self.avatar_opacity.setDuration(800)
        self.avatar_opacity.setLoopCount(-1)  # Infinite loop
        self.avatar_opacity.setEasingCurve(QEasingCurve.Type.InOutSine)
        
        frame_layout.addWidget(chat_container, stretch=1)
        
        # Input
        input_layout = QHBoxLayout()
        
        # Emoji button
        emoji_btn = QPushButton("üòä")
        emoji_btn.setToolTip("Insert emoji")
        emoji_btn.setMinimumWidth(45)
        emoji_btn.setMaximumWidth(45)
        emoji_btn.setStyleSheet("background-color: #444; font-size: 20px; border-radius: 4px; padding: 4px;")
        emoji_btn.clicked.connect(self.show_emoji_picker)
        input_layout.addWidget(emoji_btn)
        
        # Microphone button for voice input (always show, will prompt for install if needed)
        self.mic_btn = QPushButton("üé§")
        # Check import directly for tooltip
        try:
            import speech_recognition as sr
            sr_available = True
        except ImportError:
            sr_available = False
        self.mic_btn.setToolTip("Click to speak your message" + ("" if sr_available else " (Install SpeechRecognition to enable)"))
        self.mic_btn.setMinimumWidth(45)
        self.mic_btn.setMaximumWidth(45)
        self.mic_btn.setStyleSheet("background-color: #444; font-size: 20px; border-radius: 4px; padding: 4px;")
        self.mic_btn.clicked.connect(self.toggle_speech_recognition)
        input_layout.addWidget(self.mic_btn)
        
        self.input_box = ChatInputBox()
        self.input_box.setPlaceholderText("Ask Lea anything... (Enter to send, Shift+Enter for new line)\nüí° Tip: Drag & drop files here or paste text snippets")
        self.input_box.returnPressed.connect(self.on_send)
        self.input_box.fileDropped.connect(self.on_file_dropped)  # Handle dropped files
        self.input_box.setMinimumHeight(80)
        self.input_box.setStyleSheet("background-color: #222; color: #FFF; font-size: 16px;")
        input_layout.addWidget(self.input_box, stretch=1)
        
        send_btn = QPushButton("Send")
        send_btn.clicked.connect(self.on_send)
        send_btn.setSizePolicy(QSizePolicy.Policy.Fixed, QSizePolicy.Policy.Expanding)
        send_btn.setMinimumWidth(90)
        send_btn.setStyleSheet("background-color: #0078D7; font-size: 16px; font-weight: 600; border-radius: 4px;")
        input_layout.addWidget(send_btn)
        
        frame_layout.addLayout(input_layout)
        
        # Options
        options = QHBoxLayout()
        self.include_file_cb = QCheckBox("Include uploaded file in context")
        self.include_file_cb.setChecked(True)
        options.addWidget(self.include_file_cb)
        options.addStretch()
        frame_layout.addLayout(options)
        
        # Status
        self.status_label = QLabel("Ready.")
        self.status_label.setStyleSheet("color: #DDD; font-size: 12px;")
        frame_layout.addWidget(self.status_label)
        
        layout.addWidget(frame)
    
    # File operations
    def on_file_dropped(self, file_path: str):
        """Handle file dropped into input box"""
        if not file_path:
            return
        
        # Check if file exists
        path_obj = Path(file_path)
        if not path_obj.exists():
            QMessageBox.warning(self, "File Not Found", f"File not found:\n{file_path}")
            return
        
        # Check if it's a file (not a directory)
        if not path_obj.is_file():
            QMessageBox.information(self, "Directory Dropped", "Please drop a file, not a directory.")
            return
        
        # Upload the dropped file
        self._upload_file_path(file_path)
    
    def upload_file(self):
        if not FILE_READER_AVAILABLE:
            QMessageBox.warning(self, "Error", "universal_file_reader.py not found")
            return
        path, _ = QFileDialog.getOpenFileName(self, "Upload File", "", "All Files (*)")
        if not path:
            return
        self._upload_file_path(path)
    
    def _upload_file_path(self, path: str):
        """Internal method to upload a file by path"""
        if not FILE_READER_AVAILABLE:
            QMessageBox.warning(self, "Error", "universal_file_reader.py not found")
            return
        
        self.status_label.setText("Reading file...")
        QApplication.processEvents()
        # Clean up any existing file worker thread
        try:
            if hasattr(self, 'file_worker_thread') and self.file_worker_thread is not None:
                try:
                    if hasattr(self, 'file_worker') and self.file_worker is not None:
                        try:
                            self.file_worker.finished.disconnect()
                            self.file_worker.error.disconnect()
                        except:
                            pass
                except:
                    pass
        except:
            pass
        
        # Start file upload worker thread
        self.file_worker_thread = QThread()
        self.file_worker = FileUploadWorker(path)
        self.file_worker.moveToThread(self.file_worker_thread)
        
        # Store references
        self._file_worker = self.file_worker
        self._file_worker_thread = self.file_worker_thread
        
        self.file_worker_thread.started.connect(self.file_worker.run)
        self.file_worker.finished.connect(self.on_file_upload_finished)
        self.file_worker.error.connect(self.on_file_upload_error)
        self.file_worker.finished.connect(self.file_worker_thread.quit)
        self.file_worker.error.connect(self.file_worker_thread.quit)
        
        def safe_delete_file_worker():
            try:
                if hasattr(self, '_file_worker') and self._file_worker:
                    self._file_worker.deleteLater()
                    self._file_worker = None
            except:
                pass
        
        def safe_delete_file_thread():
            try:
                if hasattr(self, '_file_worker_thread') and self._file_worker_thread:
                    self._file_worker_thread.deleteLater()
                    self._file_worker_thread = None
                    if hasattr(self, 'file_worker_thread'):
                        self.file_worker_thread = None
            except:
                pass
        
        self.file_worker_thread.finished.connect(safe_delete_file_worker)
        self.file_worker_thread.finished.connect(safe_delete_file_thread)
        self.file_worker_thread.start()

    def on_file_upload_finished(self, result, backup_path, file_name):
        try:
            # Store file path before worker is deleted (use stored reference)
            if result['success']:
                # Get path from stored reference or result
                if hasattr(self, '_file_worker') and self._file_worker and hasattr(self._file_worker, 'path'):
                    self.current_file_path = self._file_worker.path
                elif hasattr(self, 'file_worker') and self.file_worker and hasattr(self.file_worker, 'path'):
                    try:
                        self.current_file_path = self.file_worker.path
                    except (RuntimeError, AttributeError):
                        # Worker deleted, use backup_path if available
                        if backup_path:
                            self.current_file_path = backup_path
                
                self.current_file_content = result.get('content', '')
                self.file_label.setText(f"üìé {file_name} ({result.get('file_type', 'unknown')})")
                # Include full file path in system message so Lea can access it for tasks like PowerPoint editing
                file_path_for_lea = self.current_file_path if hasattr(self, 'current_file_path') and self.current_file_path else (backup_path if backup_path else file_name)
                # Make file path very explicit and easy to extract - use a clear format that Lea can parse
                self.append_message("system", f"Uploaded: {file_name}\n\nFILE_PATH_FOR_TASKS: {file_path_for_lea}\n\n(Use this exact path for any file operations on this file)")
                self.status_label.setText("File loaded")
            else:
                QMessageBox.warning(self, "Error", result.get('error', 'Unknown error'))
                self.status_label.setText("Error loading file")
            
            # Clean up reference
            try:
                if hasattr(self, '_file_worker'):
                    self._file_worker = None
            except:
                pass
        except Exception as e:
            logging.error(f"Error in on_file_upload_finished: {traceback.format_exc()}")
            try:
                self.status_label.setText("Error loading file")
            except:
                pass

    def on_file_upload_error(self, error_msg):
        try:
            error_text = str(error_msg) if error_msg else "Unknown error"
            QMessageBox.warning(self, "File Upload Error", error_text)
            self.status_label.setText("Error loading file")
        except Exception as e:
            logging.error(f"Error in on_file_upload_error: {traceback.format_exc()}")
            self.status_label.setText("Error")
    
    def download_response(self):
        if not self.message_history:
            QMessageBox.information(self, "Nothing to Download", "No conversation to save")
            return
        # Get last assistant response
        last_response = None
        for msg in reversed(self.message_history):
            if msg['role'] == 'assistant':
                last_response = msg['content']
                break
        if not last_response:
            QMessageBox.information(self, "Nothing to Download", "No response to save")
            return
        # Clean up any existing download worker thread
        try:
            if hasattr(self, 'download_worker_thread') and self.download_worker_thread is not None:
                try:
                    if hasattr(self, 'download_worker') and self.download_worker is not None:
                        try:
                            self.download_worker.finished.disconnect()
                            self.download_worker.error.disconnect()
                        except:
                            pass
                except:
                    pass
        except:
            pass
        
        # Start download worker thread
        self.download_worker_thread = QThread()
        self.download_worker = DownloadWorker(last_response)
        self.download_worker.moveToThread(self.download_worker_thread)
        
        # Store references
        self._download_worker = self.download_worker
        self._download_worker_thread = self.download_worker_thread
        
        self.download_worker_thread.started.connect(self.download_worker.run)
        self.download_worker.finished.connect(self.on_download_finished)
        self.download_worker.error.connect(self.on_download_error)
        self.download_worker.finished.connect(self.download_worker_thread.quit)
        self.download_worker.error.connect(self.download_worker_thread.quit)
        
        def safe_delete_download_worker():
            try:
                if hasattr(self, '_download_worker') and self._download_worker:
                    self._download_worker.deleteLater()
                    self._download_worker = None
            except:
                pass
        
        def safe_delete_download_thread():
            try:
                if hasattr(self, '_download_worker_thread') and self._download_worker_thread:
                    self._download_worker_thread.deleteLater()
                    self._download_worker_thread = None
                    if hasattr(self, 'download_worker_thread'):
                        self.download_worker_thread = None
            except:
                pass
        
        self.download_worker_thread.finished.connect(safe_delete_download_worker)
        self.download_worker_thread.finished.connect(safe_delete_download_thread)
        self.download_worker_thread.start()

    def on_download_finished(self, download_path, basename):
        try:
            self.append_message("system", f"Downloaded to: {basename}")
            QMessageBox.information(self, "Downloaded", f"Saved to:\n{download_path}")
            
            # Clean up reference
            try:
                if hasattr(self, '_download_worker'):
                    self._download_worker = None
            except:
                pass
        except Exception as e:
            logging.error(f"Error in on_download_finished: {traceback.format_exc()}")

    def on_download_error(self, error_msg):
        try:
            error_text = str(error_msg) if error_msg else "Unknown error"
            QMessageBox.warning(self, "Download Error", error_text)
            
            # Clean up reference
            try:
                if hasattr(self, '_download_worker'):
                    self._download_worker = None
            except:
                pass
        except Exception as e:
            logging.error(f"Error in on_download_error: {traceback.format_exc()}")
    
    def export_conversation(self):
        path, _ = QFileDialog.getSaveFileName(self, "Export", "", "JSON (*.json);;Text (*.txt)")
        if not path:
            return
        # Clean up any existing export worker thread
        try:
            if hasattr(self, 'export_worker_thread') and self.export_worker_thread is not None:
                try:
                    if hasattr(self, 'export_worker') and self.export_worker is not None:
                        try:
                            self.export_worker.finished.disconnect()
                            self.export_worker.error.disconnect()
                        except:
                            pass
                except:
                    pass
        except:
            pass
        
        # Start export worker thread
        self.export_worker_thread = QThread()
        self.export_worker = ExportWorker(
            path,
            self.current_mode,
            self.current_model,
            self.message_history,
            self.chat_display.toPlainText()
        )
        self.export_worker.moveToThread(self.export_worker_thread)
        
        # Store references
        self._export_worker = self.export_worker
        self._export_worker_thread = self.export_worker_thread
        
        self.export_worker_thread.started.connect(self.export_worker.run)
        self.export_worker.finished.connect(self.on_export_finished)
        self.export_worker.error.connect(self.on_export_error)
        self.export_worker.finished.connect(self.export_worker_thread.quit)
        self.export_worker.error.connect(self.export_worker_thread.quit)
        
        def safe_delete_export_worker():
            try:
                if hasattr(self, '_export_worker') and self._export_worker:
                    self._export_worker.deleteLater()
                    self._export_worker = None
            except:
                pass
        
        def safe_delete_export_thread():
            try:
                if hasattr(self, '_export_worker_thread') and self._export_worker_thread:
                    self._export_worker_thread.deleteLater()
                    self._export_worker_thread = None
                    if hasattr(self, 'export_worker_thread'):
                        self.export_worker_thread = None
            except:
                pass
        
        self.export_worker_thread.finished.connect(safe_delete_export_worker)
        self.export_worker_thread.finished.connect(safe_delete_export_thread)
        self.export_worker_thread.start()

    def on_export_finished(self, path):
        try:
            QMessageBox.information(self, "Exported", f"Saved to:\n{path}")
            
            # Clean up reference
            try:
                if hasattr(self, '_export_worker'):
                    self._export_worker = None
            except:
                pass
        except Exception as e:
            logging.error(f"Error in on_export_finished: {traceback.format_exc()}")

    def on_export_error(self, error_msg):
        try:
            error_text = str(error_msg) if error_msg else "Unknown error"
            QMessageBox.warning(self, "Export Error", error_text)
            
            # Clean up reference
            try:
                if hasattr(self, '_export_worker'):
                    self._export_worker = None
            except:
                pass
        except Exception as e:
            logging.error(f"Error in on_export_error: {traceback.format_exc()}")
    
    def show_tasks_dialog(self):
        """Show task management dialog"""
        if not TASK_SYSTEM_AVAILABLE:
            QMessageBox.warning(self, "Tasks Unavailable", "Task system is not available.")
            return
        
        dialog = QDialog(self)
        dialog.setWindowTitle("ü§ñ Lea Task Management")
        dialog.setMinimumSize(800, 600)
        
        layout = QVBoxLayout(dialog)
        
        # Title
        title = QLabel("Available Tasks")
        title.setStyleSheet("font-size: 18px; font-weight: 600;")
        layout.addWidget(title)
        
        # Task list table
        table = QTableWidget()
        table.setColumnCount(4)
        table.setHorizontalHeaderLabels(["Task Name", "Description", "Confirmation Required", "Status"])
        table.horizontalHeader().setSectionResizeMode(QHeaderView.ResizeMode.Stretch)
        
        tasks = task_registry.list_tasks()
        table.setRowCount(len(tasks))
        
        for row, task in enumerate(tasks):
            table.setItem(row, 0, QTableWidgetItem(task["name"]))
            table.setItem(row, 1, QTableWidgetItem(task["description"]))
            table.setItem(row, 2, QTableWidgetItem("Yes" if task["requires_confirmation"] else "No"))
            status = "‚úÖ Enabled" if task["allowed"] else "‚ùå Disabled"
            table.setItem(row, 3, QTableWidgetItem(status))
        
        layout.addWidget(table)
        
        # Buttons
        buttons = QHBoxLayout()
        
        enable_btn = QPushButton("Enable Selected")
        enable_btn.clicked.connect(lambda: self.toggle_task_status(table, True))
        buttons.addWidget(enable_btn)
        
        disable_btn = QPushButton("Disable Selected")
        disable_btn.clicked.connect(lambda: self.toggle_task_status(table, False))
        buttons.addWidget(disable_btn)
        
        buttons.addStretch()
        
        close_btn = QPushButton("Close")
        close_btn.clicked.connect(dialog.accept)
        buttons.addWidget(close_btn)
        
        layout.addLayout(buttons)
        
        # Show task history
        history_group = QGroupBox("Recent Task History")
        history_layout = QVBoxLayout()
        history_text = QTextEdit()
        history_text.setReadOnly(True)
        history_text.setMaximumHeight(150)
        
        history = task_registry.task_history[-10:]  # Last 10 tasks
        if history:
            history_content = "\n".join([
                f"{item['timestamp']}: {item['task_name']} - {'‚úÖ' if item['result']['success'] else '‚ùå'} {item['result']['message']}"
                for item in reversed(history)
            ])
            history_text.setText(history_content)
        else:
            history_text.setText("No task history yet.")
        
        history_layout.addWidget(history_text)
        history_group.setLayout(history_layout)
        layout.addWidget(history_group)
        
        dialog.exec()
    
    def toggle_task_status(self, table, enable):
        """Enable or disable selected task"""
        selected = table.selectedItems()
        if not selected:
            QMessageBox.information(self, "No Selection", "Please select a task from the table.")
            return
        
        row = selected[0].row()
        task_name = table.item(row, 0).text()
        
        if enable:
            task_registry.enable_task(task_name)
        else:
            reply = QMessageBox.question(
                self, "Disable Task",
                f"Are you sure you want to disable '{task_name}'?",
                QMessageBox.StandardButton.Yes | QMessageBox.StandardButton.No
            )
            if reply == QMessageBox.StandardButton.Yes:
                task_registry.disable_task(task_name)
        
        # Refresh table
        status = "‚úÖ Enabled" if enable else "‚ùå Disabled"
        table.setItem(row, 3, QTableWidgetItem(status))
    
    def show_emoji_picker(self):
        """Show emoji picker dialog with search functionality using emoji package"""
        try:
            import emoji
            USE_EMOJI_PACKAGE = True
        except ImportError:
            USE_EMOJI_PACKAGE = False
            logging.warning("emoji package not installed, using fallback emoji list")
        
        # Most commonly used emojis - show these first
        most_used_emojis = [
            "üòä", "üòÇ", "‚ù§Ô∏è", "üòç", "üò≠", "üòò", "üëç", "üòÅ", "üòÖ", "üò¢", 
            "üòé", "üôÇ", "üòâ", "üòå", "ü§î", "üòè", "üò¥", "üòã", "üòÑ", "üòÉ",
            "‚úÖ", "‚ùå", "‚≠ê", "üî•", "üíØ", "üéâ", "üéä", "üéà", "üéÅ", "üéÇ",
            "üëç", "üëé", "üëå", "‚úåÔ∏è", "ü§û", "ü§ù", "üôè", "üëè", "üí™", "ü§ó",
            "üò±", "üò®", "üò∞", "üòì", "üò§", "üò†", "üò°", "ü§¨", "üò≥", "ü•∫",
            "üíï", "üíñ", "üíó", "üíì", "üíû", "üíù", "üíü", "‚ù£Ô∏è", "üíî", "‚ù§Ô∏è‚Äçüî•",
            "üòÄ", "üòÉ", "üòÑ", "üòÅ", "üòÜ", "üòÖ", "ü§£", "üôÇ", "üôÉ", "üòâ",
            "üòä", "üòá", "ü•∞", "üòç", "ü§©", "üòò", "üòó", "üòö", "üòô", "üòã",
            "üòõ", "üòú", "ü§™", "üòù", "ü§ë", "ü§ó", "ü§≠", "ü§´", "ü§î", "ü§ê",
            "ü§®", "üòê", "üòë", "üò∂", "üòè", "üòí", "üôÑ", "üò¨", "ü§•", "üòå",
            "üòî", "üò™", "ü§§", "üò¥", "üò∑", "ü§í", "ü§ï", "ü§¢", "ü§Æ", "ü§ß",
            "ü•µ", "ü•∂", "üò∂‚Äçüå´Ô∏è", "üòµ", "üòµ‚Äçüí´", "ü§Ø", "ü§†", "ü•≥", "üòé", "ü§ì",
            "üßê", "üòï", "üòü", "üôÅ", "‚òπÔ∏è", "üòÆ", "üòØ", "üò≤", "üò≥", "ü•∫",
            "üò¶", "üòß", "üò®", "üò∞", "üò•", "üò¢", "üò≠", "üò±", "üòñ", "üò£",
            "üòû", "üòì", "üò©", "üò´", "ü•±", "üò§", "üò°", "üò†", "ü§¨", "üòà",
            "üëø", "üíÄ", "‚ò†Ô∏è", "üí©", "ü§°", "üëπ", "üë∫", "üëª", "üëΩ", "üëæ",
            "ü§ñ", "üò∫", "üò∏", "üòπ", "üòª", "üòº", "üòΩ", "üôÄ", "üòø", "üòæ"
        ]
        
        # Use emoji package if available, otherwise fallback to hardcoded list
        if USE_EMOJI_PACKAGE:
            # Build emoji data from emoji package
            emojis_data = {}
            
            # Add "Most Used" category first
            emojis_data["‚≠ê Most Used"] = {
                "emojis": most_used_emojis,
                "keywords": "most used common popular favorite frequent smile happy love heart thumbs up check mark star fire"
            }
            
            emoji_categories = {
                "Smileys & Emotion": ["smile", "face", "happy", "sad", "angry", "laugh", "cry", "love", "kiss", "grin", "wink"],
                "People & Body": ["person", "hand", "finger", "arm", "leg", "foot", "body", "man", "woman", "child", "people"],
                "Animals & Nature": ["animal", "dog", "cat", "bird", "fish", "tree", "flower", "plant", "nature", "pet"],
                "Food & Drink": ["food", "fruit", "vegetable", "drink", "coffee", "pizza", "burger", "cake", "eat", "meal"],
                "Travel & Places": ["car", "plane", "train", "building", "house", "hotel", "travel", "place", "location"],
                "Activities": ["sport", "ball", "game", "music", "dance", "activity", "exercise", "play"],
                "Objects": ["phone", "computer", "book", "money", "tool", "object", "tech", "device"],
                "Symbols": ["heart", "star", "check", "mark", "symbol", "sign", "arrow", "arrow", "checkmark"],
                "Flags": ["flag", "country", "nation"]
            }
            
            # Get all emojis from emoji package
            all_emojis = []
            for emoji_char, emoji_info in emoji.EMOJI_DATA.items():
                aliases = emoji_info.get('alias', [])
                if aliases:
                    # Use the first alias as the main name
                    name = aliases[0].replace(':', '').replace('_', ' ')
                    all_emojis.append({
                        'emoji': emoji_char,
                        'name': name,
                        'aliases': ' '.join([a.replace(':', '').replace('_', ' ') for a in aliases]),
                        'status': emoji_info.get('status', '')
                    })
            
            # Organize emojis by category based on name/aliases
            for category, keywords in emoji_categories.items():
                category_emojis = []
                for emoji_data in all_emojis:
                    search_text = (emoji_data['name'] + ' ' + emoji_data['aliases']).lower()
                    if any(keyword in search_text for keyword in keywords):
                        category_emojis.append(emoji_data['emoji'])
                
                if category_emojis:
                    emojis_data[category] = {
                        "emojis": category_emojis[:500],  # Increased limit to 500 per category for better browsing
                        "keywords": ' '.join(keywords)
                    }
            
            # Add "All Emojis" category for browsing (show more emojis)
            all_emoji_list = [e['emoji'] for e in all_emojis[:1000]]  # Increased limit for better browsing
            emojis_data["All Emojis"] = {
                "emojis": all_emoji_list,
                "keywords": "all emoji browse complete list"
            }
        else:
            # Fallback to original hardcoded emoji library
            emojis_data = {
            "‚≠ê Most Used": {
                "emojis": most_used_emojis,
                "keywords": "most used common popular favorite frequent smile happy love heart thumbs up check mark star fire"
            },
            "Faces & Emotions": {
                "emojis": ["üòä", "üòÄ", "üòÉ", "üòÑ", "üòÅ", "üòÜ", "üòÖ", "ü§£", "üòÇ", "üôÇ", "üôÉ", "üòâ", "üòå", "üòç", "ü•∞", "üòò", "üòó", "üòô", "üòö", "üòã", "üòõ", "üòù", "üòú", "ü§™", "ü§®", "üßê", "ü§ì", "üòé", "ü§©", "ü•≥", "üòè", "üòí", "üòû", "üòî", "üòü", "üòï", "üôÅ", "‚òπÔ∏è", "üò£", "üòñ", "üò´", "üò©", "ü•∫", "üò¢", "üò≠", "üò§", "üò†", "üò°", "ü§¨", "ü§Ø", "üò≥", "ü•µ", "ü•∂", "üò±", "üò®", "üò∞", "üò•", "üòì", "ü§¢", "ü§Æ", "ü§ß", "ü•¥", "üò¥", "ü§§", "üò™", "üòµ", "ü§ê", "ü•±", "üò∑"],
                "keywords": "smile happy laugh grin face emotion feeling sad angry cry"
            },
            "Hand Gestures": {
                "emojis": ["üëã", "ü§ö", "üñêÔ∏è", "‚úã", "üññ", "üëå", "ü§å", "ü§è", "‚úåÔ∏è", "ü§û", "ü§ü", "ü§ò", "ü§ô", "üëà", "üëâ", "üëÜ", "üñï", "üëá", "‚òùÔ∏è", "üëç", "üëé", "‚úä", "üëä", "ü§õ", "ü§ú", "üëè", "üôå", "üëê", "ü§≤", "ü§ù", "üôè", "‚úçÔ∏è", "üí™", "ü¶æ", "ü¶ø"],
                "keywords": "hand wave thumbs up clap point gesture fingers"
            },
            "People & Body": {
                "emojis": ["üë§", "üë•", "üßë", "üë®", "üë©", "üßë‚Äçü¶±", "üë®‚Äçü¶±", "üë©‚Äçü¶±", "üßë‚Äçü¶∞", "üë®‚Äçü¶∞", "üë©‚Äçü¶∞", "üë±", "üë±‚Äç‚ôÇÔ∏è", "üë±‚Äç‚ôÄÔ∏è", "üßë‚Äçü¶≥", "üë®‚Äçü¶≥", "üë©‚Äçü¶≥", "üßë‚Äçü¶≤", "üë®‚Äçü¶≤", "üë©‚Äçü¶≤", "üßî", "üëµ", "üßì", "üë¥", "üë≤", "üë≥", "üë≥‚Äç‚ôÇÔ∏è", "üë≥‚Äç‚ôÄÔ∏è", "üßï", "üëÆ", "üëÆ‚Äç‚ôÇÔ∏è", "üëÆ‚Äç‚ôÄÔ∏è", "üë∑", "üë∑‚Äç‚ôÇÔ∏è", "üë∑‚Äç‚ôÄÔ∏è", "üíÇ", "üíÇ‚Äç‚ôÇÔ∏è", "üíÇ‚Äç‚ôÄÔ∏è", "üïµÔ∏è", "üïµÔ∏è‚Äç‚ôÇÔ∏è", "üïµÔ∏è‚Äç‚ôÄÔ∏è", "üë©‚Äç‚öïÔ∏è", "üë®‚Äç‚öïÔ∏è", "üë©‚Äçüåæ", "üë®‚Äçüåæ", "üë©‚Äçüç≥", "üë®‚Äçüç≥", "üë©‚Äçüéì", "üë®‚Äçüéì", "üë©‚Äçüé§", "üë®‚Äçüé§", "üë©‚Äçüè´", "üë®‚Äçüè´", "üë©‚Äçüè≠", "üë®‚Äçüè≠", "üë©‚Äçüíª", "üë®‚Äçüíª", "üë©‚Äçüíº", "üë®‚Äçüíº", "üë©‚Äçüîß", "üë®‚Äçüîß", "üë©‚Äçüî¨", "üë®‚Äçüî¨", "üë©‚Äçüé®", "üë®‚Äçüé®", "üë©‚Äçüöí", "üë®‚Äçüöí", "üë©‚Äç‚úàÔ∏è", "üë®‚Äç‚úàÔ∏è", "üë©‚ÄçüöÄ", "üë®‚ÄçüöÄ", "üë©‚Äç‚öñÔ∏è", "üë®‚Äç‚öñÔ∏è", "ü§∂", "üéÖ", "üßô‚Äç‚ôÄÔ∏è", "üßô‚Äç‚ôÇÔ∏è", "üßù‚Äç‚ôÄÔ∏è", "üßù‚Äç‚ôÇÔ∏è", "üßõ‚Äç‚ôÄÔ∏è", "üßõ‚Äç‚ôÇÔ∏è", "üßü‚Äç‚ôÄÔ∏è", "üßü‚Äç‚ôÇÔ∏è", "üßû‚Äç‚ôÄÔ∏è", "üßû‚Äç‚ôÇÔ∏è", "üßú‚Äç‚ôÄÔ∏è", "üßú‚Äç‚ôÇÔ∏è", "üßö‚Äç‚ôÄÔ∏è", "üßö‚Äç‚ôÇÔ∏è", "üëº", "ü§∞", "ü§±", "üë∂", "üßí", "üë¶", "üëß", "üßë", "üë®", "üë©", "üë®‚Äçüë©‚Äçüëß", "üë®‚Äçüë©‚Äçüëß‚Äçüë¶", "üë®‚Äçüë©‚Äçüë¶‚Äçüë¶", "üë®‚Äçüë©‚Äçüëß‚Äçüëß", "üë®‚Äçüë®‚Äçüë¶", "üë®‚Äçüë®‚Äçüëß", "üë®‚Äçüë®‚Äçüëß‚Äçüë¶", "üë®‚Äçüë®‚Äçüë¶‚Äçüë¶", "üë®‚Äçüë®‚Äçüëß‚Äçüëß", "üë©‚Äçüë©‚Äçüë¶", "üë©‚Äçüë©‚Äçüëß", "üë©‚Äçüë©‚Äçüëß‚Äçüë¶", "üë©‚Äçüë©‚Äçüë¶‚Äçüë¶", "üë©‚Äçüë©‚Äçüëß‚Äçüëß"],
                "keywords": "person people body man woman child family"
            },
            "Animals & Nature": {
                "emojis": ["üê∂", "üê±", "üê≠", "üêπ", "üê∞", "ü¶ä", "üêª", "üêº", "üê®", "üêØ", "ü¶Å", "üêÆ", "üê∑", "üêΩ", "üê∏", "üêµ", "üôà", "üôâ", "üôä", "üêí", "üêî", "üêß", "üê¶", "üê§", "üê£", "üê•", "ü¶Ü", "ü¶Ö", "ü¶â", "ü¶á", "üê∫", "üêó", "üê¥", "ü¶Ñ", "üêù", "üêõ", "ü¶ã", "üêå", "üêû", "üêú", "ü¶ü", "ü¶ó", "üï∑Ô∏è", "ü¶Ç", "üê¢", "üêç", "ü¶é", "ü¶ñ", "ü¶ï", "üêô", "ü¶ë", "ü¶ê", "ü¶û", "ü¶Ä", "üê°", "üê†", "üêü", "üê¨", "üê≥", "üêã", "ü¶à", "üêä", "üêÖ", "üêÜ", "ü¶ì", "ü¶ç", "ü¶ß", "üêò", "ü¶õ", "ü¶è", "üê™", "üê´", "ü¶í", "ü¶ò", "ü¶¨", "üêÉ", "üêÇ", "üêÑ", "üêé", "üêñ", "üêè", "üêë", "ü¶ô", "üêê", "ü¶å", "üêï", "üê©", "ü¶Æ", "üêï‚Äçü¶∫", "üêà", "üêì", "ü¶É", "ü¶§", "ü¶ö", "ü¶ú", "ü¶¢", "ü¶©", "üïäÔ∏è", "üêá", "ü¶ù", "ü¶®", "ü¶°", "ü¶´", "ü¶¶", "ü¶•", "üêÅ", "üêÄ", "üêøÔ∏è"],
                "keywords": "animal dog cat bird nature pet wildlife"
            },
            "Food & Drink": {
                "emojis": ["üçè", "üçé", "üçê", "üçä", "üçã", "üçå", "üçâ", "üçá", "üçì", "üçà", "üçí", "üçë", "ü•≠", "üçç", "ü••", "ü•ù", "üçÖ", "üçÜ", "ü•ë", "ü•¶", "ü•¨", "ü•í", "üå∂Ô∏è", "üåΩ", "ü•ï", "ü•î", "üç†", "ü•ê", "ü•Ø", "üçû", "ü•ñ", "ü•®", "üßÄ", "ü•ö", "üç≥", "ü•û", "ü•ì", "ü•©", "üçó", "üçñ", "ü¶¥", "üå≠", "üçî", "üçü", "üçï", "ü•™", "ü•ô", "üåÆ", "üåØ", "ü•ó", "ü•ò", "ü•´", "üçù", "üçú", "üç≤", "üçõ", "üç£", "üç±", "ü•ü", "ü¶™", "üç§", "üçô", "üçö", "üçò", "üç•", "ü•†", "ü•Æ", "üç¢", "üç°", "üçß", "üç®", "üç¶", "ü•ß", "üßÅ", "üç∞", "üéÇ", "üçÆ", "üç≠", "üç¨", "üç´", "üçø", "üç©", "üç™", "üå∞", "ü•ú", "üçØ", "ü•õ", "üçº", "‚òïÔ∏è", "üçµ", "üßÉ", "ü•§", "üç∂", "üç∫", "üçª", "ü•Ç", "üç∑", "ü•É", "üç∏", "üçπ", "üßâ", "üçæ"],
                "keywords": "food drink eat meal snack fruit vegetable pizza burger"
            },
            "Travel & Places": {
                "emojis": ["üèîÔ∏è", "‚õ∞Ô∏è", "üåã", "üóª", "üèïÔ∏è", "üèñÔ∏è", "üèúÔ∏è", "üèùÔ∏è", "üèûÔ∏è", "üèüÔ∏è", "üèõÔ∏è", "üèóÔ∏è", "üß±", "üèòÔ∏è", "üèöÔ∏è", "üè†", "üè°", "üè¢", "üè£", "üè§", "üè•", "üè¶", "üè®", "üè©", "üè™", "üè´", "üè¨", "üè≠", "üèØ", "üè∞", "üíí", "üóº", "üóΩ", "‚õ™Ô∏è", "üïå", "üõï", "üïç", "‚õ©Ô∏è", "üïã", "‚õ≤Ô∏è", "‚õ∫Ô∏è", "üåÅ", "üåÉ", "üèôÔ∏è", "üåÑ", "üåÖ", "üåÜ", "üåá", "üåâ", "‚ô®Ô∏è", "üé†", "üé°", "üé¢", "üíà", "üé™", "üöÇ", "üöÉ", "üöÑ", "üöÖ", "üöÜ", "üöá", "üöà", "üöâ", "üöä", "üöù", "üöû", "üöã", "üöå", "üöç", "üöé", "üöê", "üöë", "üöí", "üöì", "üöî", "üöï", "üöñ", "üöó", "üöò", "üöô", "üöö", "üöõ", "üöú", "üèéÔ∏è", "üèçÔ∏è", "üõµ", "ü¶Ω", "ü¶º", "üõ¥", "üö≤", "üõ∫", "üöÅ", "‚úàÔ∏è", "üõ©Ô∏è", "üõ´", "üõ¨", "ü™Ç", "üí∫", "üö¢", "‚õ¥Ô∏è", "üõ•Ô∏è", "üö§", "‚õµÔ∏è", "üõ∂", "üöÅ", "üöü", "üö†", "üö°"],
                "keywords": "travel place location building car plane train bus"
            },
            "Activities & Sports": {
                "emojis": ["‚öΩÔ∏è", "üèÄ", "üèà", "‚öæÔ∏è", "ü•é", "üéæ", "üèê", "üèâ", "ü•è", "üé±", "üèì", "üè∏", "üèí", "üèë", "ü•ç", "üèè", "ü•Ö", "‚õ≥Ô∏è", "ü™Å", "üèπ", "üé£", "ü§ø", "ü•ä", "ü•ã", "üéΩ", "üõπ", "üõ∑", "‚õ∏Ô∏è", "ü•å", "üéø", "‚õ∑Ô∏è", "üèÇ", "ü™Ç", "üèãÔ∏è‚Äç‚ôÄÔ∏è", "üèãÔ∏è", "üèãÔ∏è‚Äç‚ôÇÔ∏è", "ü§º‚Äç‚ôÄÔ∏è", "ü§º", "ü§º‚Äç‚ôÇÔ∏è", "ü§∏‚Äç‚ôÄÔ∏è", "ü§∏", "ü§∏‚Äç‚ôÇÔ∏è", "‚õπÔ∏è‚Äç‚ôÄÔ∏è", "‚õπÔ∏è", "‚õπÔ∏è‚Äç‚ôÇÔ∏è", "ü§∫", "ü§æ‚Äç‚ôÄÔ∏è", "ü§æ", "ü§æ‚Äç‚ôÇÔ∏è", "üèåÔ∏è‚Äç‚ôÄÔ∏è", "üèåÔ∏è", "üèåÔ∏è‚Äç‚ôÇÔ∏è", "üèá", "üßò‚Äç‚ôÄÔ∏è", "üßò", "üßò‚Äç‚ôÇÔ∏è", "üèÑ‚Äç‚ôÄÔ∏è", "üèÑ", "üèÑ‚Äç‚ôÇÔ∏è", "üèä‚Äç‚ôÄÔ∏è", "üèä", "üèä‚Äç‚ôÇÔ∏è", "ü§Ω‚Äç‚ôÄÔ∏è", "ü§Ω", "ü§Ω‚Äç‚ôÇÔ∏è", "üö£‚Äç‚ôÄÔ∏è", "üö£", "üö£‚Äç‚ôÇÔ∏è", "üßó‚Äç‚ôÄÔ∏è", "üßó", "üßó‚Äç‚ôÇÔ∏è", "üöµ‚Äç‚ôÄÔ∏è", "üöµ", "üöµ‚Äç‚ôÇÔ∏è", "üö¥‚Äç‚ôÄÔ∏è", "üö¥", "üö¥‚Äç‚ôÇÔ∏è", "üèÜ", "ü•á", "ü•à", "ü•â", "üèÖ", "üéñÔ∏è", "üèµÔ∏è", "üéóÔ∏è", "üé´", "üéüÔ∏è", "üé™", "ü§π‚Äç‚ôÄÔ∏è", "ü§π", "ü§π‚Äç‚ôÇÔ∏è"],
                "keywords": "sport activity game exercise fitness ball play"
            },
            "Objects & Tech": {
                "emojis": ["‚åöÔ∏è", "üì±", "üì≤", "üíª", "‚å®Ô∏è", "üñ•Ô∏è", "üñ®Ô∏è", "üñ±Ô∏è", "üñ≤Ô∏è", "üïπÔ∏è", "üóúÔ∏è", "üíæ", "üíø", "üìÄ", "üìº", "üì∑", "üì∏", "üìπ", "üé•", "üìΩÔ∏è", "üéûÔ∏è", "üìû", "‚òéÔ∏è", "üìü", "üì†", "üì∫", "üìª", "üéôÔ∏è", "üéöÔ∏è", "üéõÔ∏è", "‚è±Ô∏è", "‚è≤Ô∏è", "‚è∞", "üï∞Ô∏è", "‚åõÔ∏è", "‚è≥", "üì°", "üîã", "üîå", "üí°", "üî¶", "üïØÔ∏è", "üßØ", "üõ¢Ô∏è", "üí∏", "üíµ", "üí¥", "üí∂", "üí∑", "üí∞", "üí≥", "üíé", "‚öñÔ∏è", "ü™ú", "üß∞", "ü™õ", "üîß", "üî®", "‚öíÔ∏è", "üõ†Ô∏è", "‚õèÔ∏è", "ü™ö", "üî©", "‚öôÔ∏è", "ü™§", "üß±", "‚õìÔ∏è", "üß≤", "üî´", "üí£", "üß®", "ü™ì", "üî™", "üó°Ô∏è", "‚öîÔ∏è", "üõ°Ô∏è", "üö¨", "‚ö∞Ô∏è", "ü™¶", "‚ö±Ô∏è", "üè∫", "üîÆ", "üìø", "üßø", "üíà", "‚öóÔ∏è", "üî≠", "üî¨", "üï≥Ô∏è", "ü©π", "ü©∫", "üíä", "üíâ", "ü©∏", "üß¨", "ü¶†", "üß´", "üß™", "üå°Ô∏è", "üßπ", "ü™†", "üß∫", "üßª", "üöΩ", "üö∞", "üöø", "üõÅ", "üõÄ", "üßº", "ü™•", "ü™í", "üßΩ", "ü™£", "üß¥", "üõéÔ∏è", "üîë", "üóùÔ∏è", "üö™", "ü™ë", "üõãÔ∏è", "üõèÔ∏è", "üõå", "üß∏", "ü™Ü", "üñºÔ∏è", "ü™û", "ü™ü", "üõçÔ∏è", "üõí", "üéÅ", "üéà", "üéè", "üéÄ", "ü™Ñ", "ü™Ö", "üéä", "üéâ", "üéé", "üèÆ", "üéê", "üßß", "‚úâÔ∏è", "üì©", "üì®", "üìß", "üíå", "üì•", "üì§", "üì¶", "üè∑Ô∏è", "ü™ß", "üì™", "üì´", "üì¨", "üì≠", "üìÆ", "üìØ", "üìú", "üìÉ", "üìÑ", "üìë", "üßæ", "üìä", "üìà", "üìâ", "üóíÔ∏è", "üóìÔ∏è", "üìÜ", "üìÖ", "üóëÔ∏è", "üìá", "üóÉÔ∏è", "üó≥Ô∏è", "üóÑÔ∏è", "üìã", "üìÅ", "üìÇ", "üóÇÔ∏è", "üóûÔ∏è", "üì∞", "üìì", "üìî", "üìí", "üìï", "üìó", "üìò", "üìô", "üìö", "üìñ", "üîñ", "üß∑", "üîó", "üìé", "üñáÔ∏è", "üìê", "üìè", "üßÆ", "üìå", "üìç", "‚úÇÔ∏è", "üñäÔ∏è", "üñãÔ∏è", "‚úíÔ∏è", "üñåÔ∏è", "üñçÔ∏è", "üìù", "‚úèÔ∏è", "üîç", "üîé", "üîè", "üîê", "üîí", "üîì"],
                "keywords": "computer phone watch money tech device tool office"
            },
            "Symbols & Signs": {
                "emojis": ["‚ù§Ô∏è", "üß°", "üíõ", "üíö", "üíô", "üíú", "üñ§", "ü§ç", "ü§é", "üíî", "‚ù£Ô∏è", "üíï", "üíû", "üíì", "üíó", "üíñ", "üíò", "üíù", "üíü", "‚òÆÔ∏è", "‚úùÔ∏è", "‚ò™Ô∏è", "üïâÔ∏è", "‚ò∏Ô∏è", "‚ú°Ô∏è", "üîØ", "üïé", "‚òØÔ∏è", "‚ò¶Ô∏è", "üõê", "‚õé", "‚ôà", "‚ôâ", "‚ôä", "‚ôã", "‚ôå", "‚ôç", "‚ôé", "‚ôè", "‚ôê", "‚ôë", "‚ôí", "‚ôì", "üÜî", "‚öõÔ∏è", "‚úÖ", "‚òëÔ∏è", "‚úîÔ∏è", "‚ùå", "‚ùé", "‚ûñ", "‚ûó", "‚ûï", "‚û∞", "‚ûø", "„ÄΩÔ∏è", "‚ú≥Ô∏è", "‚ú¥Ô∏è", "‚ùáÔ∏è", "‚ÄºÔ∏è", "‚ÅâÔ∏è", "‚ùì", "‚ùî", "‚ùï", "‚ùó", "üîÖ", "üîÜ", "„Ä∞Ô∏è", "¬©Ô∏è", "¬ÆÔ∏è", "‚Ñ¢Ô∏è", "#Ô∏è‚É£", "*Ô∏è‚É£", "0Ô∏è‚É£", "1Ô∏è‚É£", "2Ô∏è‚É£", "3Ô∏è‚É£", "4Ô∏è‚É£", "5Ô∏è‚É£", "6Ô∏è‚É£", "7Ô∏è‚É£", "8Ô∏è‚É£", "9Ô∏è‚É£", "üîü", "üî†", "üî°", "üî¢", "üî£", "üî§", "üÖ∞Ô∏è", "üÜé", "üÖ±Ô∏è", "üÜë", "üÜí", "üÜì", "‚ÑπÔ∏è", "üÜî", "‚ìÇÔ∏è", "üÜï", "üÜñ", "üÖæÔ∏è", "üÜó", "üÖøÔ∏è", "üÜò", "üÜô", "üÜö", "üàÅ", "üàÇÔ∏è", "üà∑Ô∏è", "üà∂", "üàØ", "üâê", "üàπ", "üà≤", "üâë", "üà∏", "üà¥", "üà≥", "„äóÔ∏è", "„äôÔ∏è", "üà∫", "üàµ", "üî¥", "üü†", "üü°", "üü¢", "üîµ", "üü£", "‚ö´", "‚ö™", "üü§", "üî∂", "üî∑", "üî∏", "üîπ", "üî∫", "üîª", "üí†", "üîò", "üî≥", "üî≤"],
                "keywords": "heart love symbol star sign zodiac check mark question"
            },
            "Flags": {
                "emojis": ["üè≥Ô∏è", "üè¥", "üèÅ", "üö©", "üè≥Ô∏è‚Äçüåà", "üè≥Ô∏è‚Äç‚ößÔ∏è", "üá∫üá≥", "üá¶üá´", "üá¶üáΩ", "üá¶üá±", "üá©üáø", "üá¶üá∏", "üá¶üá©", "üá¶üá¥", "üá¶üáÆ", "üá¶üá∂", "üá¶üá¨", "üá¶üá∑", "üá¶üá≤", "üá¶üáº", "üá¶üá∫", "üá¶üáπ", "üá¶üáø", "üáßüá∏", "üáßüá≠", "üáßüá©", "üáßüáß", "üáßüáæ", "üáßüá™", "üáßüáø", "üáßüáØ", "üáßüá≤", "üáßüáπ", "üáßüá¥", "üáßüá¶", "üáßüáº", "üáßüá∑", "üáÆüá¥", "üáªüá¨", "üáßüá≥", "üáßüá¨", "üáßüá´", "üáßüáÆ", "üá∞üá≠", "üá®üá≤", "üá®üá¶", "üáÆüá∂", "üáÆüá∏", "üáÆüá∑", "üáÆüá™", "üáÆüá≤", "üáÆüá±", "üáÆüáπ", "üáØüá≤", "üáØüáµ", "üáØüá™", "üáØüá¥", "üá∞üáø", "üá∞üá™", "üá∞üáÆ", "üá∞üáº", "üá∞üá¨", "üá±üá¶", "üá±üáª", "üá±üáß", "üá±üá∏", "üá±üá∑", "üá±üáæ", "üá±üáÆ", "üá±üáπ", "üá±üá∫", "üá≤üá¥", "üá≤üá¨", "üá≤üáº", "üá≤üáæ", "üá≤üáª", "üá≤üá±", "üá≤üáπ", "üá≤üá≠", "üá≤üá∂", "üá≤üá∑", "üá≤üá∫", "üáæüáπ", "üá≤üáΩ", "üá´üá≤", "üá≤üá©", "üá≤üá®", "üá≤üá≥", "üá≤üá™", "üá≤üá∏", "üá≤üá¶", "üá≤üáø", "üá≤üá≤", "üá≥üá¶", "üá≥üá∑", "üá≥üáµ", "üá≥üá±", "üá≥üá®", "üá≥üáø", "üá≥üáÆ", "üá≥üá™", "üá≥üá¨", "üá≥üá∫", "üá≥üá´", "üá∞üáµ", "üá≤üá∞", "üá≤üáµ", "üá≥üá¥", "üá¥üá≤", "üáµüá∞", "üáµüáº", "üáµüá∏", "üáµüá¶", "üáµüá¨", "üáµüáæ", "üáµüá™", "üáµüá≠", "üáµüá≥", "üáµüá±", "üáµüáπ", "üáµüá∑", "üá∂üá¶", "üá∑üá™", "üá∑üá¥", "üá∑üá∫", "üá∑üáº", "üáºüá∏", "üá∏üá≤", "üá∏üá¶", "üá∏üá≥", "üá∑üá∏", "üá∏üá®", "üá∏üá±", "üá∏üá¨", "üá∏üá∞", "üá∏üáÆ", "üá¨üá∏", "üá∏üáß", "üá∏üá¥", "üáøüá¶", "üá∞üá∑", "üá∏üá∏", "üá™üá∏", "üá±üá∞", "üáßüá±", "üá∏üá≠", "üá∞üá≥", "üá±üá®", "üáµüá≤", "üáªüá®", "üá∏üá©", "üá∏üá∑", "üá∏üá™", "üá®üá≠", "üá∏üáæ", "üáπüáº", "üáπüáØ", "üáπüáø", "üáπüá≠", "üáπüá±", "üáπüá¨", "üáπüá∞", "üáπüá¥", "üáπüáπ", "üáπüá≥", "üáπüá∑", "üáπüá≤", "üáπüá®", "üáπüáª", "üáªüáÆ", "üá∫üá¨", "üá∫üá¶", "üá¶üá™", "üá¨üáß", "üá∫üá∏", "üá∫üáæ", "üá∫üáø", "üáªüá∫", "üáªüá¶", "üáªüá™", "üáªüá≥", "üáºüá´", "üá™üá≠", "üáæüá™", "üáøüá≤", "üáøüáº", "üè¥‚Äç‚ò†Ô∏è"],
                "keywords": "flag country nation world country"
            },
            "Weather & Nature": {
                "emojis": ["‚òÄÔ∏è", "üå§Ô∏è", "‚õÖÔ∏è", "üå•Ô∏è", "‚òÅÔ∏è", "üå¶Ô∏è", "üåßÔ∏è", "‚õàÔ∏è", "üå©Ô∏è", "‚ö°Ô∏è", "‚òîÔ∏è", "‚ùÑÔ∏è", "‚òÉÔ∏è", "‚õÑÔ∏è", "üå®Ô∏è", "üí®", "üíß", "üí¶", "‚òÇÔ∏è", "‚òîÔ∏è", "üåä", "üå´Ô∏è", "üî•", "‚≠ê", "üåü", "üí´", "‚ú®", "‚ö°", "‚òÑÔ∏è", "üí•", "üåô", "üåö", "üåõ", "üåú", "üåë", "üåí", "üåì", "üåî", "üåï", "üåñ", "üåó", "üåò", "üåç", "üåé", "üåè", "üåê", "üó∫Ô∏è", "üß≠", "üèîÔ∏è", "‚õ∞Ô∏è", "üåã", "üóª", "üèïÔ∏è", "üèñÔ∏è", "üèúÔ∏è", "üèùÔ∏è", "üèûÔ∏è", "üå≤", "üå≥", "üå¥", "üåµ", "üåæ", "üåø", "‚òòÔ∏è", "üçÄ", "üçÅ", "üçÇ", "üçÉ", "üå∫", "üåª", "üåπ", "üå∑", "üå±", "üåº", "üå∏", "üíê", "üåæ", "üå∑", "ü•Ä", "üåπ", "üåª", "üå∫", "üåº", "üå∏", "üåø", "üå±", "üçÉ", "üçÇ", "üçÅ", "üçÄ", "‚òòÔ∏è"],
                "keywords": "weather sun rain snow cloud star moon earth nature"
            },
        }
        
        dialog = QDialog(self)
        dialog.setWindowTitle("üòä Emoji Picker")
        dialog.setMinimumSize(450, 600)
        dialog.setMaximumSize(600, 700)
        dialog.setStyleSheet("""
            QDialog {
                background-color: #333;
            }
        """)
        
        layout = QVBoxLayout(dialog)
        layout.setContentsMargins(15, 15, 15, 15)
        layout.setSpacing(10)
        
        # Title
        title = QLabel("Select an emoji:")
        title.setStyleSheet("font-size: 16px; font-weight: 600; color: #FFF; margin-bottom: 5px;")
        layout.addWidget(title)
        
        # Search box
        search_box = QLineEdit()
        if USE_EMOJI_PACKAGE:
            search_box.setPlaceholderText("üîç Search all emojis (e.g., 'smile', 'heart', 'thumbs up', 'check', 'fire', 'party')...")
        else:
            search_box.setPlaceholderText("üîç Search emojis (try: smile, heart, thumbs, check, star, party)...")
        search_box.setStyleSheet("""
            QLineEdit {
                background-color: #222;
                color: #FFF;
                border: 2px solid #555;
                border-radius: 6px;
                padding: 8px;
                font-size: 14px;
            }
            QLineEdit:focus {
                border: 2px solid #0078D7;
            }
        """)
        layout.addWidget(search_box)
        
        # Emoji list
        list_widget = QListWidget()
        list_widget.setStyleSheet("""
            QListWidget {
                background-color: #222;
                color: #FFF;
                border: 1px solid #555;
                border-radius: 4px;
                font-size: 28px;
                padding: 5px;
            }
            QListWidget::item {
                padding: 10px;
                border-bottom: 1px solid #333;
                min-height: 40px;
            }
            QListWidget::item:hover {
                background-color: #444;
            }
            QListWidget::item:selected {
                background-color: #0078D7;
            }
        """)
        
        # Store all emoji items with their data for filtering
        all_items = []
        
        # Build emoji name lookup if using emoji package
        emoji_name_lookup = {}
        if USE_EMOJI_PACKAGE:
            try:
                for emoji_char, emoji_info in emoji.EMOJI_DATA.items():
                    aliases = emoji_info.get('alias', [])
                    if aliases:
                        # Create searchable text from all aliases
                        search_text = ' '.join([a.replace(':', '').replace('_', ' ') for a in aliases]).lower()
                        emoji_name_lookup[emoji_char] = search_text
            except Exception as e:
                logging.warning(f"Error building emoji lookup: {e}")
        
        def populate_list(filter_text=""):
            """Populate the list with emojis, optionally filtered"""
            nonlocal all_items, list_widget
            list_widget.clear()
            all_items.clear()
            filter_lower = filter_text.lower().strip()
            
            # Common search term mappings for better results
            search_term_mappings = {
                "thumbs": "thumbs up",
                "thumb": "thumbs up",
                "ok": "ok hand",
                "check": "check mark",
                "x": "cross mark",
                "star": "star",
                "fire": "fire",
                "100": "hundred",
                "party": "party popper",
                "celebrate": "party popper",
                "cake": "birthday cake",
                "happy": "smiling",
                "sad": "crying",
                "love": "red heart",
                "heart": "heart",
                "smile": "smiling",
                "laugh": "laughing",
                "cry": "crying",
                "angry": "angry",
                "wink": "winking",
                "kiss": "kissing",
                "cool": "smiling face with sunglasses",
                "money": "money",
                "phone": "mobile phone",
                "computer": "laptop",
                "car": "automobile",
                "plane": "airplane",
                "food": "food",
                "pizza": "pizza",
                "coffee": "hot beverage",
                "beer": "beer mug",
                "wine": "wine glass",
                "music": "musical note",
                "sport": "soccer ball",
                "ball": "soccer ball",
                "dog": "dog face",
                "cat": "cat face",
                "bird": "bird",
                "sun": "sun",
                "rain": "rain",
                "snow": "snow",
                "moon": "moon",
                "earth": "globe",
                "world": "globe"
            }
            
            # Expand search terms
            expanded_search = filter_lower
            for term, mapped in search_term_mappings.items():
                if term in filter_lower:
                    expanded_search += " " + mapped
            
            # Split search into individual words for better matching (used in both search and category filtering)
            search_words = filter_lower.split()
            expanded_words = expanded_search.split()
            
            # If using emoji package and search text provided, search by name
            if USE_EMOJI_PACKAGE and filter_lower and emoji_name_lookup:
                # Search across all emojis by name with better matching
                matching_emojis = []
                # Prioritize most used emojis in search results
                most_used_matches = []
                other_matches = []
                
                for emoji_char, search_text in emoji_name_lookup.items():
                    # Multiple matching strategies:
                    # 1. Exact phrase match (highest priority)
                    # 2. All words present (high priority)
                    # 3. Any word present (medium priority)
                    # 4. Partial substring match (lower priority)
                    
                    match_score = 0
                    search_text_lower = search_text.lower()
                    
                    # Exact phrase match
                    if filter_lower in search_text_lower:
                        match_score = 100
                    # All words present
                    elif all(word in search_text_lower for word in search_words if len(word) > 2):
                        match_score = 80
                    # Any word present
                    elif any(word in search_text_lower for word in search_words if len(word) > 2):
                        match_score = 60
                    # Expanded terms match
                    elif any(term in search_text_lower for term in expanded_words):
                        match_score = 50
                    # Partial substring match
                    elif any(word[:3] in search_text_lower for word in search_words if len(word) >= 3):
                        match_score = 30
                    
                    if match_score > 0:
                        if emoji_char in most_used_emojis:
                            # Boost score for most used emojis
                            match_score += 20
                            most_used_matches.append((emoji_char, match_score))
                        else:
                            other_matches.append((emoji_char, match_score))
                
                # Sort by match score (highest first)
                most_used_matches.sort(key=lambda x: x[1], reverse=True)
                other_matches.sort(key=lambda x: x[1], reverse=True)
                
                # Combine results: most used first (sorted by score), then others (sorted by score)
                matching_emojis = [e[0] for e in most_used_matches] + [e[0] for e in other_matches]
                
                if matching_emojis:
                    # Show matching emojis
                    category_item = QListWidgetItem(f"  üîç Search Results ({len(matching_emojis)} found)")
                    from PyQt6.QtCore import Qt
                    category_item.setFlags(category_item.flags() & ~Qt.ItemFlag.ItemIsSelectable & ~Qt.ItemFlag.ItemIsEnabled)
                    category_item.setBackground(QColor(80, 80, 80))
                    category_item.setForeground(QColor(200, 200, 200))
                    list_widget.addItem(category_item)
                    
                    # Show up to 500 results for better search experience
                    for emoji_char in matching_emojis[:500]:
                        emoji_item = QListWidgetItem(emoji_char)
                        emoji_item.setData(Qt.ItemDataRole.UserRole, emoji_char)
                        # Add tooltip with emoji name
                        emoji_name = emoji_name_lookup.get(emoji_char, '')
                        if emoji_name:
                            emoji_item.setToolTip(emoji_name.title())
                        list_widget.addItem(emoji_item)
                    return
            
            # Otherwise, show by category (original behavior)
            for category, data in emojis_data.items():
                emoji_list = data["emojis"]
                keywords = data["keywords"]
                
                # Better filtering: show category if search matches category name, keywords, or any emoji
                # Also check if any search word matches keywords
                show_category = False
                if not filter_lower:
                    show_category = True
                else:
                    # Check category name
                    if filter_lower in category.lower():
                        show_category = True
                    # Check keywords
                    elif any(word in keywords.lower() for word in search_words if len(word) > 2):
                        show_category = True
                    # Check if search term appears in emoji list (for emoji character search)
                    elif any(emoji in filter_text for emoji in emoji_list):
                        show_category = True
                    # Check expanded search terms
                    elif any(term in keywords.lower() for term in expanded_words):
                        show_category = True
                
                if show_category:
                    # Add category header
                    category_item = QListWidgetItem(f"  {category} ({len(emoji_list)})")
                    # Disable selection and enable for category headers
                    from PyQt6.QtCore import Qt
                    category_item.setFlags(category_item.flags() & ~Qt.ItemFlag.ItemIsSelectable & ~Qt.ItemFlag.ItemIsEnabled)
                    category_item.setBackground(QColor(80, 80, 80))
                    category_item.setForeground(QColor(200, 200, 200))
                    list_widget.addItem(category_item)
                    all_items.append(category_item)
                    
                    # Add emojis in this category
                    for emoji_char in emoji_list:
                        emoji_item = QListWidgetItem(emoji_char)
                        emoji_item.setData(Qt.ItemDataRole.UserRole, emoji_char)
                        emoji_item.setData(Qt.ItemDataRole.UserRole + 1, category)  # Store category
                        # Add tooltip with emoji name if available
                        if USE_EMOJI_PACKAGE and emoji_char in emoji_name_lookup:
                            emoji_name = emoji_name_lookup[emoji_char]
                            emoji_item.setToolTip(emoji_name.title())
                        list_widget.addItem(emoji_item)
                        all_items.append(emoji_item)
        
        # Initial population
        populate_list()
        
        # Search functionality
        def on_search_changed(text):
            populate_list(text)
        
        search_box.textChanged.connect(on_search_changed)
        
        # Buttons
        buttons = QHBoxLayout()
        
        def insert_emoji():
            current_item = list_widget.currentItem()
            if current_item:
                emoji = current_item.data(Qt.ItemDataRole.UserRole)
                if emoji:
                    # Insert emoji at cursor position
                    cursor = self.input_box.textCursor()
                    cursor.insertText(emoji)
                    self.input_box.setTextCursor(cursor)
                    dialog.accept()
        
        list_widget.itemDoubleClicked.connect(insert_emoji)
        
        # Allow Enter key to insert selected emoji
        def on_enter_pressed():
            insert_emoji()
        
        search_box.returnPressed.connect(lambda: list_widget.setFocus() if list_widget.count() > 0 else None)
        list_widget.itemActivated.connect(insert_emoji)
        
        insert_btn = QPushButton("Insert")
        insert_btn.clicked.connect(insert_emoji)
        insert_btn.setStyleSheet("background-color: #0078D7; padding: 8px 16px; border-radius: 4px; font-weight: 600; font-size: 14px;")
        buttons.addWidget(insert_btn)
        
        buttons.addStretch()
        
        close_btn = QPushButton("Close")
        close_btn.clicked.connect(dialog.accept)
        close_btn.setStyleSheet("background-color: #555; padding: 8px 16px; border-radius: 4px; font-size: 14px;")
        buttons.addWidget(close_btn)
        
        layout.addWidget(list_widget, stretch=1)
        layout.addLayout(buttons)
        
        # Set focus to search box
        search_box.setFocus()
        
        dialog.exec()
    
    def clear_conversation(self):
        reply = QMessageBox.question(self, "Clear", "Clear conversation?",
                                     QMessageBox.StandardButton.Yes | QMessageBox.StandardButton.No)
        if reply == QMessageBox.StandardButton.Yes:
            self.message_history = []
            self.chat_display.clear()
            self.current_file_content = None
            self.current_file_path = None
            self.file_label.setText("")
            self._save_history()
            self.append_message("system", "Conversation cleared")
    
    def toggle_speech_recognition(self):
        """Start or stop speech recognition"""
        # Check import directly instead of relying on global variable
        try:
            import speech_recognition as sr
        except ImportError:
            QMessageBox.warning(self, "Speech Recognition Unavailable", 
                              "Speech recognition is not available. Please install with:\npip install SpeechRecognition")
            return
        
        if self.is_listening:
            # Stop listening (currently active)
            self.status_label.setText("Ready")
            self.mic_btn.setText("üé§")
            self.mic_btn.setToolTip("Click to speak your message")
            self.is_listening = False
            return
        
        # Check if microphone setup has been done before
        # Only prompt on first use, not if user has already configured it (even if None/default)
        needs_setup = False
        try:
            # Check if settings file exists and has microphone_device_index key
            # If key doesn't exist, user hasn't set up microphone yet
            if not self.settings_file.exists():
                needs_setup = True
            else:
                with open(self.settings_file, 'r', encoding='utf-8') as f:
                    settings_data = json.load(f)
                    if 'microphone_device_index' not in settings_data:
                        needs_setup = True
        except (json.JSONDecodeError, IOError, Exception):
            # If error reading settings, assume first time
            needs_setup = True
        
        if needs_setup:
            if not self._setup_microphone_first_time():
                return  # User cancelled microphone setup
        
        # Start listening
        self.is_listening = True
        self.mic_btn.setText("üî¥")
        self.mic_btn.setToolTip("Listening... Click again to stop")
        
        # Clean up any existing speech worker thread
        try:
            if hasattr(self, 'speech_worker_thread') and self.speech_worker_thread is not None:
                try:
                    if hasattr(self, 'speech_worker') and self.speech_worker is not None:
                        try:
                            self.speech_worker.finished.disconnect()
                            self.speech_worker.error.disconnect()
                            self.speech_worker.listening.disconnect()
                        except:
                            pass
                except:
                    pass
        except:
            pass
        
        # Start speech recognition worker thread
        self.speech_worker_thread = QThread()
        self.speech_worker = SpeechRecognitionWorker(device_index=self.microphone_device_index)
        self.speech_worker.moveToThread(self.speech_worker_thread)
        
        # Store references
        self._speech_worker = self.speech_worker
        self._speech_worker_thread = self.speech_worker_thread
        
        self.speech_worker_thread.started.connect(self.speech_worker.run)
        self.speech_worker.finished.connect(self.on_speech_recognition_finished)
        self.speech_worker.error.connect(self.on_speech_recognition_error)
        self.speech_worker.listening.connect(self.on_speech_listening)
        self.speech_worker.finished.connect(self.speech_worker_thread.quit)
        self.speech_worker.error.connect(self.speech_worker_thread.quit)
        
        def safe_delete_speech_worker():
            try:
                if hasattr(self, '_speech_worker') and self._speech_worker:
                    self._speech_worker.deleteLater()
                    self._speech_worker = None
            except:
                pass
        
        def safe_delete_speech_thread():
            try:
                if hasattr(self, '_speech_worker_thread') and self._speech_worker_thread:
                    self._speech_worker_thread.deleteLater()
                    self._speech_worker_thread = None
                    if hasattr(self, 'speech_worker_thread'):
                        self.speech_worker_thread = None
            except:
                pass
        
        self.speech_worker_thread.finished.connect(safe_delete_speech_worker)
        self.speech_worker_thread.finished.connect(safe_delete_speech_thread)
        self.speech_worker_thread.start()
    
    def _setup_microphone_first_time(self):
        """Prompt user to select a microphone for first-time setup"""
        # Check import directly
        try:
            import speech_recognition as sr
        except ImportError:
            return False
        
        reply = QMessageBox.question(
            self, 
            "Microphone Setup", 
            "No microphone selected.\n\nWould you like to select a microphone now?",
            QMessageBox.StandardButton.Yes | QMessageBox.StandardButton.No
        )
        
        if reply == QMessageBox.StandardButton.No:
            QMessageBox.information(
                self,
                "Microphone Setup",
                "You can configure your microphone later in Settings (‚öôÔ∏è button)."
            )
            return False
        
        # Show microphone selection dialog
        dialog = QDialog(self)
        dialog.setWindowTitle("üé§ Select Microphone")
        dialog.setMinimumSize(500, 300)
        dialog.setStyleSheet("""
            QDialog {
                background-color: #333;
            }
        """)
        
        layout = QVBoxLayout(dialog)
        layout.setContentsMargins(20, 20, 20, 20)
        layout.setSpacing(15)
        
        # Title
        title = QLabel("Select your microphone:")
        title.setStyleSheet("font-size: 16px; font-weight: 600; color: #FFF; margin-bottom: 10px;")
        layout.addWidget(title)
        
        info_label = QLabel("Please select the microphone you want to use for voice input:")
        info_label.setStyleSheet("color: #CCC; font-size: 12px; margin-bottom: 10px;")
        layout.addWidget(info_label)
        
        mic_combo = QComboBox()
        mic_combo.setStyleSheet("""
            QComboBox {
                background-color: #222;
                color: #FFF;
                border: 2px solid #555;
                border-radius: 4px;
                padding: 8px;
                font-size: 14px;
            }
        """)
        
        mic_devices = []
        # Check import directly instead of global variable
        try:
            import speech_recognition as sr
            mic_list = sr.Microphone.list_microphone_names()
            if mic_list:
                mic_combo.addItem("Default (System Default)", None)
                for i, mic_name in enumerate(mic_list):
                    mic_combo.addItem(f"{i}: {mic_name}", i)
                    mic_devices.append((i, mic_name))
            else:
                QMessageBox.warning(
                    self,
                    "No Microphones Found",
                    "No microphones were detected on your system.\n\n"
                    "Please check:\n"
                    "1. Your microphone is connected\n"
                    "2. Microphone permissions are granted\n"
                    "3. Windows Privacy settings allow microphone access"
                )
                dialog.close()
                return False
        except ImportError:
            QMessageBox.warning(self, "Speech Recognition Unavailable", 
                              "Speech recognition is not available. Please install with:\npip install SpeechRecognition")
            dialog.close()
            return False
        except Exception as e:
            logging.error(f"Error listing microphones: {e}")
            QMessageBox.warning(
                self,
                "Microphone Error",
                f"Error detecting microphones:\n{str(e)}\n\n"
                "Please check your microphone connection and try again."
            )
            dialog.close()
            return False
        
        layout.addWidget(mic_combo)
        layout.addStretch()
        
        # Buttons
        buttons = QDialogButtonBox(QDialogButtonBox.StandardButton.Ok | QDialogButtonBox.StandardButton.Cancel)
        buttons.accepted.connect(dialog.accept)
        buttons.rejected.connect(dialog.reject)
        buttons.setStyleSheet("""
            QPushButton {
                background-color: #0078D7;
                color: #FFF;
                border: none;
                border-radius: 4px;
                padding: 8px 20px;
                font-size: 14px;
            }
            QPushButton:hover {
                background-color: #106ebe;
            }
        """)
        layout.addWidget(buttons)
        
        result = dialog.exec()
        if result == QDialog.DialogCode.Accepted:
            selected_index = mic_combo.currentData()
            self.microphone_device_index = selected_index
            self.save_settings()
            
            QMessageBox.information(
                self,
                "Microphone Selected",
                f"Microphone selected successfully!\n\n"
                f"Selected: {mic_combo.currentText()}\n\n"
                "You can change this later in Settings (‚öôÔ∏è button)."
            )
            return True
        else:
            return False
    
    def on_speech_listening(self):
        """Called when speech recognition starts listening"""
        if self.voice_only_mode:
            self.status_label.setText("üé§ VOICE MODE: Listening... Speak now")
            self.status_label.setStyleSheet("color: #00FF00; font-size: 14px; font-weight: bold;")
        else:
            self.status_label.setText("üé§ Listening... Speak now")
    
    def on_speech_recognition_finished(self, text):
        """Called when speech is successfully recognized"""
        try:
            # Reset listening state
            self.is_listening = False
            self.mic_btn.setText("üé§")
            self.mic_btn.setToolTip("Click to speak your message")
            
            # Insert recognized text into input box
            if text and text.strip():
                # If there's existing text, append with a space
                current_text = self.input_box.toPlainText()
                if current_text and current_text.strip():
                    self.input_box.setPlainText(current_text + " " + text.strip())
                else:
                    self.input_box.setPlainText(text.strip())
                
                # Move cursor to end
                cursor = self.input_box.textCursor()
                cursor.movePosition(cursor.MoveOperation.End)
                self.input_box.setTextCursor(cursor)
                
                # If continuous listening is enabled, auto-send and restart listening
                if self.continuous_listening:
                    # Auto-send the message
                    QApplication.processEvents()  # Process UI updates first
                    self.on_send()
                    # Note: Listening will restart in on_worker_finished if continuous_listening is True
                else:
                    self.status_label.setText("Ready")
            else:
                self.status_label.setText("No speech detected")
                # If continuous listening, restart even if no speech detected
                if self.continuous_listening:
                    QApplication.processEvents()
                    self._restart_listening_after_delay(500)  # Wait 500ms before restarting
            
            # Clean up reference
            try:
                if hasattr(self, '_speech_worker'):
                    self._speech_worker = None
            except:
                pass
        except Exception as e:
            logging.error(f"Error in on_speech_recognition_finished: {traceback.format_exc()}")
            self.status_label.setText("Error processing speech")
            # Restart listening on error if continuous mode
            if self.continuous_listening:
                self._restart_listening_after_delay(1000)
    
    def on_speech_recognition_error(self, error_msg):
        """Called when speech recognition encounters an error"""
        try:
            # Reset listening state
            self.is_listening = False
            self.mic_btn.setText("üé§")
            self.mic_btn.setToolTip("Click to speak your message")
            
            error_text = str(error_msg) if error_msg else "Unknown error"
            self.status_label.setText(f"Speech error: {error_text}")
            
            # Only show message box for significant errors, not for "try again" type messages
            if "not available" in error_text.lower() or "service error" in error_text.lower():
                QMessageBox.warning(self, "Speech Recognition Error", error_text)
            
            # Clean up reference
            try:
                if hasattr(self, '_speech_worker'):
                    self._speech_worker = None
            except:
                pass
        except Exception as e:
            logging.error(f"Error in on_speech_recognition_error: {traceback.format_exc()}")
            try:
                self.status_label.setText("Error handling speech recognition")
            except:
                pass
    
    def _restart_listening_after_delay(self, delay_ms: int = 500):
        """Restart listening after a delay (for continuous mode)"""
        if not self.continuous_listening:
            return
        
        # Use QTimer to restart after delay
        timer = QTimer(self)
        timer.setSingleShot(True)
        timer.timeout.connect(lambda: self._safe_restart_listening())
        timer.start(delay_ms)
    
    def _safe_restart_listening(self):
        """Safely restart listening (check if not already listening)"""
        try:
            if not self.is_listening and self.continuous_listening:
                # Check import directly
                try:
                    import speech_recognition as sr
                    self.toggle_speech_recognition()
                except ImportError:
                    logging.warning("Speech recognition not available for restart")
        except Exception as e:
            logging.warning(f"Error restarting listening: {e}")
    
    def keyPressEvent(self, event):
        """Handle keyboard shortcuts for push-to-talk"""
        # Check for push-to-talk key (default: Space when input box doesn't have focus)
        if self.push_to_talk_key:
            try:
                # Parse key sequence (e.g., "Space", "Ctrl+Space")
                key_seq = QKeySequence(self.push_to_talk_key)
                if key_seq.matches(event.keyCombination()):
                    # Start listening if not already
                    if not self.is_listening and SPEECH_RECOGNITION_AVAILABLE:
                        self.toggle_speech_recognition()
                    event.accept()
                    return
            except:
                pass
        
        # Call parent keyPressEvent for other keys
        super().keyPressEvent(event)
    
    def keyReleaseEvent(self, event):
        """Handle key release for push-to-talk"""
        if self.push_to_talk_key and not self.continuous_listening:
            try:
                key_seq = QKeySequence(self.push_to_talk_key)
                if key_seq.matches(event.keyCombination()):
                    # Stop listening on key release (only if not continuous mode)
                    if self.is_listening:
                        self.toggle_speech_recognition()  # This will stop it
                    event.accept()
                    return
            except:
                pass
        
        super().keyReleaseEvent(event)
    
    def _setup_avatar_position(self):
        """Position avatar widget in the center of chat display"""
        if hasattr(self, 'avatar_widget') and hasattr(self, 'chat_display'):
            chat_rect = self.chat_display.geometry()
            avatar_size = 120
            x = (chat_rect.width() - avatar_size) // 2
            y = (chat_rect.height() - avatar_size) // 2
            self.avatar_widget.setGeometry(x, y, avatar_size, avatar_size)
    
    def resizeEvent(self, event):
        """Update avatar position when window is resized"""
        super().resizeEvent(event)
        if hasattr(self, 'avatar_widget'):
            QTimer.singleShot(50, self._setup_avatar_position)
    
    def _show_avatar(self):
        """Show and animate the avatar"""
        if not hasattr(self, 'avatar_widget'):
            return
        
        # Update position in case window was resized
        self._setup_avatar_position()
        
        self.avatar_widget.show()
        self.avatar_widget.raise_()
        
        # Set initial style with glow effect
        self.avatar_widget.setStyleSheet("""
            QLabel {
                background-color: rgba(104, 189, 71, 0.15);
                border: 4px solid rgba(104, 189, 71, 0.6);
                border-radius: 60px;
                color: #68BD47;
            }
        """)
        
        # Start pulsing opacity animation
        self.avatar_opacity.setStartValue(0.5)
        self.avatar_opacity.setEndValue(1.0)
        self.avatar_opacity.start()
    
    def _hide_avatar(self):
        """Hide the avatar and stop animations"""
        if not hasattr(self, 'avatar_widget'):
            return
        
        self.avatar_opacity.stop()
        self.avatar_widget.hide()
    
    def _check_environment(self):
        """Check environment at startup: F: drive, iCloud, TTS, speech availability"""
        status_parts = []
        
        # Check F: drive availability
        f_drive_available = Path("F:/").exists()
        if f_drive_available:
            status_parts.append("F: ‚úì")
        else:
            status_parts.append("F: ‚úó")
            logging.warning("F: drive not available - backups to F: drive will fail")
        
        # Check iCloud backup directory
        icloud_accessible = False
        try:
            BACKUP_DIR_ICLOUD.mkdir(parents=True, exist_ok=True)
            icloud_accessible = BACKUP_DIR_ICLOUD.exists()
            if icloud_accessible:
                status_parts.append("iCloud ‚úì")
            else:
                status_parts.append("iCloud ‚úó")
                logging.warning(f"iCloud backup directory not accessible: {BACKUP_DIR_ICLOUD}")
        except Exception as e:
            status_parts.append("iCloud ‚úó")
            logging.warning(f"Cannot create iCloud backup directory: {e}")
        
        # Check TTS availability
        if TTS_AVAILABLE:
            if EDGE_TTS_AVAILABLE:
                status_parts.append("TTS ‚úì")
            elif GTTS_AVAILABLE:
                status_parts.append("TTS (gTTS) ‚úì")
        else:
            status_parts.append("TTS ‚úó")
            logging.warning("TTS not available")
        
        # Check speech recognition availability
        if SPEECH_RECOGNITION_AVAILABLE:
            status_parts.append("Speech ‚úì")
        else:
            status_parts.append("Speech ‚úó")
            logging.warning("Speech recognition not available")
        
        # Update status label
        status_text = " | ".join(status_parts)
        if hasattr(self, 'status_label'):
            self.status_label.setText(f"Status: {status_text}")
        
        # Log summary
        logging.info(f"Environment check: {status_text}")
        
        # Show warning if critical components missing
        if not f_drive_available:
            logging.error("‚ö†Ô∏è CRITICAL: F: drive not available - backup system compromised")
        if not icloud_accessible:
            logging.error("‚ö†Ô∏è CRITICAL: iCloud backup directory not accessible - backup system compromised")
    
    def _setup_automatic_reports(self):
        """Setup automatic daily email report on startup and 5 PM calendar report"""
        try:
            # Check if daily email report has been run today
            today = date.today().isoformat()
            last_email_report = self.settings_file.parent / "last_email_report_date.txt"
            
            # Run daily email report if not run today
            if not last_email_report.exists() or last_email_report.read_text().strip() != today:
                logging.info("Running automatic daily email report on startup...")
                QTimer.singleShot(2000, self._run_daily_email_report)  # Wait 2 seconds after startup
            
            # Setup 5 PM calendar report timer
            self._setup_5pm_calendar_timer()
            
        except Exception as e:
            logging.error(f"Error setting up automatic reports: {e}")
    
    def _run_daily_email_report(self):
        """Run the daily email report for yesterday"""
        try:
            # Switch to Executive Assistant mode for email reports
            if self.current_mode != "Executive Assistant & Operations":
                self.current_mode = "Executive Assistant & Operations"
                if hasattr(self, 'mode_combo'):
                    self.mode_combo.setCurrentText("Executive Assistant & Operations")
            
            # Get yesterday's date
            yesterday = date.today() - timedelta(days=1)
            yesterday_str = yesterday.strftime("%Y-%m-%d")
            
            # Create request message
            request_text = f"Please create my daily email report for yesterday ({yesterday_str})"
            
            # Add system message
            self.append_message("system", f"Running automatic daily email report for {yesterday_str}...")
            
            # Set the input box text and trigger send
            if hasattr(self, 'input_box'):
                self.input_box.setPlainText(request_text)
                # Use QTimer to trigger send after a brief delay to ensure UI is ready
                QTimer.singleShot(500, lambda: self.on_send())
            
            # Mark report as run today (will be updated after successful completion)
            last_email_report = self.settings_file.parent / "last_email_report_date.txt"
            last_email_report.write_text(date.today().isoformat())
            
            logging.info(f"Daily email report request triggered for {yesterday_str}")
            
        except Exception as e:
            logging.error(f"Error running daily email report: {e}")
            self.append_message("system", f"Error running automatic daily email report: {str(e)}")
    
    def _setup_5pm_calendar_timer(self):
        """Setup timer to check for 5 PM and run calendar report"""
        try:
            # Create a timer that checks every minute
            self.calendar_report_timer = QTimer(self)
            self.calendar_report_timer.timeout.connect(self._check_and_run_5pm_calendar_report)
            self.calendar_report_timer.start(60000)  # Check every 60 seconds (1 minute)
            
            # Also check immediately in case it's already past 5 PM
            QTimer.singleShot(1000, self._check_and_run_5pm_calendar_report)
            
            logging.info("5 PM calendar report timer started")
            
        except Exception as e:
            logging.error(f"Error setting up 5 PM calendar timer: {e}")
    
    def _check_and_run_5pm_calendar_report(self):
        """Check if it's 5 PM (or later) and run calendar report if not already run today"""
        try:
            now = datetime.now()
            current_time = now.time()
            today = date.today().isoformat()
            
            # Check if it's 5 PM or later (but before midnight)
            target_time = time(17, 0)  # 5:00 PM
            if current_time >= target_time:
                # Check if calendar report has been run today
                last_calendar_report = self.settings_file.parent / "last_calendar_report_date.txt"
                
                if not last_calendar_report.exists() or last_calendar_report.read_text().strip() != today:
                    # Run the calendar report
                    logging.info("Running automatic 5 PM calendar report...")
                    self._run_5pm_calendar_report()
                    
                    # Mark as run today
                    last_calendar_report.write_text(today)
            
        except Exception as e:
            logging.error(f"Error checking 5 PM calendar report: {e}")
    
    def _run_5pm_calendar_report(self):
        """Run the next-day calendar report"""
        try:
            # Switch to Executive Assistant mode for calendar reports
            if self.current_mode != "Executive Assistant & Operations":
                self.current_mode = "Executive Assistant & Operations"
                if hasattr(self, 'mode_combo'):
                    self.mode_combo.setCurrentText("Executive Assistant & Operations")
            
            # Get tomorrow's date
            tomorrow = date.today() + timedelta(days=1)
            tomorrow_str = tomorrow.strftime("%Y-%m-%d")
            
            # Create request message
            request_text = f"Please summarize my calendar and Bryant's calendar for tomorrow ({tomorrow_str})"
            
            # Add system message
            self.append_message("system", f"Running automatic 5 PM calendar report for tomorrow ({tomorrow_str})...")
            
            # Set the input box text and trigger send
            if hasattr(self, 'input_box'):
                self.input_box.setPlainText(request_text)
                # Use QTimer to trigger send after a brief delay to ensure UI is ready
                QTimer.singleShot(500, lambda: self.on_send())
            
            logging.info(f"5 PM calendar report request triggered for {tomorrow_str}")
            
        except Exception as e:
            logging.error(f"Error running 5 PM calendar report: {e}")
            self.append_message("system", f"Error running automatic 5 PM calendar report: {str(e)}")
    
    def _on_tts_started(self):
        """Handle TTS started signal - update UI on main thread"""
        try:
            if self.voice_only_mode:
                self.status_label.setText("üîä VOICE MODE: Lea is speaking...")
                self.status_label.setStyleSheet("color: #00BFFF; font-size: 14px; font-weight: bold;")
            self._show_avatar()
        except Exception as e:
            logging.warning(f"Error in _on_tts_started: {e}")
    
    def _on_tts_finished(self):
        """Handle TTS finished signal - update UI on main thread"""
        try:
            self._hide_avatar()
            if self.voice_only_mode:
                self.status_label.setText("üéôÔ∏è Voice Mode Active - Click mic to speak again")
                self.status_label.setStyleSheet("color: #FFA500; font-size: 12px; font-weight: normal;")
            else:
                self.status_label.setText("Ready")
                self.status_label.setStyleSheet("color: #DDD; font-size: 12px;")
            
            # If continuous listening, restart after TTS finishes
            if self.continuous_listening:
                self._restart_listening_after_delay(500)
        except Exception as e:
            logging.warning(f"Error in _on_tts_finished: {e}")
    
    def _on_tts_error(self, msg: str):
        """Handle TTS error signal - update UI on main thread"""
        try:
            logging.warning(f"TTS error: {msg}")
            self._hide_avatar()
            if self.voice_only_mode:
                self.status_label.setText("üéôÔ∏è Voice Mode Active - Click mic to speak again")
                self.status_label.setStyleSheet("color: #FFA500; font-size: 12px; font-weight: normal;")
            else:
                self.status_label.setText("Ready")
                self.status_label.setStyleSheet("color: #DDD; font-size: 12px;")
            
            # Restart listening even if TTS fails
            if self.continuous_listening:
                self._restart_listening_after_delay(500)
        except Exception as e:
            logging.warning(f"Error in _on_tts_error: {e}")
    
    def _start_tts(self, text_to_speak: str):
        """Start TTS in a worker thread - thread-safe"""
        if not text_to_speak or not text_to_speak.strip():
            return
        
        if not ENABLE_TTS:
            return
        
        # Clean up any existing TTS thread
        try:
            if hasattr(self, '_tts_thread') and self._tts_thread is not None:
                try:
                    if hasattr(self, '_tts_worker') and self._tts_worker is not None:
                        try:
                            self._tts_worker.speaking_started.disconnect()
                            self._tts_worker.speaking_finished.disconnect()
                            self._tts_worker.error.disconnect()
                            self._tts_worker.finished.disconnect()
                        except:
                            pass
                except:
                    pass
        except:
            pass
        
        # Create TTS worker thread
        self._tts_thread = QThread(self)
        self._tts_worker = TTSWorker(
            text=text_to_speak,
            edge_tts_voice=self.edge_tts_voice,
            gtts_voice_id=self.tts_voice_id,
            enable_gtts_fallback=self.enable_gtts_fallback
        )
        self._tts_worker.moveToThread(self._tts_thread)
        
        # Wire signals - connect to UI handlers
        self._tts_thread.started.connect(self._tts_worker.run)
        self._tts_worker.speaking_started.connect(self._on_tts_started)
        self._tts_worker.speaking_finished.connect(self._on_tts_finished)
        self._tts_worker.error.connect(self._on_tts_error)
        self._tts_worker.finished.connect(self._tts_thread.quit)
        self._tts_worker.error.connect(self._tts_thread.quit)
        
        # Cleanup when thread finishes
        def cleanup_tts():
            try:
                if hasattr(self, '_tts_worker') and self._tts_worker:
                    self._tts_worker.deleteLater()
                    self._tts_worker = None
                if hasattr(self, '_tts_thread') and self._tts_thread:
                    self._tts_thread.deleteLater()
                    self._tts_thread = None
            except:
                pass
        
        self._tts_thread.finished.connect(cleanup_tts)
        
        # Start the thread
        self._tts_thread.start()
    
    def on_mode_changed(self, mode):
        self.current_mode = mode
        # Use hybrid capability-based system for cost optimization
        if MODEL_REGISTRY_AVAILABLE:
            try:
                # Get capability for this mode (chat_fast for simple tasks, chat_deep for complex)
                capability = MODE_TO_CAPABILITY.get(mode, "chat_default")
                registry = get_model_registry()
                best_model = registry.get_model_for_capability(capability)
                logging.info(f"Mode '{mode}' ‚Üí Capability '{capability}' ‚Üí Model '{best_model}' (hybrid cost optimization)")
            except Exception as e:
                logging.warning(f"Error getting model from capability for {mode}: {e}, using fallback")
                best_model = DEFAULT_MODEL_PER_MODE.get(mode, "gpt-5-mini")
        else:
            # Fallback to direct mapping
            best_model = DEFAULT_MODEL_PER_MODE.get(mode, "gpt-5-mini")
        
        self.current_model = best_model
        self.model_combo.setCurrentText(best_model)
        self.append_message("system", f"Switched to: {mode} (Model: {best_model})")
        self._save_history()
    
    def on_model_changed(self, model):
        self.current_model = model
        self._save_history()
    
    # Messaging
    def append_message(self, kind: str, text: str):
        try:
            if not text:
                text = "(empty message)"
            
            # In voice-only mode, show indicator instead of full text
            if self.voice_only_mode and kind != "system":
                if kind == "user":
                    # Show brief indicator for user speech
                    indicator = "üé§ You spoke..."
                    safe = html.escape(indicator)
                    label, color = LEA_USER_NAME, self.USER_COLOR
                elif kind == "assistant":
                    # Show brief indicator for agent's response
                    indicator = f"üîä {LEA_AGENT_NAME} is responding..."
                    safe = html.escape(indicator)
                    label, color = LEA_AGENT_NAME, self.ASSIST_COLOR
                
                html_block = f'<div style="margin: 6px 0;"><span style="color:{color}; font-weight:600;">{label}:</span> <span style="color:{color}; font-style:italic;">{safe}</span></div>'
                
                if hasattr(self, 'chat_display') and self.chat_display:
                    self.chat_display.append(html_block)
                
                # Still save to history for later review
                # (History saving happens elsewhere in the code)
                return
            
            # Normal mode: show full text
            if kind == "user":
                label, color = LEA_USER_NAME, self.USER_COLOR
            elif kind == "assistant":
                label, color = LEA_AGENT_NAME, self.ASSIST_COLOR
            else:
                label, color = "System", self.SYSTEM_COLOR

            # Ensure text is always a string and safe for HTML
            safe = html.escape(str(text)).replace("\n", "<br>")
            html_block = f'<div style="margin: 6px 0;"><span style="color:{color}; font-weight:600;">{label}:</span> <span style="color:{color};">{safe}</span></div>'
            
            # Add system messages (especially file uploads) to message_history so Lea can see them
            if kind == "system":
                # Ensure message_history exists
                if not hasattr(self, 'message_history') or not isinstance(self.message_history, list):
                    self.message_history = []
                # Add system message to history so Lea can access file paths
                # Format as a user message so it's visible in conversation context
                self.message_history.append({"role": "user", "content": f"[System]: {text}"})
                # Limit history to last 20 messages
                if len(self.message_history) > 20:
                    self.message_history = self.message_history[-20:]
                # Save history
                self._save_history()
            
            # Ensure we're on the main thread (Qt requirement)
            if hasattr(self, 'chat_display') and self.chat_display:
                self.chat_display.append(html_block)
                # Scroll to show the new message (scroll to bottom)
                QTimer.singleShot(10, lambda: self._scroll_to_bottom())
        except Exception as e:
            logging.error(f"Error appending message: {traceback.format_exc()}")
            # Fallback to plain text if HTML fails
            try:
                if hasattr(self, 'chat_display') and self.chat_display:
                    self.chat_display.append(f"{label}: {str(text)}")
            except:
                pass
    
    def _scroll_to_bottom(self):
        """Helper method to scroll chat display to bottom to show latest messages"""
        try:
            if hasattr(self, 'chat_display') and self.chat_display:
                scrollbar = self.chat_display.verticalScrollBar()
                if scrollbar:
                    scrollbar.setValue(scrollbar.maximum())
        except Exception as e:
            logging.warning(f"Error scrolling to bottom: {e}")
    
    def on_send(self):
        try:
            # Prevent multiple simultaneous requests
            # Safely check if worker thread is running (handle deleted thread gracefully)
            try:
                if hasattr(self, 'worker_thread') and self.worker_thread is not None:
                    # Check if thread still exists before accessing it
                    thread_exists = True
                    try:
                        is_running = self.worker_thread.isRunning()
                    except RuntimeError:
                        # Thread has been deleted
                        thread_exists = False
                        self.worker_thread = None
                    
                    if thread_exists and is_running:
                        QMessageBox.information(self, "Please Wait", "A request is already in progress. Please wait for it to complete.")
                        return
            except Exception as thread_check_error:
                # Thread was deleted, just continue
                logging.warning(f"Thread check failed (likely deleted): {thread_check_error}")
                if hasattr(self, 'worker_thread'):
                    self.worker_thread = None
            
            text = self.input_box.toPlainText().strip()
            if not text:
                return
            
            if not openai_client:
                QMessageBox.warning(self, "API Key Missing", 
                                  "OpenAI API key not found. Please set OPENAI_API_KEY in your .env file.")
                return
            
            # Validate mode and model
            if self.current_mode not in AGENTS:
                QMessageBox.warning(self, "Invalid Mode", f"Selected mode '{self.current_mode}' is not valid.")
                return
            
            if self.current_model not in MODEL_OPTIONS:
                QMessageBox.warning(self, "Invalid Model", f"Selected model '{self.current_model}' is not valid.")
                return
            
            # Check if user is requesting finance/accounting or legal tasks - auto-switch to appropriate mode
            # Finance and legal checks happen FIRST to ensure specialized questions take priority even if work-related
            text_lower = text.lower()
            
            # Initialize matched keyword lists
            matched_finance_keywords = []
            matched_legal_keywords = []
            
            # Finance and accounting keywords that should trigger Finance & Tax mode
            finance_keywords = [
                # Accounting terms
                "accounting", "accountant", "account", "accounts", "accounting question", "accounting help",
                "bookkeeping", "bookkeeper", "books", "ledger", "journal", "journal entry", "journal entries",
                "balance sheet", "income statement", "cash flow", "financial statement", "financial statements",
                "accounts payable", "accounts receivable", "ap", "ar", "payables", "receivables",
                "general ledger", "chart of accounts", "trial balance", "reconciliation", "reconcile",
                "depreciation", "amortization", "accrual", "accruals", "deferred", "prepaid",
                "cost accounting", "managerial accounting", "financial accounting", "audit", "auditing",
                "gaap", "ifrs", "accounting standard", "accounting standards", "accounting principle",
                # Finance terms
                "finance", "financial", "financing", "finances", "finance question", "finance help",
                "budget", "budgeting", "budget analysis", "budget review", "budget planning",
                "financial planning", "financial analysis", "financial report", "financial reporting",
                "financial model", "financial modeling", "valuation", "valuation model",
                "cash flow", "cashflow", "cash management", "working capital", "capital structure",
                "investment", "investments", "portfolio", "portfolio management", "asset allocation",
                "roi", "return on investment", "npv", "net present value", "irr", "internal rate of return",
                "financial ratio", "financial ratios", "profit margin", "gross margin", "net margin",
                "ebitda", "ebit", "revenue", "revenues", "expense", "expenses", "cost", "costs",
                "profit", "profits", "loss", "losses", "income", "earnings", "p&l", "profit and loss",
                # Tax terms
                "tax", "taxes", "taxation", "tax question", "tax help", "tax return", "tax returns",
                "irs", "internal revenue service", "tax form", "tax forms", "tax filing", "tax season",
                "deduction", "deductions", "tax deduction", "tax credit", "tax credits",
                "w-2", "w2", "1099", "1040", "tax bracket", "tax rate", "tax liability",
                "capital gains", "capital loss", "taxable income", "adjusted gross income", "agi",
                # Business finance
                "business finance", "corporate finance", "business accounting", "corporate accounting",
                "financial controller", "cfo", "chief financial officer", "controller",
                "financial close", "month end", "month-end", "quarter end", "quarter-end", "year end", "year-end",
                "financial period", "fiscal year", "fiscal period", "accounting period",
                # Banking and transactions
                "bank reconciliation", "bank statement", "bank statements", "bank account", "bank accounts",
                "transaction", "transactions", "payment", "payments", "invoice", "invoices", "billing",
                "payroll", "payroll tax", "payroll taxes", "pay stub", "pay stubs", "wage", "wages",
                # Financial reporting
                "financial report", "financial reports", "monthly report", "quarterly report", "annual report",
                "financial dashboard", "financial metrics", "kpi", "key performance indicator",
                # Work-related finance/accounting (should still go to Finance & Tax)
                "work accounting", "work finance", "business accounting", "business finance",
                "work tax", "business tax", "corporate tax", "company tax", "company accounting",
                "work financial", "business financial", "corporate financial"
            ]
            
            # Check for finance/accounting keywords FIRST (takes priority over work keywords)
            matched_finance_keywords = [kw for kw in finance_keywords if kw in text_lower]
            if matched_finance_keywords:
                logging.info(f"Finance/accounting keywords detected: {matched_finance_keywords}")
                if self.current_mode != "Finance & Tax":
                    logging.info(f"Auto-switching from '{self.current_mode}' to 'Finance & Tax'")
                    self.current_mode = "Finance & Tax"
                    if hasattr(self, 'mode_combo'):
                        self.mode_combo.setCurrentText("Finance & Tax")
                    # Use hybrid capability-based system for cost optimization
                    if MODEL_REGISTRY_AVAILABLE:
                        try:
                            capability = MODE_TO_CAPABILITY.get("Finance & Tax", "chat_default")
                            registry = get_model_registry()
                            best_model = registry.get_model_for_capability(capability)
                            logging.info(f"Auto-switched to Finance & Tax ‚Üí Capability '{capability}' ‚Üí Model '{best_model}'")
                        except Exception as e:
                            logging.warning(f"Error getting model from capability: {e}, using fallback")
                            best_model = DEFAULT_MODEL_PER_MODE.get("Finance & Tax", "gpt-5-mini")
                    else:
                        best_model = DEFAULT_MODEL_PER_MODE.get("Finance & Tax", "gpt-5-mini")
                    self.current_model = best_model
                    if hasattr(self, 'model_combo'):
                        self.model_combo.setCurrentText(best_model)
                    self.append_message("system", f"Switched to: Finance & Tax mode (handles all accounting and finance questions)")
                    # Save the mode change immediately
                    self._save_history()
            
            # Check if user is requesting legal-related tasks - auto-switch to Legal Research & Drafting mode
            # (Only if finance keywords were not detected)
            if not matched_finance_keywords:
                # Legal keywords that should trigger Legal Research & Drafting mode
                legal_keywords = [
                    # Legal terms
                    "legal", "law", "laws", "lawyer", "attorney", "attorneys", "counsel", "counselor",
                    "legal question", "legal help", "legal advice", "legal matter", "legal matters",
                    "legal issue", "legal issues", "legal problem", "legal problems", "legal case", "legal cases",
                    "legal research", "legal document", "legal documents", "legal drafting", "legal writing",
                    "legal opinion", "legal analysis", "legal review", "legal consultation",
                    # Court and litigation
                    "court", "courts", "lawsuit", "lawsuits", "litigation", "litigate", "sue", "suing", "sued",
                    "plaintiff", "defendant", "defendants", "complaint", "complaints", "petition", "petitions",
                    "motion", "motions", "brief", "briefs", "filing", "filings", "hearing", "hearings",
                    "trial", "trials", "judge", "judges", "judgment", "judgments", "verdict", "verdicts",
                    "appeal", "appeals", "appellate", "settlement", "settlements", "mediation", "arbitration",
                    # Legal documents and contracts
                    "contract", "contracts", "agreement", "agreements", "lease", "leases", "lease agreement",
                    "terms and conditions", "terms of service", "privacy policy", "nda", "non-disclosure",
                    "will", "wills", "estate", "estates", "trust", "trusts", "power of attorney",
                    "deed", "deeds", "title", "titles", "lien", "liens", "mortgage", "mortgages",
                    # Legal entities and business law
                    "corporation", "llc", "partnership", "partnerships", "sole proprietorship",
                    "business entity", "business entities", "incorporate", "incorporation",
                    "articles of incorporation", "bylaws", "operating agreement", "shareholder", "shareholders",
                    # Criminal law
                    "criminal", "crime", "crimes", "felony", "felonies", "misdemeanor", "misdemeanors",
                    "arrest", "arrested", "charge", "charges", "charged", "prosecution", "prosecutor",
                    # Family law
                    "divorce", "custody", "child custody", "alimony", "spousal support", "child support",
                    "adoption", "prenup", "prenuptial", "prenuptial agreement", "marriage", "marital",
                    # Employment law
                    "employment law", "labor law", "employment contract", "employment agreement",
                    "discrimination", "harassment", "wrongful termination", "unemployment", "workers comp",
                    "workers compensation", "wage", "wages", "overtime", "fmla", "family medical leave",
                    # Real estate law
                    "real estate law", "property law", "landlord", "tenant", "tenants", "eviction", "evictions",
                    "zoning", "easement", "easements", "property rights", "real property",
                    # Intellectual property
                    "patent", "patents", "trademark", "trademarks", "copyright", "copyrights", "ip", "intellectual property",
                    # Arizona-specific legal terms
                    "arizona law", "arizona legal", "arizona statute", "arizona statutes", "arizona code",
                    "arizona rules", "arizona court", "arizona courts", "arizona case law",
                    # Paralegal and legal research
                    "paralegal", "legal assistant", "legal research", "case law", "statute", "statutes",
                    "regulation", "regulations", "code section", "code sections", "legal precedent", "precedents",
                    "legal citation", "legal citations", "cite", "cites", "citation", "citations",
                    # Work-related legal (should still go to Legal Research & Drafting)
                    "work legal", "business legal", "corporate legal", "company legal", "work law", "business law"
                ]
                
                # Check for legal keywords (takes priority over work keywords)
                matched_legal_keywords = [kw for kw in legal_keywords if kw in text_lower]
                if matched_legal_keywords:
                    logging.info(f"Legal keywords detected: {matched_legal_keywords}")
                    if self.current_mode != "Legal Research & Drafting":
                        logging.info(f"Auto-switching from '{self.current_mode}' to 'Legal Research & Drafting'")
                        self.current_mode = "Legal Research & Drafting"
                        if hasattr(self, 'mode_combo'):
                            self.mode_combo.setCurrentText("Legal Research & Drafting")
                        # Use hybrid capability-based system for cost optimization
                        if MODEL_REGISTRY_AVAILABLE:
                            try:
                                capability = MODE_TO_CAPABILITY.get("Legal Research & Drafting", "chat_default")
                                registry = get_model_registry()
                                best_model = registry.get_model_for_capability(capability)
                                logging.info(f"Auto-switched to Legal Research & Drafting ‚Üí Capability '{capability}' ‚Üí Model '{best_model}'")
                            except Exception as e:
                                logging.warning(f"Error getting model from capability: {e}, using fallback")
                                best_model = DEFAULT_MODEL_PER_MODE.get("Legal Research & Drafting", "gpt-5-mini")
                        else:
                            best_model = DEFAULT_MODEL_PER_MODE.get("Legal Research & Drafting", "gpt-5-mini")
                        self.current_model = best_model
                        if hasattr(self, 'model_combo'):
                            self.model_combo.setCurrentText(best_model)
                        self.append_message("system", f"Switched to: Legal Research & Drafting mode (handles all legal questions)")
                        # Save the mode change immediately
                        self._save_history()
            
            # Check if user is requesting work-related tasks - auto-switch to Executive Assistant mode
            # (Only if finance and legal keywords were not detected)
            if not matched_finance_keywords and not matched_legal_keywords:
                # Work-related keywords that should trigger Executive Assistant mode
                # Expanded to catch more variations, especially email/calendar requests
                work_keywords = [
                    # Email/work communication - expanded variations
                    "email", "emails", "inbox", "outlook", "mail", "draft", "compose", "check email", "check emails",
                    "email report", "inbox report", "outlook report", "email check", "check my email", "check my emails",
                    "check work email", "work email", "business email", "professional email",
                    "draft email", "send email", "reply to email", "write email", "create email",
                    # Calendar/scheduling
                    "calendar", "schedule", "appointment", "meeting", "event", "events",
                    "check calendar", "check my calendar", "show calendar", "calendar report",
                    # PowerPoint/presentations
                    "powerpoint", "ppt", "pptx", "presentation", "presentations", "slide", "slides",
                    "format powerpoint", "edit presentation", "format ppt", "format slides", "edit slides",
                    "powerpoint file", "presentation file", "format text in", "change font in",
                    # Work tasks and projects
                    "work task", "work project", "work assignment", "work deadline",
                    "business task", "professional task", "work report", "work presentation",
                    # Scheduling and meetings
                    "schedule meeting", "work meeting", "business meeting", "client meeting",
                    "work calendar", "schedule work", "work appointment",
                    # Professional communication
                    "work communication", "client communication", "work correspondence",
                    # Executive Assistant references
                    "ea tasks", "ea task", "executive assistant", "exec assistant", "exec assistant tasks",
                    "help with my ea", "ea help", "executive assistant help",
                    # General work context
                    "for work", "at work", "work related", "work stuff", "work thing",
                    "work document", "work file", "work project", "work assignment",
                    "work deadline", "work presentation", "work report"
                ]
                
                # Check for work-related keywords - use any() for substring matching
                matched_keywords = [kw for kw in work_keywords if kw in text_lower]
                if matched_keywords:
                    logging.info(f"Work-related keywords detected: {matched_keywords}")
                    if self.current_mode != "Executive Assistant & Operations":
                        logging.info(f"Auto-switching from '{self.current_mode}' to 'Executive Assistant & Operations'")
                        self.current_mode = "Executive Assistant & Operations"
                        if hasattr(self, 'mode_combo'):
                            self.mode_combo.setCurrentText("Executive Assistant & Operations")
                        # Use hybrid capability-based system for cost optimization
                        if MODEL_REGISTRY_AVAILABLE:
                            try:
                                capability = MODE_TO_CAPABILITY.get("Executive Assistant & Operations", "chat_fast")
                                registry = get_model_registry()
                                best_model = registry.get_model_for_capability(capability)
                                logging.info(f"Auto-switched to Executive Assistant & Operations ‚Üí Capability '{capability}' ‚Üí Model '{best_model}'")
                            except Exception as e:
                                logging.warning(f"Error getting model from capability: {e}, using fallback")
                                best_model = DEFAULT_MODEL_PER_MODE.get("Executive Assistant & Operations", "gpt-5-mini")
                        else:
                            best_model = DEFAULT_MODEL_PER_MODE.get("Executive Assistant & Operations", "gpt-5-mini")
                        self.current_model = best_model
                        if hasattr(self, 'model_combo'):
                            self.model_combo.setCurrentText(best_model)
                        self.append_message("system", f"Switched to: Executive Assistant & Operations mode (handles all work tasks)")
                        # Save the mode change immediately
                        self._save_history()
            
            self.append_message("user", text)
            self.input_box.clear()
            self.status_label.setText("Thinking...")
            
            # Use file content only if checkbox is checked
            file_content = None
            if hasattr(self, 'include_file_cb') and self.include_file_cb.isChecked():
                file_content = self.current_file_content
            
            # Ensure message_history is a list
            if not isinstance(self.message_history, list):
                self.message_history = []
            
            # Clean up any existing thread references first - CRITICAL to prevent duplicate responses
            try:
                # Stop any running worker thread
                if hasattr(self, 'worker_thread') and self.worker_thread is not None:
                    try:
                        # Disconnect ALL signals first
                        if hasattr(self, 'worker') and self.worker is not None:
                            try:
                                self.worker.finished.disconnect()
                                self.worker.error.disconnect()
                                self.worker.stream_chunk.disconnect()
                                self.worker.memory_context.disconnect()
                            except (TypeError, RuntimeError):
                                # Signals already disconnected or object deleted
                                pass
                        
                        # Disconnect thread signals
                        try:
                            self.worker_thread.started.disconnect()
                            self.worker_thread.finished.disconnect()
                        except (TypeError, RuntimeError):
                            pass
                        
                        # Stop and wait for thread to finish
                        if self.worker_thread.isRunning():
                            self.worker_thread.quit()
                            self.worker_thread.wait(1000)  # Wait up to 1 second
                        
                        # Clean up old worker
                        if hasattr(self, '_current_worker') and self._current_worker:
                            try:
                                self._current_worker.deleteLater()
                            except:
                                pass
                            self._current_worker = None
                        
                        # Clean up old thread
                        if hasattr(self, '_current_thread') and self._current_thread:
                            try:
                                self._current_thread.deleteLater()
                            except:
                                pass
                            self._current_thread = None
                        
                        self.worker = None
                        self.worker_thread = None
                    except (RuntimeError, AttributeError):
                        # Thread/worker already deleted
                        self.worker = None
                        self.worker_thread = None
                        self._current_worker = None
                        self._current_thread = None
            except Exception as cleanup_error:
                logging.warning(f"Error during cleanup: {cleanup_error}")
                # Reset everything to be safe
                self.worker = None
                self.worker_thread = None
                self._current_worker = None
                self._current_thread = None
            
            # Reset streaming state BEFORE creating new worker
            self.current_streaming_response = ""
            self.is_streaming = False  # Set to False initially, will be set to True after worker starts
            self.streaming_message_started = False
            self._streaming_msg_created = False  # CRITICAL: Reset flag to prevent duplicate messages
            self._streaming_block_start = None  # Reset block reference
            self._streaming_item_index = -1  # Reset block index
            if hasattr(self, '_last_stream_update'):
                self._last_stream_update = 0  # Reset update timer
            self.streaming_cursor_position = None
            self.streaming_message_count = 0  # Reset count
            
            # Start worker thread
            self.worker_thread = QThread()
            self.worker = LeaWorker(
                openai_client,
                MODEL_OPTIONS,
                AGENTS,
                self.current_mode,
                self.current_model,
                self.message_history,
                file_content,
                text,
                self.memory_system  # Pass memory system
            )
            self.worker.moveToThread(self.worker_thread)
            
            # Store references to prevent garbage collection
            self._current_worker = self.worker
            self._current_thread = self.worker_thread
            
            # Connect signals - connect each signal only once
            self.worker_thread.started.connect(self.worker.run)
            self.worker.stream_chunk.connect(self.on_stream_chunk)  # Handle streaming
            self.worker.memory_context.connect(self.on_memory_context)  # Handle memory context
            self.worker.finished.connect(self.on_worker_finished)
            self.worker.error.connect(self.on_worker_error)
            # Connect thread quit to worker signals - but use lambda to avoid duplicate connections
            def quit_thread_on_finished():
                if hasattr(self, 'worker_thread') and self.worker_thread:
                    self.worker_thread.quit()
            
            def quit_thread_on_error():
                if hasattr(self, 'worker_thread') and self.worker_thread:
                    self.worker_thread.quit()
            
            self.worker.finished.connect(quit_thread_on_finished)
            self.worker.error.connect(quit_thread_on_error)
            
            # Set streaming flag AFTER everything is connected
            self.is_streaming = True
            
            # Use a lambda to safely handle deletion
            def safe_delete_worker():
                try:
                    if hasattr(self, '_current_worker') and self._current_worker:
                        self._current_worker.deleteLater()
                        self._current_worker = None
                except:
                    pass
            
            def safe_delete_thread():
                try:
                    if hasattr(self, '_current_thread') and self._current_thread:
                        self._current_thread.deleteLater()
                        self._current_thread = None
                        if hasattr(self, 'worker_thread'):
                            self.worker_thread = None
                except:
                    pass
            
            self.worker_thread.finished.connect(safe_delete_worker)
            self.worker_thread.finished.connect(safe_delete_thread)
            
            # Add timeout mechanism to detect if worker hangs
            def check_worker_timeout():
                """Check if worker is taking too long and show error if needed"""
                try:
                    if hasattr(self, 'worker_thread') and self.worker_thread and self.worker_thread.isRunning():
                        # Worker is still running after timeout - might be hung
                        logging.warning("Worker thread appears to be taking longer than expected")
                        # Don't kill it, but log for debugging
                except:
                    pass
            
            # Set a timeout check after 120 seconds (2 minutes)
            QTimer.singleShot(120000, check_worker_timeout)
            
            self.worker_thread.start()
            logging.info(f"Worker thread started for mode: {self.current_mode}, model: {self.current_model}")
            
        except Exception as e:
            error_msg = f"Error sending message: {str(e)}"
            logging.error(f"Error in on_send: {traceback.format_exc()}")
            QMessageBox.critical(self, "Error", error_msg)
            self.status_label.setText("Error")

    def on_stream_chunk(self, chunk: str):
        """Handle streaming response chunks - throttled updates to prevent spam"""
        # CRITICAL: Guard against stale chunks
        if not self.is_streaming:
            return
        if not chunk or not chunk.strip():
            return
        
        # Accumulate the full response
        self.current_streaming_response += chunk
        
        # In voice-only mode, don't show streaming text
        if self.voice_only_mode:
            return
        
        # Throttle updates - only update UI every 50ms or if chunk is significant
        if not hasattr(self, '_last_stream_update'):
            self._last_stream_update = 0
        
        import time
        now = time.time() * 1000  # milliseconds
        time_since_update = now - self._last_stream_update
        
        # Update if: significant time passed (50ms) OR chunk is large (>20 chars)
        if time_since_update < 50 and len(chunk) < 20:
            return  # Skip this update, will catch up on next chunk
        
        self._last_stream_update = now
        
        try:
            # Simple approach: always rebuild the last message
            safe_response = html.escape(self.current_streaming_response).replace("\n", "<br>")
            new_message_html = f'<div id="streaming-lea-msg" style="margin: 6px 0;"><span style="color:{self.ASSIST_COLOR}; font-weight:600;">Lea:</span> <span style="color:{self.ASSIST_COLOR};">{safe_response}</span></div>'
            
            if self._streaming_msg_created:
                # Get current HTML and find/replace the streaming message
                current_html = self.chat_display.toHtml()
                
                # Simple approach: find the ID marker and replace everything from its div start to div end
                id_marker = 'id="streaming-lea-msg"'
                id_pos = current_html.find(id_marker)
                
                if id_pos >= 0:
                    # Find div start (look backwards)
                    div_start = current_html.rfind('<div', 0, id_pos)
                    if div_start >= 0:
                        # Find the matching closing div (simple: find next </div> after id marker)
                        # This works because our div doesn't contain nested divs
                        div_end = current_html.find('</div>', id_pos)
                        if div_end >= 0:
                            before = current_html[:div_start]
                            after = current_html[div_end + 6:]  # +6 for "</div>"
                            new_html = before + new_message_html + after
                            
                            self.chat_display.blockSignals(True)
                            try:
                                self.chat_display.setHtml(new_html)
                                QTimer.singleShot(10, lambda: self._scroll_to_bottom())
                            finally:
                                self.chat_display.blockSignals(False)
                            return
                
                # Fallback: find last "Lea:" marker
                marker = f'<span style="color:{self.ASSIST_COLOR}; font-weight:600;">Lea:</span>'
                last_pos = current_html.rfind(marker)
                if last_pos >= 0:
                    div_start = current_html.rfind('<div', 0, last_pos)
                    if div_start >= 0:
                        div_end = current_html.find('</div>', last_pos)
                        if div_end >= 0:
                            before = current_html[:div_start]
                            after = current_html[div_end + 6:]
                            new_html = before + new_message_html + after
                            self.chat_display.blockSignals(True)
                            try:
                                self.chat_display.setHtml(new_html)
                                QTimer.singleShot(10, lambda: self._scroll_to_bottom())
                            finally:
                                self.chat_display.blockSignals(False)
                            return
            
            # First chunk - create new message
            if not self._streaming_msg_created:
                self._streaming_msg_created = True
                self.streaming_message_started = True
                self._last_stream_update = now
                self.chat_display.append(new_message_html)
                QTimer.singleShot(10, lambda: self._scroll_to_bottom())
            
        except Exception as e:
            logging.error(f"Error in stream chunk: {traceback.format_exc()}")
            # Don't create duplicate messages on error
            pass
    
    def on_memory_context(self, context_msg: str):
        """Handle memory context information"""
        try:
            # Optionally show memory context in status or log it
            logging.info(f"Memory context: {context_msg}")
            # You could also show this in the UI if desired
        except Exception as e:
            logging.warning(f"Error handling memory context: {e}")
    
    def on_worker_finished(self, answer, status):
        try:
            logging.info(f"on_worker_finished called - answer length: {len(answer) if answer else 0}, status: {status}")
            
            # Store streaming state BEFORE resetting it
            was_streaming = self.is_streaming
            streaming_response = self.current_streaming_response.strip() if self.current_streaming_response else ""
            
            # Reset streaming state
            self.is_streaming = False
            self.streaming_message_started = False
            
            # If we were streaming, ensure final message is displayed correctly with full text
            # Use the final answer from API if available (it's complete), otherwise use streaming response
            if was_streaming:
                # Prefer final answer from API over accumulated streaming response
                final_display_text = answer.strip() if answer and answer.strip() else streaming_response
                # If still empty, show error message
                if not final_display_text:
                    final_display_text = "I apologize, but I didn't receive a complete response. Please try again."
                if final_display_text:
                    # Check if the message is already displayed correctly
                    safe_text = html.escape(final_display_text).replace("\n", "<br>")
                    html_content = self.chat_display.toHtml()
                    lea_pattern = f'<span style="color:{self.ASSIST_COLOR}; font-weight:600;">Lea:</span>'
                    
                    # Find the LAST occurrence of Lea: marker (should be our streaming message)
                    last_marker_pos = html_content.rfind(lea_pattern)
                    
                    # Only update if we find the marker and we created a streaming message
                    if last_marker_pos >= 0 and self._streaming_msg_created:
                        after_lea_content = html_content[last_marker_pos + len(lea_pattern):]
                        # Extract the text between marker and </div>
                        div_end_pos = after_lea_content.find('</div>')
                        if div_end_pos >= 0:
                            before = html_content[:last_marker_pos + len(lea_pattern)]
                            after = after_lea_content[div_end_pos:]
                            new_content = f' <span style="color:{self.ASSIST_COLOR};">{safe_text}</span>'
                            new_html = before + new_content + after
                            try:
                                self.chat_display.blockSignals(True)
                                self.chat_display.setHtml(new_html)
                                self.chat_display.blockSignals(False)
                                # Scroll to show updated message
                                QTimer.singleShot(10, lambda: self._scroll_to_bottom())
                            except Exception as e:
                                logging.warning(f"Error updating final streaming message: {e}")
                    elif not self._streaming_msg_created:
                        # No streaming message was created - append it (shouldn't happen often)
                        try:
                            self.chat_display.append(f"<div style='margin: 6px 0;'><span style='color:{self.ASSIST_COLOR}; font-weight:600;'>Lea:</span> <span style='color:{self.ASSIST_COLOR};'>{safe_text}</span></div>")
                            self._streaming_msg_created = True  # Mark as created to prevent duplicates
                            # Scroll to show new message
                            QTimer.singleShot(10, lambda: self._scroll_to_bottom())
                        except Exception as e:
                            logging.warning(f"Error appending final streaming message: {e}")
                
                # Save streaming response to history - prefer final answer if available
                final_answer_for_history = answer.strip() if answer and answer.strip() else streaming_response
                if final_answer_for_history:
                    # Check if we need to add or update the assistant message in history
                    if self.message_history:
                        last_msg = self.message_history[-1]
                        if last_msg.get('role') == 'assistant':
                            # Update existing assistant message with complete response
                            self.message_history[-1]['content'] = final_answer_for_history
                        else:
                            # Add new assistant message
                            self.message_history.append({"role": "assistant", "content": final_answer_for_history})
                    else:
                        # No history yet, add the message
                        self.message_history.append({"role": "assistant", "content": final_answer_for_history})
            elif answer:
                # Non-streaming mode - display and save the message
                answer_str = str(answer).strip()
                if answer_str:
                    self.append_message("assistant", answer_str)
                    # Ensure it's in history
                    if not (self.message_history and self.message_history[-1].get('role') == 'assistant'):
                        self.message_history.append({"role": "assistant", "content": answer_str})
                    elif self.message_history[-1].get('role') == 'assistant':
                        # Update existing assistant message
                        self.message_history[-1]['content'] = answer_str
            else:
                # No answer received - show error message to user
                error_msg = "I apologize, but I didn't receive a response. This might be due to an API issue. Please try again."
                self.append_message("assistant", error_msg)
                logging.warning("No answer received in on_worker_finished - showing error message to user")
            
            # Limit history to last 20 messages
            if len(self.message_history) > 20:
                self.message_history = self.message_history[-20:]
            
            # Reset streaming state for next time (but only after we've finished processing)
            # Don't reset _streaming_msg_created here - it's already been used to determine if we should update
            # Reset it at the start of the next request instead
            self.current_streaming_response = ""
            
            # Reset status label style and show appropriate status
            if self.voice_only_mode:
                self.status_label.setText("üéôÔ∏è Voice Mode Active - Click mic to speak again")
                self.status_label.setStyleSheet("color: #FFA500; font-size: 12px; font-weight: normal;")
            else:
                self.status_label.setText(str(status) if status else "Ready")
                self.status_label.setStyleSheet("color: #DDD; font-size: 12px;")
            
            # Always save history after receiving a response
            self._save_history()
            
            # Speak response if TTS is enabled (using thread-safe TTS system)
            if self.tts_enabled and answer:
                try:
                    text_to_speak = self.current_streaming_response.strip() if self.current_streaming_response else str(answer)
                    if text_to_speak:
                        # Start TTS using thread-safe method
                        self._start_tts(text_to_speak)
                except Exception as tts_exception:
                    # TTS errors are non-critical - never crash Lea
                    logging.warning(f"TTS error (non-critical, Lea continues): {tts_exception}")
                    try:
                        self._hide_avatar()
                    except:
                        pass
                    # Restart listening even if TTS fails
                    if self.continuous_listening:
                        self._restart_listening_after_delay(1000)
            elif self.continuous_listening:
                # No TTS, but continuous listening enabled - restart immediately
                self._restart_listening_after_delay(500)
            
            # Clean up references after successful completion
            try:
                if hasattr(self, '_current_worker'):
                    self._current_worker = None
            except:
                pass
        except Exception as e:
            logging.error(f"Error in on_worker_finished: {traceback.format_exc()}")
            try:
                self.status_label.setText("Error displaying response")
            except:
                pass

    def on_worker_error(self, error_msg):
        try:
            error_text = str(error_msg) if error_msg else "Unknown error"
            self.append_message("system", f"‚ùå Error: {error_text}")
            self.status_label.setText("Error")
            # Show user-friendly error dialog
            QMessageBox.warning(self, "Error", 
                              f"An error occurred:\n\n{error_text}\n\nCheck lea_crash.log for details.")
            
            # Clean up references after error
            try:
                if hasattr(self, '_current_worker'):
                    self._current_worker = None
            except:
                pass
        except Exception as e:
            logging.error(f"Error in on_worker_error: {traceback.format_exc()}")
            try:
                self.status_label.setText("Error handling failed")
            except:
                pass
    
    def _save_history(self):
        try:
            # Ensure message_history is a list
            if not isinstance(self.message_history, list):
                self.message_history = []
            
            # Use absolute path in project directory
            history_path = PROJECT_DIR / self.history_file
            # Limit history to last 20 messages
            history = self.message_history[-20:] if len(self.message_history) > 20 else self.message_history.copy()
            
            data = {
                'mode': str(self.current_mode) if self.current_mode else '',
                'model': str(self.current_model) if self.current_model else '',
                'history': history
            }
            
            # Try to save, with better error handling
            try:
                with open(history_path, 'w', encoding='utf-8') as f:
                    json.dump(data, f, indent=2, ensure_ascii=False)
            except PermissionError:
                # If permission denied, try alternative location
                try:
                    alt_path = Path.home() / "lea_history.json"
                    with open(alt_path, 'w', encoding='utf-8') as f:
                        json.dump(data, f, indent=2, ensure_ascii=False)
                    logging.info(f"Saved history to {alt_path} due to permission issue")
                except Exception as e2:
                    logging.warning(f"Could not save history to alternative location: {e2}")
            except OSError as os_err:
                logging.warning(f"File system error saving history: {os_err}")
        except Exception as e:
            logging.error(f"Error saving history: {traceback.format_exc()}")
    
    def _load_history(self):
        try:
            # Try project directory first
            history_path = PROJECT_DIR / self.history_file
            # Fallback to home directory if needed
            if not history_path.exists():
                alt_path = Path.home() / "lea_history.json"
                if alt_path.exists():
                    history_path = alt_path
            
            if not history_path.exists():
                msg = "Welcome to Lea Multi-Agent System!\n\n"
                msg += f"üíæ Backups: {BACKUPS_DIR}\n"
                msg += f"üì• Downloads: {DOWNLOADS_DIR}\n\n"
                msg += "üìé Upload files when you need to reference them\n"
                msg += "üì• Download Lea's responses to save them\n"
                msg += "üîç Lea can search the web for current information\n\n"
                # Check if web search is configured
                if os.getenv("SERPAPI_API_KEY"):
                    msg += "‚úÖ Web search enabled"
                else:
                    msg += "‚ö†Ô∏è Web search not configured (add SERPAPI_API_KEY to .env)"
                self.append_message("system", msg)
                return
            
            try:
                with open(history_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # Validate loaded data
                if not isinstance(data, dict):
                    logging.warning(f"Invalid history file format: {history_path}")
                    return
                
                # Load history with validation first
                loaded_history = data.get('history', [])
                if isinstance(loaded_history, list):
                    self.message_history = loaded_history
                    # Limit history to last 20 messages
                    if len(self.message_history) > 20:
                        self.message_history = self.message_history[-20:]
                else:
                    logging.warning("Invalid history format in file")
                    self.message_history = []
                
                # Always start in General Assistant & Triage mode on startup
                # Lea can transfer to the appropriate agent as needed
                # This ensures consistent startup behavior regardless of last used mode
                self.current_mode = "General Assistant & Triage"
                
                # Load model from history if valid, otherwise use default
                loaded_model = data.get('model', "gpt-5-mini")
                if loaded_model in MODEL_OPTIONS:
                    self.current_model = loaded_model
                else:
                    logging.warning(f"Invalid model in history: {loaded_model}")
                    self.current_model = "gpt-5-mini"
                
                # Update UI safely
                try:
                    if hasattr(self, 'mode_combo'):
                        self.mode_combo.setCurrentText(self.current_mode)
                    if hasattr(self, 'model_combo'):
                        self.model_combo.setCurrentText(self.current_model)
                    self.append_message("system", f"Loaded previous conversation ({len(self.message_history)} messages)")
                    
                    # Display ALL messages from history (not just last 5)
                    for msg in self.message_history:
                        if not isinstance(msg, dict):
                            continue
                        role = msg.get('role')
                        content = msg.get('content', '')
                        if not content:
                            continue
                        try:
                            # Clean up user messages that have file content prefixes
                            if role == 'user' and 'Dre\'s question:' in str(content):
                                # Extract just the user's question part
                                content = str(content).split('Dre\'s question:')[-1].strip()
                                # Also remove file content if present
                                if '=== UPLOADED FILE' in content:
                                    parts = content.split('=== END FILE ===')
                                    if len(parts) > 1:
                                        content = parts[-1].strip()
                                        if content.startswith('Dre\'s question:'):
                                            content = content.replace('Dre\'s question:', '').strip()
                            
                            # Display the message
                            if role == 'user':
                                self.append_message('user', content)
                            elif role == 'assistant':
                                self.append_message('assistant', content)
                            # Skip system messages in history (they're internal)
                        except Exception as msg_error:
                            logging.warning(f"Error displaying message: {msg_error}")
                            continue
                except Exception as ui_error:
                    logging.error(f"Error updating UI: {ui_error}")
                    
            except json.JSONDecodeError as json_err:
                logging.error(f"Invalid JSON in history file: {json_err}")
                # Show welcome message instead
                self.append_message("system", "Welcome! (Previous conversation could not be loaded)")
            except PermissionError:
                logging.warning(f"Permission denied reading history: {history_path}")
            except OSError as os_err:
                logging.warning(f"File system error reading history: {os_err}")
                
        except Exception as e:
            logging.error(f"Error loading history: {traceback.format_exc()}")
            # Continue with defaults
            self.append_message("system", "Welcome! (Error loading previous conversation)")
    
    def load_settings(self):
        """Load settings from file"""
        try:
            if self.settings_file.exists():
                with open(self.settings_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.tts_enabled = data.get('tts_enabled', False)
                    self.tts_voice_id = data.get('tts_voice_id', ("en", "com"))
                    self.edge_tts_voice = data.get('edge_tts_voice', "en-US-AriaNeural")
                    self.microphone_device_index = data.get('microphone_device_index', None)
                    self.voice_only_mode = data.get('voice_only_mode', False)
                    self.continuous_listening = data.get('continuous_listening', False)
                    self.push_to_talk_key = data.get('push_to_talk_key', None)
                    self.enable_gtts_fallback = data.get('enable_gtts_fallback', False)  # Default to False
                    
                    # Update global TTS flags from settings
                    global ENABLE_TTS, PREFER_EDGE_TTS, ENABLE_GTTTS_FALLBACK
                    ENABLE_TTS = data.get('enable_tts_global', True)  # Default to True
                    PREFER_EDGE_TTS = data.get('prefer_edge_tts', True)
                    ENABLE_GTTTS_FALLBACK = data.get('enable_gtts_fallback', False)
        except Exception as e:
            logging.warning(f"Error loading settings: {e}")
            # Use defaults
    
    def save_settings(self):
        """Save settings to file"""
        try:
            global ENABLE_TTS, PREFER_EDGE_TTS, ENABLE_GTTTS_FALLBACK
            data = {
                'tts_enabled': self.tts_enabled,
                'tts_voice_id': self.tts_voice_id,
                'edge_tts_voice': self.edge_tts_voice,
                'microphone_device_index': self.microphone_device_index,
                'voice_only_mode': self.voice_only_mode,
                'continuous_listening': self.continuous_listening,
                'push_to_talk_key': self.push_to_talk_key,
                'enable_gtts_fallback': self.enable_gtts_fallback,
                'enable_tts_global': ENABLE_TTS,
                'prefer_edge_tts': PREFER_EDGE_TTS,
                'enable_gtts_fallback_global': ENABLE_GTTTS_FALLBACK
            }
            with open(self.settings_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2)
        except Exception as e:
            logging.warning(f"Error saving settings: {e}")
    
    def show_settings(self):
        """Show settings dialog for TTS voice and microphone selection"""
        dialog = QDialog(self)
        dialog.setWindowTitle("‚öôÔ∏è Audio Settings")
        dialog.setMinimumSize(500, 400)
        dialog.setStyleSheet("""
            QDialog {
                background-color: #333;
            }
        """)
        
        layout = QVBoxLayout(dialog)
        layout.setContentsMargins(20, 20, 20, 20)
        layout.setSpacing(15)
        
        # Title
        title = QLabel("Audio Settings")
        title.setStyleSheet("font-size: 18px; font-weight: 600; color: #FFF; margin-bottom: 10px;")
        layout.addWidget(title)
        
        # Text-to-Speech Section
        tts_group = QGroupBox("Text-to-Speech (Lea's Voice)")
        tts_group.setStyleSheet("""
            QGroupBox {
                color: #FFF;
                border: 2px solid #555;
                border-radius: 6px;
                margin-top: 10px;
                padding-top: 10px;
            }
            QGroupBox::title {
                subcontrol-origin: margin;
                left: 10px;
                padding: 0 5px;
            }
        """)
        tts_layout = QVBoxLayout(tts_group)
        
        # Check TTS availability directly FIRST (before using variables)
        # Also check global variables as fallback
        edge_tts_available = False
        try:
            import edge_tts
            edge_tts_available = True
        except ImportError as e:
            logging.warning(f"edge-tts import failed in settings: {e}")
            # Fallback to global variable
            try:
                edge_tts_available = EDGE_TTS_AVAILABLE
            except:
                pass
        
        gtts_available = False
        try:
            from gtts import gTTS
            gtts_available = True
        except ImportError as e:
            logging.warning(f"gtts import failed in settings: {e}")
            # Fallback to global variable
            try:
                gtts_available = GTTS_AVAILABLE
            except:
                pass
        
        # TTS Enable checkbox
        tts_enable_cb = QCheckBox("Enable text-to-speech (Lea will speak her responses)")
        tts_enable_cb.setChecked(self.tts_enabled)
        tts_enable_cb.setStyleSheet("color: #FFF; font-size: 14px;")
        # Always enable the checkbox - let user decide (packages are installed)
        tts_enable_cb.setEnabled(True)
        tts_layout.addWidget(tts_enable_cb)
        
        # Show which TTS engine is available - check directly
        if edge_tts_available:
            tts_engine_info = QLabel("[OK] Using edge-tts (offline, high quality) - Recommended!")
            tts_engine_info.setStyleSheet("color: #00FF00; font-size: 11px; margin-top: 5px; font-weight: bold;")
            tts_layout.addWidget(tts_engine_info)
        elif gtts_available:
            tts_engine_info = QLabel("[INFO] Using gTTS (requires internet) - Install edge-tts for better quality")
            tts_engine_info.setStyleSheet("color: #FFA500; font-size: 11px; margin-top: 5px;")
            tts_layout.addWidget(tts_engine_info)
        else:
            tts_warning = QLabel("[INFO] TTS packages not detected. You can still enable TTS - it will work if packages are installed.")
            tts_warning.setStyleSheet("color: #FFA500; font-size: 11px; margin-top: 5px;")
            tts_layout.addWidget(tts_warning)
            # Don't disable checkbox - let user try anyway
        
        pygame_available = False
        try:
            import pygame
            pygame_available = True
        except ImportError:
            pass
        
        if not pygame_available and (edge_tts_available or gtts_available):
            tts_info = QLabel("[INFO] For seamless audio (no media player windows), install: pip install pygame")
            tts_info.setStyleSheet("color: #00BFFF; font-size: 11px; margin-top: 5px;")
            tts_layout.addWidget(tts_info)
        
        # Edge-TTS voice selection (if available)
        edge_voice_combo = None
        if edge_tts_available:
            edge_voice_label = QLabel("Select edge-tts Voice (Recommended - Offline):")
            edge_voice_label.setStyleSheet("color: #FFF; font-size: 12px; margin-top: 10px;")
            tts_layout.addWidget(edge_voice_label)
            
            edge_voice_combo = QComboBox()
            edge_voice_combo.setStyleSheet("""
                QComboBox {
                    background-color: #222;
                    color: #FFF;
                    border: 2px solid #555;
                    border-radius: 4px;
                    padding: 6px;
                }
            """)
            
            # Popular edge-tts voices (Windows neural voices)
            edge_voice_options = [
                ("English (US) - Aria (Female)", "en-US-AriaNeural"),
                ("English (US) - Jenny (Female)", "en-US-JennyNeural"),
                ("English (US) - Guy (Male)", "en-US-GuyNeural"),
                ("English (UK) - Sonia (Female)", "en-GB-SoniaNeural"),
                ("English (UK) - Ryan (Male)", "en-GB-RyanNeural"),
                ("English (Australia) - Natasha (Female)", "en-AU-NatashaNeural"),
                ("English (Australia) - William (Male)", "en-AU-WilliamNeural"),
                ("Spanish (Spain) - Elvira (Female)", "es-ES-ElviraNeural"),
                ("Spanish (Mexico) - Dalia (Female)", "es-MX-DaliaNeural"),
                ("French (France) - Denise (Female)", "fr-FR-DeniseNeural"),
                ("French (France) - Henri (Male)", "fr-FR-HenriNeural"),
                ("German - Katja (Female)", "de-DE-KatjaNeural"),
                ("German - Conrad (Male)", "de-DE-ConradNeural"),
                ("Italian - Elsa (Female)", "it-IT-ElsaNeural"),
                ("Japanese - Nanami (Female)", "ja-JP-NanamiNeural"),
                ("Chinese (Mandarin) - Xiaoxiao (Female)", "zh-CN-XiaoxiaoNeural"),
            ]
            
            for display_name, voice_id in edge_voice_options:
                edge_voice_combo.addItem(display_name, voice_id)
            
            # Set saved edge-tts voice if available
            if hasattr(self, 'edge_tts_voice') and self.edge_tts_voice:
                for i in range(edge_voice_combo.count()):
                    if edge_voice_combo.itemData(i) == self.edge_tts_voice:
                        edge_voice_combo.setCurrentIndex(i)
                        break
            
            tts_layout.addWidget(edge_voice_combo)
        
        # gTTS fallback checkbox (defined early so it's accessible in save section)
        gtts_fallback_cb = QCheckBox("Allow gTTS online fallback if edge-tts fails (requires internet)")
        gtts_fallback_cb.setChecked(self.enable_gtts_fallback)
        gtts_fallback_cb.setStyleSheet("""
            QCheckBox {
                color: #FFF;
                font-size: 12px;
                margin-top: 10px;
                padding: 6px;
                background-color: #2a2a2a;
                border-radius: 4px;
            }
            QCheckBox::indicator {
                width: 16px;
                height: 16px;
            }
        """)
        gtts_fallback_cb.setToolTip(
            "When enabled:\n"
            "‚Ä¢ gTTS will be used as a last resort if edge-tts fails\n"
            "‚Ä¢ Requires internet connection\n"
            "‚Ä¢ May have network issues or quota limits\n"
            "‚Ä¢ Recommended: Keep disabled unless edge-tts is unavailable"
        )
        if not gtts_available:
            gtts_fallback_cb.setEnabled(False)
            gtts_fallback_cb.setToolTip("gTTS not installed. Install with: pip install gtts")
        
        # gTTS voice selection (fallback)
        voice_combo = None
        if gtts_available:
            gtts_label = QLabel("Select gTTS Voice (Fallback - Requires Internet):" if edge_tts_available else "Select Voice/Accent:")
            gtts_label.setStyleSheet("color: #FFF; font-size: 12px; margin-top: 10px;")
            tts_layout.addWidget(gtts_label)
            
            voice_combo = QComboBox()
            voice_combo.setStyleSheet("""
                QComboBox {
                    background-color: #222;
                    color: #FFF;
                    border: 2px solid #555;
                    border-radius: 4px;
                    padding: 6px;
                }
            """)
            
            # gTTS supports different accents through the 'tld' parameter (Top Level Domain)
            # Format: (Display Name, language code, tld for accent)
            voice_options = [
                ("English (US) - Female", "en", "us"),
                ("English (US) - Default", "en", "com"),
                ("English (UK) - British", "en", "co.uk"),
                ("English (Australia)", "en", "com.au"),
                ("English (India)", "en", "co.in"),
                ("English (Canada)", "en", "ca"),
                ("English (South Africa)", "en", "co.za"),
                ("Spanish (Spain)", "es", "es"),
                ("Spanish (Mexico)", "es", "com.mx"),
                ("French (France)", "fr", "fr"),
                ("French (Canada)", "fr", "ca"),
                ("German", "de", "de"),
                ("Italian", "it", "it"),
                ("Portuguese (Brazil)", "pt", "com.br"),
                ("Portuguese (Portugal)", "pt", "pt"),
                ("Japanese", "ja", "co.jp"),
                ("Chinese (Mandarin)", "zh-CN", "com"),
                ("Korean", "ko", "co.kr"),
                ("Russian", "ru", "ru"),
                ("Arabic", "ar", "com"),
                ("Hindi", "hi", "co.in"),
            ]
            
            for display_name, lang, tld in voice_options:
                voice_combo.addItem(display_name, (lang, tld))
            
            # Set saved voice if available
            if self.tts_voice_id:
                try:
                    # tts_voice_id is stored as tuple (lang, tld)
                    for i in range(voice_combo.count()):
                        if voice_combo.itemData(i) == self.tts_voice_id:
                            voice_combo.setCurrentIndex(i)
                            break
                except:
                    pass
            
            tts_layout.addWidget(voice_combo)
        
        # Voice-only conversation mode
        voice_only_cb = QCheckBox("üéôÔ∏è Voice-Only Mode (Hide text during voice conversations)")
        voice_only_cb.setChecked(self.voice_only_mode)
        voice_only_cb.setStyleSheet("""
            QCheckBox {
                color: #FFF;
                font-size: 13px;
                margin-top: 15px;
                padding: 8px;
                background-color: #2a2a2a;
                border-radius: 4px;
            }
            QCheckBox::indicator {
                width: 18px;
                height: 18px;
            }
        """)
        voice_only_cb.setToolTip(
            "When enabled:\n"
            "‚Ä¢ Text won't appear on screen during voice conversations\n"
            "‚Ä¢ You'll see a visual indicator showing Lea is listening/speaking\n"
            "‚Ä¢ Perfect for hands-free, natural conversations\n"
            "‚Ä¢ Messages are still saved to history"
        )
        tts_layout.addWidget(voice_only_cb)
        
        # Add gTTS fallback checkbox (already defined above)
        tts_layout.addWidget(gtts_fallback_cb)
        
        layout.addWidget(tts_group)
        
        # Continuous Listening Section
        continuous_group = QGroupBox("üé§ Voice Input Mode")
        continuous_group.setStyleSheet("""
            QGroupBox {
                color: #FFF;
                border: 2px solid #555;
                border-radius: 6px;
                margin-top: 10px;
                padding-top: 10px;
            }
            QGroupBox::title {
                subcontrol-origin: margin;
                left: 10px;
                padding: 0 5px;
            }
        """)
        continuous_layout = QVBoxLayout(continuous_group)
        
        # Continuous listening checkbox
        continuous_cb = QCheckBox("üîÑ Continuous Listening Mode (Auto-restart after each response)")
        continuous_cb.setChecked(self.continuous_listening)
        continuous_cb.setStyleSheet("""
            QCheckBox {
                color: #FFF;
                font-size: 13px;
                padding: 8px;
                background-color: #2a2a2a;
                border-radius: 4px;
            }
            QCheckBox::indicator {
                width: 18px;
                height: 18px;
            }
        """)
        continuous_cb.setToolTip(
            "When enabled:\n"
            "‚Ä¢ After you speak, Lea will automatically listen again\n"
            "‚Ä¢ No need to click the mic button repeatedly\n"
            "‚Ä¢ Perfect for natural back-and-forth conversations\n"
            "‚Ä¢ Click the mic button to stop continuous mode"
        )
        continuous_layout.addWidget(continuous_cb)
        
        # Push-to-talk key input
        ptt_label = QLabel("Push-to-Talk Key (optional - leave empty to disable):")
        ptt_label.setStyleSheet("color: #FFF; font-size: 12px; margin-top: 10px;")
        continuous_layout.addWidget(ptt_label)
        
        ptt_input = QLineEdit()
        ptt_input.setPlaceholderText("e.g., Space, Ctrl+Space, F1 (leave empty to disable)")
        if self.push_to_talk_key:
            ptt_input.setText(self.push_to_talk_key)
        ptt_input.setStyleSheet("""
            QLineEdit {
                background-color: #222;
                color: #FFF;
                border: 2px solid #555;
                border-radius: 4px;
                padding: 6px;
                font-size: 13px;
            }
        """)
        ptt_input.setToolTip(
            "Set a keyboard shortcut for push-to-talk.\n"
            "Examples: 'Space', 'Ctrl+Space', 'F1'\n"
            "When pressed, starts listening. When released, stops (if not in continuous mode).\n"
            "Leave empty to disable."
        )
        continuous_layout.addWidget(ptt_input)
        
        layout.addWidget(continuous_group)
        
        # Microphone Section
        mic_group = QGroupBox("Microphone")
        mic_group.setStyleSheet("""
            QGroupBox {
                color: #FFF;
                border: 2px solid #555;
                border-radius: 6px;
                margin-top: 10px;
                padding-top: 10px;
            }
            QGroupBox::title {
                subcontrol-origin: margin;
                left: 10px;
                padding: 0 5px;
            }
        """)
        mic_layout = QVBoxLayout(mic_group)
        
        mic_label = QLabel("Select Microphone:")
        mic_label.setStyleSheet("color: #FFF; font-size: 12px;")
        mic_layout.addWidget(mic_label)
        
        mic_combo = QComboBox()
        mic_combo.setStyleSheet("""
            QComboBox {
                background-color: #222;
                color: #FFF;
                border: 2px solid #555;
                border-radius: 4px;
                padding: 6px;
            }
        """)
        
        mic_devices = []
        # Always try to list microphones, even if SPEECH_RECOGNITION_AVAILABLE might be False
        # (it could be a runtime detection issue)
        mic_combo_enabled = False
        mic_count = 0
        try:
            import speech_recognition as sr
            mic_list = sr.Microphone.list_microphone_names()
            if mic_list and len(mic_list) > 0:
                mic_combo.addItem("Default (System Default)", None)
                mic_count = 1
                for i, mic_name in enumerate(mic_list):
                    # Show just the name, not the index (cleaner)
                    mic_combo.addItem(mic_name, i)
                    mic_devices.append((i, mic_name))
                    mic_count += 1
                    # Select saved microphone if available
                    if self.microphone_device_index == i:
                        mic_combo.setCurrentIndex(i + 1)  # +1 for "Default" option
                # Enable the combo box if we found microphones
                mic_combo_enabled = True
                logging.info(f"Successfully listed {len(mic_list)} microphones")
            else:
                mic_combo.addItem("No microphones detected", None)
                mic_combo.setEnabled(False)
                logging.warning("No microphones found in list")
        except ImportError as ie:
            mic_combo.addItem("Speech recognition not available - Install: pip install SpeechRecognition", None)
            mic_combo.setEnabled(False)
            logging.warning(f"Speech recognition import failed: {ie}")
        except Exception as e:
            logging.error(f"Error listing microphones: {e}", exc_info=True)
            error_msg = str(e)[:80]  # Longer error message
            mic_combo.addItem(f"Error: {error_msg}", None)
            mic_combo.setEnabled(False)
            # Don't disable if we already added items
            if mic_count > 0:
                mic_combo_enabled = True
        
        # Explicitly enable if we successfully listed microphones
        if mic_combo_enabled and mic_count > 1:  # More than just "Default"
            # Ensure it's enabled and interactive
            mic_combo.setEnabled(True)
            mic_combo.setEditable(False)
            # Make sure it's not disabled by style
            mic_combo.setStyleSheet("""
                QComboBox {
                    background-color: #222;
                    color: #FFF;
                    border: 2px solid #555;
                    border-radius: 4px;
                    padding: 6px;
                }
                QComboBox:enabled {
                    background-color: #222;
                    color: #FFF;
                }
                QComboBox:hover {
                    border: 2px solid #0078D7;
                }
                QComboBox::drop-down {
                    border: none;
                    width: 30px;
                }
                QComboBox::down-arrow {
                    image: none;
                    border-left: 5px solid transparent;
                    border-right: 5px solid transparent;
                    border-top: 5px solid #FFF;
                    margin-right: 10px;
                }
            """)
            # Force update to ensure it's interactive
            mic_combo.update()
            mic_combo.repaint()
            logging.info(f"Microphone dropdown enabled with {mic_combo.count()} items, isEnabled={mic_combo.isEnabled()}")
        else:
            logging.warning(f"Microphone dropdown NOT enabled - enabled={mic_combo_enabled}, count={mic_count}")
        
        mic_layout.addWidget(mic_combo)
        
        # Add a label showing combo box state for debugging
        if mic_combo_enabled and mic_count > 1:
            status_label = QLabel(f"Status: {mic_combo.count()} microphones available - Dropdown should be selectable")
            status_label.setStyleSheet("color: #00FF00; font-size: 11px; margin-top: 5px;")
            mic_layout.addWidget(status_label)
        else:
            status_label = QLabel(f"Status: Dropdown disabled - {mic_count} items, enabled={mic_combo_enabled}")
            status_label.setStyleSheet("color: #FF9900; font-size: 11px; margin-top: 5px;")
            mic_layout.addWidget(status_label)
        
        # Add Test Microphone button (try to enable if speech_recognition can be imported)
        test_mic_available = False
        try:
            import speech_recognition as sr
            test_mic_available = True
        except ImportError:
            pass
        
        if test_mic_available:
            def test_selected_microphone():
                selected_index = mic_combo.currentData()
                try:
                    import speech_recognition as sr
                    recognizer = sr.Recognizer()
                    
                    if selected_index is None:
                        mic_name = "Default Microphone"
                    else:
                        try:
                            mic_list = sr.Microphone.list_microphone_names()
                            mic_name = mic_list[selected_index] if selected_index < len(mic_list) else f"Microphone #{selected_index}"
                        except:
                            mic_name = f"Microphone #{selected_index}"
                    
                    # Test the microphone
                    with sr.Microphone(device_index=selected_index) as source:
                        recognizer.adjust_for_ambient_noise(source, duration=0.5)
                        # Just opening and calibrating is enough to test
                    
                    QMessageBox.information(
                        dialog, 
                        "Microphone Test", 
                        f"‚úÖ Microphone '{mic_name}' is working!\n\nThe microphone was successfully accessed and calibrated."
                    )
                except Exception as e:
                    error_msg = str(e)
                    QMessageBox.warning(
                        dialog, 
                        "Microphone Test Failed", 
                        f"‚ùå Error testing microphone:\n\n{error_msg}\n\n"
                        "Possible solutions:\n"
                        "‚Ä¢ Close other apps using the microphone\n"
                        "‚Ä¢ Check microphone permissions\n"
                        "‚Ä¢ Try a different microphone"
                    )
                    logging.warning(f"Microphone test error: {e}")
            
            test_mic_btn = QPushButton("üé§ Test Selected Microphone")
            test_mic_btn.setStyleSheet("""
                QPushButton {
                    background-color: #107C10;
                    color: #FFF;
                    border: none;
                    border-radius: 4px;
                    padding: 8px 16px;
                    font-size: 13px;
                    margin-top: 8px;
                }
                QPushButton:hover {
                    background-color: #0e6b0e;
                }
            """)
            test_mic_btn.clicked.connect(test_selected_microphone)
            mic_layout.addWidget(test_mic_btn)
        
        layout.addWidget(mic_group)
        
        # Buttons
        buttons = QDialogButtonBox(QDialogButtonBox.StandardButton.Ok | QDialogButtonBox.StandardButton.Cancel)
        buttons.accepted.connect(dialog.accept)
        buttons.rejected.connect(dialog.reject)
        buttons.setStyleSheet("""
            QPushButton {
                background-color: #0078D7;
                color: #FFF;
                border: none;
                border-radius: 4px;
                padding: 8px 20px;
                font-size: 14px;
            }
            QPushButton:hover {
                background-color: #106ebe;
            }
        """)
        layout.addWidget(buttons)
        
        result = dialog.exec()
        if result == QDialog.DialogCode.Accepted:
            # Save TTS settings - check availability directly
            tts_actually_available = edge_tts_available or gtts_available
            self.tts_enabled = tts_enable_cb.isChecked() and tts_actually_available
            
            # Save edge-tts voice selection (if available)
            if edge_voice_combo and edge_voice_combo.currentData():
                self.edge_tts_voice = edge_voice_combo.currentData()
            
            # Save gTTS voice selection (fallback)
            if voice_combo:
                voice_data = voice_combo.currentData()
                if voice_data:
                    self.tts_voice_id = voice_data  # Save as tuple (lang, tld)
                else:
                    self.tts_voice_id = ("en", "com")  # Default
            
            # Save voice-only mode
            self.voice_only_mode = voice_only_cb.isChecked()
            
            # Save gTTS fallback setting
            self.enable_gtts_fallback = gtts_fallback_cb.isChecked() if gtts_available else False
            
            # Save continuous listening mode
            self.continuous_listening = continuous_cb.isChecked()
            
            # Save push-to-talk key
            ptt_key_text = ptt_input.text().strip()
            self.push_to_talk_key = ptt_key_text if ptt_key_text else None
            
            # Save microphone settings
            if mic_combo.currentData() is not None:
                self.microphone_device_index = mic_combo.currentData()
            else:
                self.microphone_device_index = None
            
            self.save_settings()
            
            # Show appropriate message based on mode
            msg_parts = []
            if self.voice_only_mode:
                msg_parts.append("üéôÔ∏è Voice-Only Mode: ENABLED")
            if self.continuous_listening:
                msg_parts.append("üîÑ Continuous Listening: ENABLED (auto-restart after each response)")
            if self.push_to_talk_key:
                msg_parts.append(f"‚å®Ô∏è Push-to-Talk: {self.push_to_talk_key}")
            
            if msg_parts:
                QMessageBox.information(
                    self, 
                    "Settings Saved", 
                    "Audio settings saved!\n\n" + "\n".join(msg_parts) + "\n\n"
                    "You can now have natural voice conversations with Lea!"
                )
            else:
                QMessageBox.information(self, "Settings Saved", "Audio settings have been saved successfully!")

# =====================================================
# ENHANCED STRESS TEST
# =====================================================

def run_enhanced_stress_test():
    """Comprehensive stress test for Lea Assistant components"""
    import time
    import threading
    from concurrent.futures import ThreadPoolExecutor, as_completed
    from PyQt6.QtCore import QThread, QCoreApplication
    
    print("=" * 80)
    print("üöÄ ENHANCED STRESS TEST - Lea Assistant")
    print("=" * 80)
    print()
    
    test_results = {
        "total_tests": 0,
        "passed": 0,
        "failed": 0,
        "errors": [],
        "timings": {}
    }
    
    def log_test(test_name, passed=True, error=None, duration=None):
        test_results["total_tests"] += 1
        if passed:
            test_results["passed"] += 1
            status = "‚úÖ PASS"
        else:
            test_results["failed"] += 1
            status = "‚ùå FAIL"
            if error:
                test_results["errors"].append(f"{test_name}: {error}")
        
        time_str = f" ({duration:.3f}s)" if duration else ""
        print(f"{status} - {test_name}{time_str}")
        if error and not passed:
            print(f"   Error: {error}")
        if duration:
            test_results["timings"][test_name] = duration
    
    print("üìã Test 1: Worker Thread Creation & Cleanup")
    print("-" * 80)
    
    # Test 1: ExportWorker thread creation and cleanup
    try:
        start = time.time()
        worker = ExportWorker(
            path=str(PROJECT_DIR / "test_export.json"),
            mode="chat",
            model="gpt-4o-mini",
            message_history=[{"role": "user", "content": "test"}],
            chat_text="Test content"
        )
        thread = QThread()
        worker.moveToThread(thread)
        thread.start()
        thread.quit()
        thread.wait(1000)
        duration = time.time() - start
        log_test("ExportWorker creation & cleanup", True, duration=duration)
    except Exception as e:
        log_test("ExportWorker creation & cleanup", False, str(e))
    
    # Test 2: DownloadWorker
    try:
        start = time.time()
        worker = DownloadWorker("Test response content")
        thread = QThread()
        worker.moveToThread(thread)
        thread.start()
        thread.quit()
        thread.wait(1000)
        duration = time.time() - start
        log_test("DownloadWorker creation & cleanup", True, duration=duration)
    except Exception as e:
        log_test("DownloadWorker creation & cleanup", False, str(e))
    
    # Test 3: TTSWorker
    try:
        start = time.time()
        worker = TTSWorker("Test TTS text", edge_tts_voice="en-US-AriaNeural")
        thread = QThread()
        worker.moveToThread(thread)
        thread.start()
        thread.quit()
        thread.wait(1000)
        duration = time.time() - start
        log_test("TTSWorker creation & cleanup", True, duration=duration)
    except Exception as e:
        log_test("TTSWorker creation & cleanup", False, str(e))
    
    # Test 4: FileUploadWorker
    try:
        test_file = PROJECT_DIR / "test_upload.txt"
        test_file.write_text("Test upload content", encoding='utf-8')
        start = time.time()
        worker = FileUploadWorker(str(test_file))
        thread = QThread()
        worker.moveToThread(thread)
        thread.start()
        thread.quit()
        thread.wait(1000)
        if test_file.exists():
            test_file.unlink()
        duration = time.time() - start
        log_test("FileUploadWorker creation & cleanup", True, duration=duration)
    except Exception as e:
        log_test("FileUploadWorker creation & cleanup", False, str(e))
    
    print()
    print("üìã Test 2: Memory System Operations")
    print("-" * 80)
    
    # Test 5: LeaMemory initialization
    try:
        start = time.time()
        memory = LeaMemory()
        duration = time.time() - start
        log_test("LeaMemory initialization", True, duration=duration)
    except Exception as e:
        log_test("LeaMemory initialization", False, str(e))
        memory = None
    
    # Test 6: Memory load/save operations
    if memory:
        try:
            start = time.time()
            test_memory_dir = PROJECT_DIR / "test_memory"
            test_memory = LeaMemory(memory_dir=test_memory_dir)
            # Create test memory file
            test_memory_file = test_memory_dir / "conversation_memory.json"
            if test_memory_file.exists():
                test_memory_file.unlink()
            test_memory._save_memories()
            test_memory._load_memories()
            duration = time.time() - start
            log_test("Memory load/save operations", True, duration=duration)
            # Cleanup
            if test_memory_dir.exists():
                import shutil
                shutil.rmtree(test_memory_dir, ignore_errors=True)
        except Exception as e:
            log_test("Memory load/save operations", False, str(e))
    
    print()
    print("üìã Test 3: File Operations")
    print("-" * 80)
    
    # Test 7: File backup creation
    try:
        start = time.time()
        test_file = PROJECT_DIR / "test_backup_file.txt"
        test_file.write_text("Test content for backup", encoding='utf-8')
        backup_path = create_backup(test_file)
        duration = time.time() - start
        success = backup_path and Path(backup_path).exists()
        if backup_path and Path(backup_path).exists():
            Path(backup_path).unlink()
        if test_file.exists():
            test_file.unlink()
        log_test("File backup creation", success, duration=duration)
    except Exception as e:
        log_test("File backup creation", False, str(e))
    
    # Test 8: Save to downloads
    try:
        start = time.time()
        download_path = save_to_downloads("Test download content", "test_stress.txt")
        duration = time.time() - start
        success = download_path and Path(download_path).exists()
        if download_path and Path(download_path).exists():
            try:
                Path(download_path).unlink()
            except:
                pass
        log_test("Save to downloads", success, duration=duration)
    except Exception as e:
        log_test("Save to downloads", False, str(e))
    
    print()
    print("üìã Test 4: Retry Logic")
    print("-" * 80)
    
    # Test 9: Retry mechanism
    try:
        start = time.time()
        call_count = [0]
        def failing_function():
            call_count[0] += 1
            if call_count[0] < 3:
                raise Exception("Simulated failure")
            return "Success"
        
        result = retry_api_call(failing_function, max_attempts=3, base_delay=0.1)
        duration = time.time() - start
        success = result == "Success" and call_count[0] == 3
        log_test("Retry logic with exponential backoff", success, duration=duration)
    except Exception as e:
        log_test("Retry logic with exponential backoff", False, str(e))
    
    print()
    print("üìã Test 5: Concurrent Operations")
    print("-" * 80)
    
    # Test 10: Concurrent worker creation
    try:
        start = time.time()
        workers = []
        threads = []
        for i in range(5):
            worker = ExportWorker(
                path=str(PROJECT_DIR / f"test_concurrent_{i}.json"),
                mode="chat",
                model="gpt-4o-mini",
                message_history=[],
                chat_text=f"Concurrent test {i}"
            )
            thread = QThread()
            worker.moveToThread(thread)
            workers.append(worker)
            threads.append(thread)
            thread.start()
        
        # Wait for all threads
        for thread in threads:
            thread.quit()
            thread.wait(1000)
        
        # Cleanup test files
        for i in range(5):
            test_file = PROJECT_DIR / f"test_concurrent_{i}.json"
            if test_file.exists():
                test_file.unlink()
        
        duration = time.time() - start
        log_test("Concurrent worker creation (5 threads)", True, duration=duration)
    except Exception as e:
        log_test("Concurrent worker creation (5 threads)", False, str(e))
    
    # Test 11: Thread pool stress
    try:
        start = time.time()
        def dummy_task(n):
            time.sleep(0.01)
            return n * 2
        
        with ThreadPoolExecutor(max_workers=10) as executor:
            futures = [executor.submit(dummy_task, i) for i in range(20)]
            results = [f.result() for f in as_completed(futures)]
        
        duration = time.time() - start
        success = len(results) == 20 and all(r % 2 == 0 for r in results)
        log_test("Thread pool stress (20 tasks)", success, duration=duration)
    except Exception as e:
        log_test("Thread pool stress (20 tasks)", False, str(e))
    
    print()
    print("üìã Test 6: Error Handling")
    print("-" * 80)
    
    # Test 12: Error handling in workers
    try:
        start = time.time()
        worker = ExportWorker(
            path="",  # Invalid path to trigger error
            mode="chat",
            model="gpt-4o-mini",
            message_history=[],
            chat_text="Test"
        )
        error_caught = False
        def catch_error(msg):
            nonlocal error_caught
            error_caught = True
        
        worker.error.connect(catch_error)
        thread = QThread()
        worker.moveToThread(thread)
        thread.started.connect(worker.run)
        thread.start()
        thread.quit()
        thread.wait(2000)
        duration = time.time() - start
        log_test("Error handling in workers", error_caught, duration=duration)
    except Exception as e:
        log_test("Error handling in workers", False, str(e))
    
    print()
    print("üìã Test 7: API Integration (if available)")
    print("-" * 80)
    
    # Test 13: Model fetching (if API key available)
    api_key = os.getenv("OPENAI_API_KEY")
    if api_key:
        try:
            start = time.time()
            models = fetch_available_models(api_key)
            duration = time.time() - start
            success = isinstance(models, dict) and len(models) > 0
            log_test("Model fetching from API", success, duration=duration)
            if success:
                print(f"   Found {len(models)} models")
        except Exception as e:
            log_test("Model fetching from API", False, str(e))
    else:
        print("‚è≠Ô∏è  SKIP - Model fetching (no API key)")
        test_results["total_tests"] += 1
    
    print()
    print("üìã Test 8: Resource Cleanup")
    print("-" * 80)
    
    # Test 14: Resource cleanup
    try:
        start = time.time()
        cleanup_count = 0
        
        # Create and cleanup multiple resources
        for i in range(10):
            test_file = PROJECT_DIR / f"test_cleanup_{i}.tmp"
            test_file.write_text("temp", encoding='utf-8')
            cleanup_count += 1
        
        # Cleanup
        for i in range(10):
            test_file = PROJECT_DIR / f"test_cleanup_{i}.tmp"
            if test_file.exists():
                test_file.unlink()
        
        duration = time.time() - start
        success = cleanup_count == 10
        log_test("Resource cleanup (10 files)", success, duration=duration)
    except Exception as e:
        log_test("Resource cleanup (10 files)", False, str(e))
    
    print()
    print("=" * 80)
    print("üìä STRESS TEST RESULTS")
    print("=" * 80)
    print(f"Total Tests: {test_results['total_tests']}")
    print(f"‚úÖ Passed: {test_results['passed']}")
    print(f"‚ùå Failed: {test_results['failed']}")
    print(f"Success Rate: {(test_results['passed'] / test_results['total_tests'] * 100):.1f}%")
    print()
    
    if test_results['timings']:
        print("‚è±Ô∏è  PERFORMANCE METRICS:")
        print("-" * 80)
        sorted_timings = sorted(test_results['timings'].items(), key=lambda x: x[1], reverse=True)
        for test_name, timing in sorted_timings[:5]:
            print(f"  {test_name}: {timing:.3f}s")
        print()
    
    if test_results['errors']:
        print("‚ö†Ô∏è  ERRORS:")
        print("-" * 80)
        for error in test_results['errors']:
            print(f"  ‚Ä¢ {error}")
        print()
    
    total_time = sum(test_results['timings'].values())
    print(f"‚è±Ô∏è  Total Test Duration: {total_time:.3f}s")
    print("=" * 80)
    
    return test_results['failed'] == 0

# =====================================================
# MAIN
# =====================================================

def main():
    import sys
    global AGENTS  # Declare global at function level
    
    # Set up exception handling before creating QApplication
    def qt_exception_handler(exc_type, exc_value, exc_traceback):
        """Handle exceptions in Qt event loop"""
        if issubclass(exc_type, KeyboardInterrupt):
            sys.__excepthook__(exc_type, exc_value, exc_traceback)
            return
        
        # Log the exception
        handle_exception(exc_type, exc_value, exc_traceback)
    
    # Install Qt exception handler
    sys.excepthook = qt_exception_handler
    
    try:
        app = QApplication(sys.argv)
        
        # Create 14-day backups on startup (failsafe system - both locations required)
        try:
            # Determine the source file path
            if getattr(sys, 'frozen', False):
                # Running as executable - use the executable path
                source_file = Path(sys.executable)
            else:
                # Running as script - use __file__ if available, otherwise construct from PROJECT_DIR
                try:
                    source_file = Path(__file__)
                except NameError:
                    # Fallback: construct path from PROJECT_DIR
                    source_file = PROJECT_DIR / "Lea_Visual_Code_v2.5_ TTS.py"
            
            # Only backup if file exists and is a Python file
            if source_file.exists() and source_file.suffix == '.py':
                logging.info(f"üîÑ Creating 14-day failsafe backup of: {source_file}")
                backup_results = manage_14_day_backups(source_file, max_backups=14)
                
                # Log summary for user visibility
                if backup_results:
                    f_drive_ok = backup_results.get("f_drive", False)
                    icloud_ok = backup_results.get("icloud", False)
                    
                    if f_drive_ok and icloud_ok:
                        print("‚úÖ Backup Status: Both locations successful (F: drive + iCloud)")
                    elif f_drive_ok or icloud_ok:
                        failed = "iCloud" if not icloud_ok else "F: drive"
                        print(f"‚ö†Ô∏è Backup Warning: Only one location succeeded. {failed} backup failed - check logs!")
                    else:
                        print("‚ùå Backup Error: Both backup locations failed - check logs immediately!")
            else:
                logging.info(f"Skipping backup - file not found or not a Python file: {source_file}")
        except Exception as backup_error:
            # Don't fail startup if backup fails, but log the error
            logging.error(f"‚ùå Critical backup error: {backup_error}")
            import traceback
            logging.error(traceback.format_exc())
            print(f"‚ùå Backup system error: {backup_error} - check logs for details")
        
        # Refresh models from OpenAI API on startup (background, non-blocking)
        if MODEL_REGISTRY_AVAILABLE:
            try:
                logging.info("Refreshing model list from OpenAI API...")
                refresh_models(force=False)  # Use cache if available, otherwise fetch
                # Re-initialize model assignments with fresh models
                initialize_model_per_mode()
                # Update MODEL_OPTIONS with fresh models
                registry = get_model_registry()
                all_models = registry.get_all_models()
                MODEL_OPTIONS.clear()
                MODEL_OPTIONS.update({model_id: model_id for model_id in all_models})
                logging.info(f"Loaded {len(MODEL_OPTIONS)} models from OpenAI API")
            except Exception as e:
                logging.warning(f"Error refreshing models on startup: {e}")
        
        # Only run installer in executable version, not in main Python file
        # Check if running as executable (PyInstaller bundle)
        is_executable = getattr(sys, 'frozen', False) or hasattr(sys, '_MEIPASS')
        
        if is_executable:
            # Running as executable - check for config and run installer if needed
            config_file = PROJECT_DIR / "agent_config.json"
            
            if not config_file.exists():
                try:
                    from installer import run_installer, save_config
                    config = run_installer()
                    if config:
                        save_config(config, config_file)
                        # Rebuild AGENTS with custom config
                        AGENTS = build_agents(
                            agent_name=config["agent_name"],
                            user_name=config["user_name"],
                            custom_personality=config.get("personality")
                        )
                    else:
                        # User cancelled installation
                        sys.exit(0)
                except Exception as e:
                    logging.warning(f"Installer error: {e}. Using defaults.")
            else:
                # Config exists, load it and rebuild AGENTS
                try:
                    with open(config_file, 'r', encoding='utf-8') as f:
                        config = json.load(f)
                        # agent_config.json overrides .env (higher priority)
                        AGENTS = build_agents(
                            agent_name=config.get("agent_name", LEA_AGENT_NAME),
                            user_name=config.get("user_name", LEA_USER_NAME),
                            custom_personality=config.get("personality")
                        )
                except Exception as e:
                    logging.warning(f"Error loading config: {e}. Using defaults.")
        
        # If running as Python script (not executable), use hardcoded defaults (Lea/Dre)
        # AGENTS is already initialized with defaults above
        
        # Set application properties for better error handling
        app.setQuitOnLastWindowClosed(True)
        
        splash = None
        try:
            if SPLASH_FILE.exists():
                splash = QSplashScreen(QPixmap(str(SPLASH_FILE)))
                # Ensure splash appears on top of all windows
                splash.setWindowFlags(Qt.WindowType.WindowStaysOnTopHint | Qt.WindowType.SplashScreen)
                splash.raise_()  # Bring to front
                splash.activateWindow()  # Activate to ensure it's on top
                splash.show()
                app.processEvents()
        except Exception as splash_error:
            logging.warning(f"Splash screen error: {splash_error}")
            splash = None
        
        try:
            window = LeaWindow()
            # Show window but keep it behind splash initially
            window.show()
            window.lower()  # Keep main window behind splash
            app.processEvents()
            
            if splash:
                # Ensure splash stays on top
                splash.raise_()
                splash.activateWindow()
                app.processEvents()
                
                # Keep splash screen visible for 1.5 seconds
                def finish_splash():
                    if splash:
                        splash.finish(window)
                        # Bring main window to front after splash closes
                        window.raise_()
                        window.activateWindow()
                QTimer.singleShot(1500, finish_splash)  # 1500ms = 1.5 seconds
        except Exception as window_error:
            logging.error(f"Window creation error: {traceback.format_exc()}")
            QMessageBox.critical(None, "Fatal Error", 
                               f"Failed to create window:\n{str(window_error)}\n\nCheck lea_crash.log for details.")
            sys.exit(1)
        
        try:
            sys.exit(app.exec())
        except Exception as app_error:
            logging.error(f"Application error: {traceback.format_exc()}")
            sys.exit(1)
            
    except Exception as main_error:
        logging.error(f"Main function error: {traceback.format_exc()}")
        print(f"Fatal error: {main_error}")
        print("Check lea_crash.log for details.")
        sys.exit(1)

if __name__ == "__main__":
    # Check for stress test argument
    if len(sys.argv) > 1 and sys.argv[1] == "--stress-test":
        try:
            from PyQt6.QtCore import QCoreApplication
            app = QCoreApplication(sys.argv)
            success = run_enhanced_stress_test()
            sys.exit(0 if success else 1)
        except Exception as e:
            print(f"Error running stress test: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)
    else:
        # Test import and basic initialization
        try:
            print("Testing Lea Assistant initialization...")
            main()
        except Exception as e:
            print(f"Error starting application: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)
